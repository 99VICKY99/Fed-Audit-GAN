{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0259965b",
   "metadata": {},
   "source": [
    "# üîß Fed-Audit-GAN v2.0 - CIFAR-10 (Ablation Study) - Kaggle GPU Edition\n",
    "\n",
    "## üéØ Experiments Run (SEPARATE IMPLEMENTATIONS):\n",
    "\n",
    "### üîµ Baseline: FedAvg (Federated Averaging)\n",
    "- **Completely separate algorithm** (NOT gamma=0!)\n",
    "- Standard data-weighted averaging (McMahan et al., 2017)\n",
    "- NO GAN, NO fairness scoring, NO FedProx\n",
    "\n",
    "### üü¢ Our Method: Fed-Audit-GAN\n",
    "- **Fed-Audit-GAN Œ≥ = 0.3** - Mild fairness weighting\n",
    "- **Fed-Audit-GAN Œ≥ = 0.7** - Strong fairness weighting\n",
    "\n",
    "## üîß Fed-Audit-GAN 4-Phase Architecture:\n",
    "1. **Phase 1**: Local Client Training (with FedProx)\n",
    "2. **Phase 2**: GAN Training (Fairness Generator)\n",
    "3. **Phase 3**: Fairness Scoring (with Momentum/EMA)\n",
    "4. **Phase 4**: Fairness-Aware Aggregation\n",
    "\n",
    "## ‚≠ê PATHOLOGICAL NON-IID (3 Classes Per Client):\n",
    "- Each client ONLY has 3 out of 10 classes\n",
    "- Creates REAL fairness challenges (not like Dirichlet where all classes are present)\n",
    "- Clients have 0% initial accuracy on unseen classes\n",
    "- Forces Fed-Audit-GAN to balance between different class subsets\n",
    "\n",
    "## üìä Fairness Metrics (Per-Client Accuracy Based!):\n",
    "- **JFI**: Jain's Fairness Index\n",
    "- **Max-Min Fairness**: min(acc)/max(acc)\n",
    "- **Accuracy Gap**: max(acc) - min(acc)\n",
    "\n",
    "## üöÄ GPU Optimizations (Kaggle T4 x2):\n",
    "- Mixed Precision Training (AMP) with torch.amp\n",
    "- cuDNN benchmark mode enabled\n",
    "- TF32 enabled for faster matrix operations\n",
    "- Optimized DataLoaders with pin_memory & prefetching\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies (Kaggle GPU Edition)\n",
    "!pip install -q torch torchvision tqdm matplotlib numpy wandb\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838d55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"‚úÖ WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b10d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Imports and GPU Setup (Kaggle T4 x2)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# üöÄ GPU SETUP (Kaggle T4 x2)\n",
    "# ============================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    \n",
    "    # Enable cuDNN optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # Enable TF32 for faster matrix operations on Ampere+ GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"‚úÖ GPU(s) detected: {NUM_GPUS}\")\n",
    "    for i in range(NUM_GPUS):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"   GPU {i}: {props.name}\")\n",
    "        print(f\"      Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"      Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    NUM_GPUS = 0\n",
    "    print(\"‚ö†Ô∏è  No GPU detected. Using CPU (training will be slow).\")\n",
    "\n",
    "# Mixed Precision Training\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "if USE_AMP:\n",
    "    print(\"\\n‚úÖ Mixed Precision Training (AMP) enabled.\")\n",
    "\n",
    "print(f\"\\nüìç Device: {DEVICE}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   GPUs available: {NUM_GPUS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0192088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEFINITIONS (CIFAR-10 - Deeper CNN)\n",
    "# ============================================================\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"Deeper CNN for CIFAR-10 classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32->16\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16->8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8->4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 4->2\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    \"\"\"Generator that produces paired samples (x, x') for fairness testing\"\"\"\n",
    "    def __init__(self, latent_dim=128, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4  # 8\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 512 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input).view(-1, 512, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        return x, torch.clamp(x + delta, -1, 1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Conditional Discriminator - outputs logits for BCEWithLogitsLoss\"\"\"\n",
    "    def __init__(self, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 1)  # No Sigmoid\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Models defined: CNN, FairnessGenerator, Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d97bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS (GPU Optimized with AMP)\n",
    "# ============================================================\n",
    "\n",
    "def train_gan(G, D, model, loader, epochs=20, device='cuda', l1=1.0, l2=1.0):\n",
    "    \"\"\"Train the Fairness GAN with Mixed Precision\"\"\"\n",
    "    G, D, model = G.to(device), D.to(device), model.to(device)\n",
    "    model.eval()\n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # AMP scalers\n",
    "    if USE_AMP:\n",
    "        scaler_G = torch.amp.GradScaler(device='cuda')\n",
    "        scaler_D = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            bs = imgs.size(0)\n",
    "            real = torch.ones(bs, 1, device=device)\n",
    "            fake_t = torch.zeros(bs, 1, device=device)\n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            z = torch.randn(bs, G.latent_dim, device=device)\n",
    "            gl = torch.randint(0, G.num_classes, (bs,), device=device)\n",
    "            \n",
    "            # Generator\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    with torch.no_grad():\n",
    "                        px, pxp = model(x), model(xp)\n",
    "                    t1 = -torch.mean((px - pxp) ** 2)\n",
    "                    t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                    t3 = l2 * (bce(D(x, gl), real) + bce(D(xp, gl), real)) / 2\n",
    "                    g_loss = t1 + t2 + t3\n",
    "                scaler_G.scale(g_loss).backward()\n",
    "                scaler_G.step(opt_G)\n",
    "                scaler_G.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                with torch.no_grad():\n",
    "                    px, pxp = model(x), model(xp)\n",
    "                t1 = -torch.mean((px - pxp) ** 2)\n",
    "                t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                t3 = l2 * (bce(D(x, gl), real) + bce(D(xp, gl), real)) / 2\n",
    "                g_loss = t1 + t2 + t3\n",
    "                g_loss.backward()\n",
    "                opt_G.step()\n",
    "            \n",
    "            # Discriminator\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    d_loss = (bce(D(imgs, labels), real) + bce(D(x.detach(), gl), fake_t) + bce(D(xp.detach(), gl), fake_t)) / 3\n",
    "                scaler_D.scale(d_loss).backward()\n",
    "                scaler_D.step(opt_D)\n",
    "                scaler_D.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                d_loss = (bce(D(imgs, labels), real) + bce(D(x.detach(), gl), fake_t) + bce(D(xp.detach(), gl), fake_t)) / 3\n",
    "                d_loss.backward()\n",
    "                opt_D.step()\n",
    "    \n",
    "    return G, D\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bias(model, x, xp, device):\n",
    "    \"\"\"Compute bias as difference in model predictions between x and x'\"\"\"\n",
    "    model.eval()\n",
    "    x, xp = x.to(device), xp.to(device)\n",
    "    \n",
    "    if USE_AMP:\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    else:\n",
    "        diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    \n",
    "    return diff.item()\n",
    "\n",
    "\n",
    "def partition_data_pathological_non_iid(dataset, n_clients, classes_per_client=3):\n",
    "    \"\"\"\n",
    "    ‚≠ê PATHOLOGICAL NON-IID: Each client gets ONLY 'classes_per_client' classes.\n",
    "    \n",
    "    This creates REAL fairness challenges:\n",
    "    - Client with [airplane, car, bird] has 0% accuracy on other 7 classes\n",
    "    - Forces the global model to balance between clients with different class subsets\n",
    "    - Fairness metrics will show real variation (not 99% in 5 rounds!)\n",
    "    \n",
    "    Args:\n",
    "        dataset: PyTorch dataset\n",
    "        n_clients: Number of clients\n",
    "        classes_per_client: How many classes each client gets (default=3)\n",
    "    \n",
    "    Returns:\n",
    "        List of index arrays, one per client\n",
    "    \"\"\"\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    n_classes = len(np.unique(labels))  # 10 for CIFAR-10\n",
    "    \n",
    "    # Get indices for each class\n",
    "    class_indices = {c: np.where(labels == c)[0] for c in range(n_classes)}\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "    \n",
    "    # Assign classes to clients (cycling through)\n",
    "    client_classes = []\n",
    "    all_classes = list(range(n_classes))\n",
    "    \n",
    "    for cid in range(n_clients):\n",
    "        # Each client gets 'classes_per_client' consecutive classes (with wrap-around)\n",
    "        start_class = (cid * classes_per_client) % n_classes\n",
    "        assigned = []\n",
    "        for i in range(classes_per_client):\n",
    "            assigned.append((start_class + i) % n_classes)\n",
    "        client_classes.append(assigned)\n",
    "    \n",
    "    # Count how many clients share each class\n",
    "    class_client_count = {c: sum(1 for cc in client_classes if c in cc) for c in range(n_classes)}\n",
    "    \n",
    "    # Distribute samples to clients\n",
    "    class_pointers = {c: 0 for c in range(n_classes)}\n",
    "    client_indices = [[] for _ in range(n_clients)]\n",
    "    \n",
    "    for cid in range(n_clients):\n",
    "        for c in client_classes[cid]:\n",
    "            # Each client gets an equal share of each class they have\n",
    "            n_samples_for_class = len(class_indices[c]) // class_client_count[c]\n",
    "            start = class_pointers[c]\n",
    "            end = start + n_samples_for_class\n",
    "            \n",
    "            if end <= len(class_indices[c]):\n",
    "                client_indices[cid].extend(class_indices[c][start:end].tolist())\n",
    "                class_pointers[c] = end\n",
    "    \n",
    "    # Shuffle each client's data\n",
    "    result = []\n",
    "    for cid in range(n_clients):\n",
    "        indices = np.array(client_indices[cid])\n",
    "        np.random.shuffle(indices)\n",
    "        result.append(indices)\n",
    "    \n",
    "    return result, client_classes  # Also return which classes each client has\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model accuracy with AMP\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for d, t in loader:\n",
    "        d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                preds = model(d).argmax(1)\n",
    "        else:\n",
    "            preds = model(d).argmax(1)\n",
    "        \n",
    "        correct += (preds == t).sum().item()\n",
    "        total += len(t)\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_client(model, client_loaders, device):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy on EACH client's data.\n",
    "    This measures how fairly the model performs across clients.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    client_accuracies = []\n",
    "    \n",
    "    for loader in client_loaders:\n",
    "        correct, total = 0, 0\n",
    "        for d, t in loader:\n",
    "            d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "            \n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    preds = model(d).argmax(1)\n",
    "            else:\n",
    "                preds = model(d).argmax(1)\n",
    "            \n",
    "            correct += (preds == t).sum().item()\n",
    "            total += len(t)\n",
    "        \n",
    "        acc = 100 * correct / total if total > 0 else 0\n",
    "        client_accuracies.append(acc)\n",
    "    \n",
    "    return client_accuracies\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FAIRNESS METRICS (Based on Per-Client Performance!)\n",
    "# ============================================================\n",
    "\n",
    "def calculate_jfi(performances):\n",
    "    \"\"\"Jain's Fairness Index: (Œ£p·µ¢)¬≤ / (N √ó Œ£p·µ¢¬≤)\"\"\"\n",
    "    p = np.array(performances)\n",
    "    n = len(p)\n",
    "    if np.sum(p ** 2) == 0:\n",
    "        return 1.0\n",
    "    return (np.sum(p) ** 2) / (n * np.sum(p ** 2))\n",
    "\n",
    "\n",
    "def calculate_max_min_fairness(performances):\n",
    "    \"\"\"Max-Min Fairness: min(acc) / max(acc)\"\"\"\n",
    "    p = np.array(performances)\n",
    "    if np.max(p) == 0:\n",
    "        return 0.0\n",
    "    return np.min(p) / np.max(p)\n",
    "\n",
    "\n",
    "def calculate_variance(performances):\n",
    "    return np.var(performances)\n",
    "\n",
    "\n",
    "def calculate_accuracy_gap(performances):\n",
    "    return np.max(performances) - np.min(performances)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined (GPU optimized with AMP)\")\n",
    "print(\"   ‚≠ê Using PATHOLOGICAL Non-IID partitioning (3 classes per client)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Training Parameters\n",
    "N_ROUNDS = 50           # Total training rounds\n",
    "N_CLIENTS = 20          # Number of federated clients\n",
    "N_GAN_EPOCHS = 15       # GAN training epochs per round (Fed-Audit-GAN only)\n",
    "N_PROBES = 500          # Number of probe samples (Fed-Audit-GAN only)\n",
    "LOCAL_EPOCHS = 3        # Local training epochs per client\n",
    "\n",
    "# ‚≠ê Fed-Audit-GAN Specific Parameters (NOT used by FedAvg!)\n",
    "MOMENTUM = 0.8          # EMA momentum for fairness scores\n",
    "WARMUP_ROUNDS = 5       # Rounds before activating fairness scoring\n",
    "MU = 0.01               # FedProx proximal term strength\n",
    "\n",
    "# DataLoader Parameters (GPU Optimized for T4 x2)\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "PREFETCH_FACTOR = 2\n",
    "\n",
    "# ‚≠ê PATHOLOGICAL NON-IID\n",
    "# Each client gets ONLY 3 classes - creates real fairness challenges\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß Fed-Audit-GAN v2.0 - CIFAR-10 ABLATION STUDY (Kaggle GPU)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPUs: {NUM_GPUS}\")\n",
    "print(f\"AMP Enabled: {USE_AMP}\")\n",
    "print(f\"Rounds: {N_ROUNDS}, Clients: {N_CLIENTS}\")\n",
    "print(f\"\\nüéØ EXPERIMENTS TO RUN:\")\n",
    "print(f\"   üîµ 1. FedAvg (SEPARATE BASELINE)\")\n",
    "print(f\"      - Standard Federated Averaging\")\n",
    "print(f\"      - Data-weighted aggregation\")\n",
    "print(f\"      - NO GAN, NO fairness scoring\")\n",
    "print(f\"   üü¢ 2. Fed-Audit-GAN Œ≥=0.3 (Mild fairness)\")\n",
    "print(f\"   üü¢ 3. Fed-Audit-GAN Œ≥=0.7 (Strong fairness)\")\n",
    "print(f\"\\n‚≠ê PATHOLOGICAL NON-IID:\")\n",
    "print(f\"   Each client gets ONLY 3/10 classes\")\n",
    "print(f\"   (Creates REAL fairness challenges!)\")\n",
    "print(f\"\\n‚≠ê FED-AUDIT-GAN PARAMETERS:\")\n",
    "print(f\"   Momentum (Œ≤): {MOMENTUM}\")\n",
    "print(f\"   Warm-up Rounds: {WARMUP_ROUNDS}\")\n",
    "print(f\"   FedProx (Œº): {MU}\")\n",
    "print(f\"\\nüì¶ BATCH SIZES:\")\n",
    "print(f\"   Training: {BATCH_SIZE}\")\n",
    "print(f\"   Validation: {VAL_BATCH_SIZE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING (CIFAR-10 with GPU Optimizations)\n",
    "# ‚≠ê PATHOLOGICAL NON-IID: Each client gets ONLY 3 classes!\n",
    "# ============================================================\n",
    "\n",
    "# CIFAR-10 class names for reference\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# How many classes each client gets (PATHOLOGICAL setting)\n",
    "CLASSES_PER_CLIENT = 3\n",
    "\n",
    "# CIFAR-10 transforms with augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# ‚≠ê Create PATHOLOGICAL Non-IID partitions (3 classes per client)\n",
    "np.random.seed(42)\n",
    "client_idx, client_classes = partition_data_pathological_non_iid(\n",
    "    train_data, N_CLIENTS, classes_per_client=CLASSES_PER_CLIENT\n",
    ")\n",
    "\n",
    "# Calculate data weights for each client\n",
    "client_data_sizes = [len(idx) for idx in client_idx]\n",
    "total_samples = sum(client_data_sizes)\n",
    "CLIENT_DATA_WEIGHTS = [size / total_samples for size in client_data_sizes]\n",
    "\n",
    "# DataLoader kwargs (GPU optimized)\n",
    "dataloader_kwargs = {\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY,\n",
    "    'persistent_workers': True,\n",
    "    'prefetch_factor': PREFETCH_FACTOR,\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=VAL_BATCH_SIZE, shuffle=False, **dataloader_kwargs)\n",
    "val_loader = DataLoader(\n",
    "    Subset(train_data, np.random.choice(len(train_data), 2000, replace=False)),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **dataloader_kwargs\n",
    ")\n",
    "\n",
    "# Client data loaders\n",
    "client_loaders = [\n",
    "    DataLoader(Subset(train_data, client_idx[c]), batch_size=BATCH_SIZE, shuffle=True, **dataloader_kwargs)\n",
    "    for c in range(N_CLIENTS)\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Data loaded!\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"‚≠ê PATHOLOGICAL NON-IID DISTRIBUTION ({CLASSES_PER_CLIENT} classes per client)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   This creates REAL fairness challenges:\")\n",
    "print(f\"   - Each client only sees {CLASSES_PER_CLIENT}/10 classes\")\n",
    "print(f\"   - Clients have 0% accuracy on unseen classes initially\")\n",
    "print(f\"   - Forces global model to balance between different class subsets\")\n",
    "print(f\"\\nüìä CLIENT CLASS ASSIGNMENTS:\")\n",
    "for i in range(min(10, N_CLIENTS)):  # Show first 10 clients\n",
    "    class_names = [CIFAR10_CLASSES[c] for c in client_classes[i]]\n",
    "    print(f\"   Client {i:2d}: {client_classes[i]} ‚Üí {class_names} ({client_data_sizes[i]} samples)\")\n",
    "if N_CLIENTS > 10:\n",
    "    print(f\"   ... ({N_CLIENTS - 10} more clients)\")\n",
    "\n",
    "print(f\"\\nüì¶ DATA SIZES:\")\n",
    "print(f\"   Samples per client: min={min(client_data_sizes)}, max={max(client_data_sizes)}\")\n",
    "print(f\"   Total distributed: {sum(client_data_sizes)} / {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üöÄ SEPARATE IMPLEMENTATIONS: FedAvg vs Fed-Audit-GAN\n",
    "# FedAvg is a SEPARATE baseline, NOT gamma=0!\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# PART A: FedAvg (Federated Averaging) - SEPARATE BASELINE\n",
    "# ============================================================\n",
    "\n",
    "def run_fedavg(n_rounds, n_clients, train_data, client_idx, val_loader, test_loader, \n",
    "               client_loaders, local_epochs, device, use_amp, client_data_weights):\n",
    "    \"\"\"\n",
    "    Standard FedAvg (McMahan et al., 2017)\n",
    "    - No GAN, no fairness scoring\n",
    "    - Pure data-weighted averaging of model updates\n",
    "    - Completely separate from Fed-Audit-GAN\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üîµ RUNNING: FedAvg (Federated Averaging) - BASELINE\")\n",
    "    print(\"   This is a SEPARATE algorithm, NOT gamma=0 of Fed-Audit-GAN!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = CNN().to(device)\n",
    "    \n",
    "    # AMP scaler for CUDA\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'acc': [], 'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=\"FedAvg\"):\n",
    "        # ================================================================\n",
    "        # LOCAL CLIENT TRAINING\n",
    "        # ================================================================\n",
    "        local_weights = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            # Copy global model to local\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            loss = criterion(output, target)\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            # Collect local weights\n",
    "            local_weights.append(copy.deepcopy(local_model.state_dict()))\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # ================================================================\n",
    "        # SERVER AGGREGATION (Data-Weighted Averaging)\n",
    "        # ================================================================\n",
    "        avg_weights = copy.deepcopy(local_weights[0])\n",
    "        for key in avg_weights.keys():\n",
    "            avg_weights[key] = avg_weights[key] * client_data_weights[0]\n",
    "            for i in range(1, len(local_weights)):\n",
    "                avg_weights[key] += local_weights[i][key] * client_data_weights[i]\n",
    "        \n",
    "        # Update global model\n",
    "        model.load_state_dict(avg_weights)\n",
    "        \n",
    "        # ================================================================\n",
    "        # EVALUATION\n",
    "        # ================================================================\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PART B: Fed-Audit-GAN (4-Phase Architecture)\n",
    "# ============================================================\n",
    "\n",
    "def run_fed_audit_gan(gamma, n_rounds, n_clients, warmup_rounds, momentum, mu,\n",
    "                      train_data, client_idx, val_loader, test_loader, client_loaders,\n",
    "                      n_gan_epochs, n_probes, local_epochs, device, use_amp,\n",
    "                      client_data_weights):\n",
    "    \"\"\"\n",
    "    Fed-Audit-GAN v2.0 with 4-Phase Architecture\n",
    "    - Phase 1: Local Client Training (with FedProx)\n",
    "    - Phase 2: GAN Training (Fairness Generator)\n",
    "    - Phase 3: Fairness Scoring (with Momentum/EMA)\n",
    "    - Phase 4: Fairness-Aware Aggregation\n",
    "    \n",
    "    This is DIFFERENT from FedAvg - uses GAN-based fairness auditing!\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"üü¢ RUNNING: Fed-Audit-GAN (Œ≥={gamma})\")\n",
    "    print(f\"   4-Phase: Local Training ‚Üí GAN ‚Üí Fairness Scoring ‚Üí Aggregation\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = CNN().to(device)\n",
    "    \n",
    "    # AMP scaler for CUDA\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    # Fairness score history for momentum\n",
    "    fairness_history = {i: 0.0 for i in range(n_clients)}\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'acc': [], 'bias': [], 'alphas': [],\n",
    "        'raw_scores': [], 'smoothed_scores': [],\n",
    "        'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=f\"Fed-Audit-GAN Œ≥={gamma}\"):\n",
    "        # ================================================================\n",
    "        # PHASE 1: Local Client Training (with FedProx)\n",
    "        # ================================================================\n",
    "        updates = []\n",
    "        global_params = [p.clone().detach() for p in model.parameters()]\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            before_state = copy.deepcopy(model.state_dict())\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            ce_loss = F.cross_entropy(output, target)\n",
    "                            # FedProx proximal term\n",
    "                            prox_loss = sum(((lp - gp) ** 2).sum() \n",
    "                                          for lp, gp in zip(local_model.parameters(), global_params))\n",
    "                            loss = ce_loss + (mu / 2) * prox_loss\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        ce_loss = F.cross_entropy(output, target)\n",
    "                        prox_loss = sum(((lp - gp) ** 2).sum() \n",
    "                                      for lp, gp in zip(local_model.parameters(), global_params))\n",
    "                        loss = ce_loss + (mu / 2) * prox_loss\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            update = {k: local_model.state_dict()[k] - before_state[k] for k in before_state}\n",
    "            updates.append(update)\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 2: GAN Training (Fairness Generator)\n",
    "        # ================================================================\n",
    "        G = FairnessGenerator(img_shape=(3, 32, 32)).to(device)\n",
    "        D = Discriminator(img_shape=(3, 32, 32)).to(device)\n",
    "        G, D = train_gan(G, D, model, val_loader, epochs=n_gan_epochs, device=device)\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 3: Fairness Scoring (with Momentum/EMA)\n",
    "        # ================================================================\n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_probes, G.latent_dim, device=device)\n",
    "            labels = torch.randint(0, 10, (n_probes,), device=device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x_probe, xp_probe = G(z, labels)\n",
    "            else:\n",
    "                x_probe, xp_probe = G(z, labels)\n",
    "        \n",
    "        B_base = compute_bias(model, x_probe, xp_probe, device)\n",
    "        \n",
    "        S_fair_raw = []\n",
    "        S_fair_smoothed = []\n",
    "        \n",
    "        for cid, upd in enumerate(updates):\n",
    "            hyp_model = copy.deepcopy(model)\n",
    "            hyp_state = hyp_model.state_dict()\n",
    "            for k in hyp_state:\n",
    "                hyp_state[k] = hyp_state[k] + upd[k]\n",
    "            hyp_model.load_state_dict(hyp_state)\n",
    "            \n",
    "            B_client = compute_bias(hyp_model, x_probe, xp_probe, device)\n",
    "            S_current = B_base - B_client\n",
    "            S_fair_raw.append(S_current)\n",
    "            \n",
    "            # Apply momentum (EMA smoothing)\n",
    "            S_prev = fairness_history[cid]\n",
    "            S_smoothed = (momentum * S_prev) + ((1 - momentum) * S_current)\n",
    "            fairness_history[cid] = S_smoothed\n",
    "            S_fair_smoothed.append(S_smoothed)\n",
    "            del hyp_model\n",
    "        \n",
    "        del G, D, x_probe, xp_probe\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        history['raw_scores'].append(S_fair_raw.copy())\n",
    "        history['smoothed_scores'].append(S_fair_smoothed.copy())\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 4: Fairness-Aware Aggregation\n",
    "        # ================================================================\n",
    "        if rnd < warmup_rounds:\n",
    "            # During warm-up: use data-weighted averaging (like FedAvg)\n",
    "            alphas = client_data_weights.copy()\n",
    "        else:\n",
    "            # After warm-up: use fairness-weighted aggregation\n",
    "            alphas = F.softmax(torch.tensor(S_fair_smoothed) * gamma, dim=0).tolist()\n",
    "        \n",
    "        new_state = model.state_dict()\n",
    "        for k in new_state:\n",
    "            new_state[k] = new_state[k] + sum(a * u[k] for a, u in zip(alphas, updates))\n",
    "        model.load_state_dict(new_state)\n",
    "        \n",
    "        # ================================================================\n",
    "        # EVALUATION\n",
    "        # ================================================================\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['bias'].append(B_base)\n",
    "        history['alphas'].append(alphas.copy())\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'bias': B_base,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN ALL EXPERIMENTS\n",
    "# ============================================================\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 1: FedAvg (SEPARATE BASELINE)\n",
    "# ============================================================\n",
    "wandb.init(\n",
    "    project=\"FED_AUDIT_GAN_TEST_2_CIFAR10\",\n",
    "    name=f\"FedAvg_CIFAR10_clients{N_CLIENTS}_GPU_pathological\",\n",
    "    config={\n",
    "        \"method\": \"FedAvg\",\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "        \"n_rounds\": N_ROUNDS,\n",
    "        \"n_clients\": N_CLIENTS,\n",
    "        \"non_iid_type\": \"pathological\",\n",
    "        \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "        \"device\": str(DEVICE),\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"amp_enabled\": USE_AMP\n",
    "    }\n",
    ")\n",
    "\n",
    "fedavg_model, fedavg_history = run_fedavg(\n",
    "    n_rounds=N_ROUNDS,\n",
    "    n_clients=N_CLIENTS,\n",
    "    train_data=train_data,\n",
    "    client_idx=client_idx,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    client_loaders=client_loaders,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    device=DEVICE,\n",
    "    use_amp=USE_AMP,\n",
    "    client_data_weights=CLIENT_DATA_WEIGHTS\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "all_results['FedAvg'] = {\n",
    "    'model': fedavg_model,\n",
    "    'history': fedavg_history,\n",
    "    'name': 'FedAvg'\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ FedAvg Complete!\")\n",
    "print(f\"   Final Accuracy: {fedavg_history['acc'][-1]:.2f}%\")\n",
    "print(f\"   Final JFI: {fedavg_history['jfi'][-1]:.4f}\")\n",
    "print(f\"   Accuracy Gap: {fedavg_history['accuracy_gap'][-1]:.2f}%\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 2 & 3: Fed-Audit-GAN with Œ≥ = 0.3, 0.7\n",
    "# ============================================================\n",
    "GAMMA_VALUES_FED_AUDIT = [0.3, 0.7]  # Only Fed-Audit-GAN gamma values (NOT 0.0!)\n",
    "\n",
    "for gamma in GAMMA_VALUES_FED_AUDIT:\n",
    "    method_name = f\"FedAuditGAN_Œ≥={gamma}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"FED_AUDIT_GAN_TEST_2_CIFAR10\",\n",
    "        name=f\"{method_name}_CIFAR10_clients{N_CLIENTS}_GPU_pathological\",\n",
    "        config={\n",
    "            \"method\": method_name,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"n_rounds\": N_ROUNDS,\n",
    "            \"n_clients\": N_CLIENTS,\n",
    "            \"gamma\": gamma,\n",
    "            \"momentum\": MOMENTUM,\n",
    "            \"warmup_rounds\": WARMUP_ROUNDS,\n",
    "            \"mu_fedprox\": MU,\n",
    "            \"non_iid_type\": \"pathological\",\n",
    "            \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "            \"device\": str(DEVICE),\n",
    "            \"num_gpus\": NUM_GPUS,\n",
    "            \"amp_enabled\": USE_AMP\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model, history = run_fed_audit_gan(\n",
    "        gamma=gamma,\n",
    "        n_rounds=N_ROUNDS,\n",
    "        n_clients=N_CLIENTS,\n",
    "        warmup_rounds=WARMUP_ROUNDS,\n",
    "        momentum=MOMENTUM,\n",
    "        mu=MU,\n",
    "        train_data=train_data,\n",
    "        client_idx=client_idx,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        client_loaders=client_loaders,\n",
    "        n_gan_epochs=N_GAN_EPOCHS,\n",
    "        n_probes=N_PROBES,\n",
    "        local_epochs=LOCAL_EPOCHS,\n",
    "        device=DEVICE,\n",
    "        use_amp=USE_AMP,\n",
    "        client_data_weights=CLIENT_DATA_WEIGHTS\n",
    "    )\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    all_results[method_name] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'name': method_name\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {method_name} Complete!\")\n",
    "    print(f\"   Final Accuracy: {history['acc'][-1]:.2f}%\")\n",
    "    print(f\"   Final JFI: {history['jfi'][-1]:.4f}\")\n",
    "    print(f\"   Accuracy Gap: {history['accuracy_gap'][-1]:.2f}%\")\n",
    "    print(f\"   Final Bias: {history['bias'][-1]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä Experiments Run:\")\n",
    "print(\"   1. FedAvg (Separate Baseline)\")\n",
    "print(\"   2. Fed-Audit-GAN Œ≥=0.3\")\n",
    "print(\"   3. Fed-Audit-GAN Œ≥=0.7\")\n",
    "print(\"üìä Check your WandB dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d32b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä RESULTS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"üìä CIFAR-10 ABLATION STUDY: FedAvg vs Fed-Audit-GAN (Kaggle GPU T4 x2)\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "print(f\"\\n{'METHOD':<25} {'GLOBAL ACC':<12} {'JFI':<10} {'MAX-MIN':<10} {'GAP':<10} {'MIN ACC':<10} {'MAX ACC':<10}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "# Get all method names\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "best_acc = max(all_results[m]['history']['acc'][-1] for m in method_names)\n",
    "best_jfi = max(all_results[m]['history']['jfi'][-1] for m in method_names)\n",
    "lowest_gap = min(all_results[m]['history']['accuracy_gap'][-1] for m in method_names)\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    max_min = all_results[method]['history']['max_min_fairness'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    min_acc = all_results[method]['history']['min_client_acc'][-1]\n",
    "    max_acc = all_results[method]['history']['max_client_acc'][-1]\n",
    "    \n",
    "    acc_mark = \"üèÜ\" if acc == best_acc else \"\"\n",
    "    jfi_mark = \"‚≠ê\" if jfi == best_jfi else \"\"\n",
    "    gap_mark = \"‚úÖ\" if gap == lowest_gap else \"\"\n",
    "    \n",
    "    print(f\"{name:<25} {acc:>8.2f}% {acc_mark:<2} {jfi:>8.4f} {jfi_mark:<2} {max_min:>8.4f}   {gap:>6.2f}% {gap_mark:<2} {min_acc:>8.2f}%  {max_acc:>8.2f}%\")\n",
    "\n",
    "print(\"=\" * 110)\n",
    "\n",
    "# Improvement over FedAvg\n",
    "fedavg_acc = all_results['FedAvg']['history']['acc'][-1]\n",
    "fedavg_jfi = all_results['FedAvg']['history']['jfi'][-1]\n",
    "fedavg_gap = all_results['FedAvg']['history']['accuracy_gap'][-1]\n",
    "\n",
    "print(f\"\\nüìà IMPROVEMENT OVER FedAvg (Separate Baseline):\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    \n",
    "    acc_diff = acc - fedavg_acc\n",
    "    jfi_diff = jfi - fedavg_jfi\n",
    "    gap_reduction = fedavg_gap - gap\n",
    "    \n",
    "    print(f\"   {name}:\")\n",
    "    print(f\"      Accuracy: {'+' if acc_diff >= 0 else ''}{acc_diff:.2f}%\")\n",
    "    print(f\"      JFI: {'+' if jfi_diff >= 0 else ''}{jfi_diff:.4f}\")\n",
    "    print(f\"      Gap Reduction: {gap_reduction:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c88b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä COMPREHENSIVE VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "# Colors for each method\n",
    "colors = {\n",
    "    'FedAvg': '#e74c3c',              # Red - FedAvg (Baseline)\n",
    "    'FedAuditGAN_Œ≥=0.3': '#3498db',   # Blue\n",
    "    'FedAuditGAN_Œ≥=0.7': '#2ecc71',   # Green\n",
    "}\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "rounds = range(1, N_ROUNDS + 1)\n",
    "\n",
    "# Plot 1: Global Accuracy\n",
    "ax = axes[0, 0]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    linewidth = 2.5 if method == 'FedAvg' else 2\n",
    "    ax.plot(rounds, acc, color=colors[method], linestyle=linestyle, \n",
    "            linewidth=linewidth, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='gray', label='Warm-up')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('CIFAR-10: Global Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: JFI\n",
    "ax = axes[0, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    jfi = all_results[method]['history']['jfi']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, jfi, color=colors[method], linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='gray')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('JFI', fontsize=12)\n",
    "ax.set_title(\"Jain's Fairness Index (Higher = Fairer)\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy Gap\n",
    "ax = axes[0, 2]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    gap = all_results[method]['history']['accuracy_gap']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, gap, color=colors[method], linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='gray')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy Gap (%)', fontsize=12)\n",
    "ax.set_title('Best-Worst Client Gap (Lower = Fairer)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Variance\n",
    "ax = axes[1, 0]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    var = all_results[method]['history']['variance']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, var, color=colors[method], linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='gray')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Variance', fontsize=12)\n",
    "ax.set_title('Per-Client Accuracy Variance (Lower = Fairer)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Min-Max Range\n",
    "ax = axes[1, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    min_acc = all_results[method]['history']['min_client_acc']\n",
    "    max_acc = all_results[method]['history']['max_client_acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.fill_between(rounds, min_acc, max_acc, color=colors[method], alpha=0.2)\n",
    "    ax.plot(rounds, min_acc, color=colors[method], linestyle=linestyle, linewidth=1.5)\n",
    "    ax.plot(rounds, max_acc, color=colors[method], linestyle=linestyle, linewidth=1.5, label=name)\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Min-Max Client Accuracy Range', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final Per-Client Accuracy\n",
    "ax = axes[1, 2]\n",
    "x = np.arange(N_CLIENTS)\n",
    "width = 0.25\n",
    "for i, method in enumerate(method_names):\n",
    "    name = all_results[method]['name']\n",
    "    client_accs = all_results[method]['history']['client_accuracies'][-1]\n",
    "    ax.bar(x + i*width, client_accs, width, label=name, color=colors[method], alpha=0.8)\n",
    "ax.set_xlabel('Client ID', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Per-Client Accuracy (Final Round)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cifar10_fedavg_vs_fed_audit_gan_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìÅ Results saved to: cifar10_fedavg_vs_fed_audit_gan_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a95feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE ALL MODELS AND RESULTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs('results_cifar10_v2_gpu', exist_ok=True)\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    filename = f\"results_cifar10_v2_gpu/{name.replace('=', '').replace('.', '_').replace('Œ≥', 'gamma')}_CIFAR10.pth\"\n",
    "    \n",
    "    save_dict = {\n",
    "        'model_state_dict': all_results[method]['model'].state_dict(),\n",
    "        'history': all_results[method]['history'],\n",
    "        'config': {\n",
    "            'n_rounds': N_ROUNDS,\n",
    "            'n_clients': N_CLIENTS,\n",
    "            'classes_per_client': CLASSES_PER_CLIENT,\n",
    "            'device': str(DEVICE),\n",
    "            'num_gpus': NUM_GPUS\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add Fed-Audit-GAN specific config\n",
    "    if method != 'FedAvg':\n",
    "        save_dict['config']['momentum'] = MOMENTUM\n",
    "        save_dict['config']['warmup_rounds'] = WARMUP_ROUNDS\n",
    "        save_dict['config']['mu'] = MU\n",
    "    \n",
    "    torch.save(save_dict, filename)\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "\n",
    "with open('results_cifar10_v2_gpu/all_results_summary.pkl', 'wb') as f:\n",
    "    summary = {\n",
    "        method: {\n",
    "            'name': all_results[method]['name'],\n",
    "            'history': all_results[method]['history'],\n",
    "            'final_acc': all_results[method]['history']['acc'][-1],\n",
    "            'final_jfi': all_results[method]['history']['jfi'][-1]\n",
    "        }\n",
    "        for method in method_names\n",
    "    }\n",
    "    pickle.dump(summary, f)\n",
    "print(\"‚úÖ Saved: results_cifar10_v2_gpu/all_results_summary.pkl\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üîµ BASELINE:\")\n",
    "print(f\"   FedAvg: {all_results['FedAvg']['history']['acc'][-1]:.2f}% accuracy, JFI={all_results['FedAvg']['history']['jfi'][-1]:.4f}\")\n",
    "print(\"\\nüü¢ FED-AUDIT-GAN:\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    print(f\"   {name}: {acc:.2f}% accuracy, JFI={jfi:.4f}, Gap={gap:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Check WandB dashboard: https://wandb.ai\")\n",
    "print(f\"   Trained on: {NUM_GPUS} GPU(s) with AMP={USE_AMP}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
