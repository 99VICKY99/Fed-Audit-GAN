{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebb15712",
   "metadata": {},
   "source": [
    "# ðŸ”§ Fed-Audit-GAN v4.0 - CIFAR-10 (V2 Linear Formula) - Kaggle GPU Edition\n",
    "\n",
    "## ðŸŽ¯ Experiments Run (SEPARATE IMPLEMENTATIONS):\n",
    "\n",
    "### ðŸ”µ Baseline: FedAvg (Federated Averaging)\n",
    "- **Completely separate algorithm** (NOT gamma=0!)\n",
    "- Standard data-weighted averaging (McMahan et al., 2017)\n",
    "- NO GAN, NO fairness scoring, NO FedProx\n",
    "\n",
    "### ðŸŸ¢ Our Method: Fed-Audit-GAN (with V2 Linear Formula!)\n",
    "- **Fed-Audit-GAN Î³ = 0.3** - Accuracy-weighted fairness\n",
    "- **Fed-Audit-GAN Î³ = 0.7** - Fairness-weighted aggregation\n",
    "\n",
    "## ðŸ”§ Fed-Audit-GAN 4-Phase Architecture:\n",
    "1. **Phase 1**: Local Client Training (with FedProx + Gradient Clipping)\n",
    "2. **Phase 2**: GAN Training (with Soft Labels for stability)\n",
    "3. **Phase 3**: Fairness Scoring (with Momentum/EMA)\n",
    "4. **Phase 4**: V2 Linear Aggregation (NEW! - Min-Max Normalized)\n",
    "\n",
    "## â­ KEY CHANGE: V2 Linear Aggregation Formula\n",
    "\n",
    "### The Problem with V3 (Softmax):\n",
    "- If scores are tiny (e.g., 0.001), Softmax(Î³ * 0.001) â‰ˆ 1/N for any Î³\n",
    "- Gamma effectively does nothing!\n",
    "\n",
    "### The V2 Linear Solution:\n",
    "```\n",
    "Weight = (1 - Î³) Ã— Accuracy_norm + Î³ Ã— Fairness_norm\n",
    "```\n",
    "- **Î³ = 0.0** â†’ Only Data Size (Accuracy proxy) matters\n",
    "- **Î³ = 1.0** â†’ Only Fairness matters\n",
    "- Changing Î³ MUST change the outcome!\n",
    "\n",
    "### Critical: Min-Max Normalization\n",
    "- Both Accuracy and Fairness scores normalized to [0, 1]\n",
    "- Prevents one score from dominating\n",
    "\n",
    "## â­ PATHOLOGICAL NON-IID (2 Classes + Unequal Sizes):\n",
    "- Each client ONLY has 2 out of 10 classes\n",
    "- Clients have DIFFERENT sample sizes (Dirichlet distribution)\n",
    "- Creates EXTREME fairness challenges\n",
    "\n",
    "## ðŸ“Š Expected Results:\n",
    "- âœ… Î³ = 0.3 and Î³ = 0.7 should produce DIFFERENT results!\n",
    "- âœ… Smooth accuracy curve (no zig-zag)\n",
    "- âœ… Clear gamma sensitivity demonstrated\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies (Kaggle GPU Edition)\n",
    "!pip install -q torch torchvision tqdm matplotlib numpy wandb\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9650c0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"âœ… WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504aec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Imports and GPU Setup (Kaggle T4 x2)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# ðŸš€ GPU SETUP (Kaggle T4 x2)\n",
    "# ============================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    \n",
    "    # Enable cuDNN optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # Enable TF32 for faster matrix operations on Ampere+ GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"âœ… GPU(s) detected: {NUM_GPUS}\")\n",
    "    for i in range(NUM_GPUS):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"   GPU {i}: {props.name}\")\n",
    "        print(f\"      Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"      Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    NUM_GPUS = 0\n",
    "    print(\"âš ï¸  No GPU detected. Using CPU (training will be slow).\")\n",
    "\n",
    "# Mixed Precision Training\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "if USE_AMP:\n",
    "    print(\"\\nâœ… Mixed Precision Training (AMP) enabled.\")\n",
    "\n",
    "print(f\"\\nðŸ“ Device: {DEVICE}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   GPUs available: {NUM_GPUS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc5c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEFINITIONS (CIFAR-10 - Deeper CNN)\n",
    "# ============================================================\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"Deeper CNN for CIFAR-10 classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32->16\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16->8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8->4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 4->2\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    \"\"\"Generator that produces paired samples (x, x') for fairness testing\"\"\"\n",
    "    def __init__(self, latent_dim=128, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4  # 8\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 512 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input).view(-1, 512, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        return x, torch.clamp(x + delta, -1, 1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Conditional Discriminator - outputs logits for BCEWithLogitsLoss\"\"\"\n",
    "    def __init__(self, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 1)  # No Sigmoid\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "\n",
    "print(\"âœ… Models defined: CNN, FairnessGenerator, Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd378f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS (GPU Optimized with AMP)\n",
    "# â­ FIX 4: SOFT LABELS FOR GAN (Label Smoothing)\n",
    "# ============================================================\n",
    "\n",
    "def train_gan(G, D, model, loader, epochs=20, device='cuda', l1=1.0, l2=1.0):\n",
    "    \"\"\"\n",
    "    Train the Fairness GAN with Mixed Precision\n",
    "    \n",
    "    â­ FIX 4: Uses SOFT LABELS instead of hard 0/1 labels\n",
    "    - Real labels: uniform(0.9, 1.0) instead of 1.0\n",
    "    - Fake labels: uniform(0.0, 0.1) instead of 0.0\n",
    "    - Prevents Discriminator from becoming too confident (mode collapse)\n",
    "    \"\"\"\n",
    "    G, D, model = G.to(device), D.to(device), model.to(device)\n",
    "    model.eval()\n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # AMP scalers\n",
    "    if USE_AMP:\n",
    "        scaler_G = torch.amp.GradScaler(device='cuda')\n",
    "        scaler_D = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            bs = imgs.size(0)\n",
    "            \n",
    "            # â­ FIX 4: SOFT LABELS (Label Smoothing)\n",
    "            real_labels = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "            fake_labels = torch.empty(bs, 1, device=device).uniform_(0.0, 0.1)\n",
    "            \n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            z = torch.randn(bs, G.latent_dim, device=device)\n",
    "            gl = torch.randint(0, G.num_classes, (bs,), device=device)\n",
    "            \n",
    "            # Generator\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    with torch.no_grad():\n",
    "                        px, pxp = model(x), model(xp)\n",
    "                    t1 = -torch.mean((px - pxp) ** 2)\n",
    "                    t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                    g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                    t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                    g_loss = t1 + t2 + t3\n",
    "                scaler_G.scale(g_loss).backward()\n",
    "                scaler_G.step(opt_G)\n",
    "                scaler_G.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                with torch.no_grad():\n",
    "                    px, pxp = model(x), model(xp)\n",
    "                t1 = -torch.mean((px - pxp) ** 2)\n",
    "                t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                g_loss = t1 + t2 + t3\n",
    "                g_loss.backward()\n",
    "                opt_G.step()\n",
    "            \n",
    "            # Discriminator\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                              bce(D(x.detach(), gl), fake_labels) + \n",
    "                              bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                scaler_D.scale(d_loss).backward()\n",
    "                scaler_D.step(opt_D)\n",
    "                scaler_D.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                          bce(D(x.detach(), gl), fake_labels) + \n",
    "                          bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                d_loss.backward()\n",
    "                opt_D.step()\n",
    "    \n",
    "    return G, D\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bias(model, x, xp, device):\n",
    "    \"\"\"Compute bias as difference in model predictions between x and x'\"\"\"\n",
    "    model.eval()\n",
    "    x, xp = x.to(device), xp.to(device)\n",
    "    \n",
    "    if USE_AMP:\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    else:\n",
    "        diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    \n",
    "    return diff.item()\n",
    "\n",
    "\n",
    "def partition_data_pathological_non_iid(dataset, n_clients, classes_per_client=2):\n",
    "    \"\"\"\n",
    "    â­ PATHOLOGICAL NON-IID: Each client gets ONLY 'classes_per_client' classes.\n",
    "    â­ UNEQUAL SAMPLE SIZES: Clients have different amounts of data.\n",
    "    \"\"\"\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    n_classes = len(np.unique(labels))  # 10 for CIFAR-10\n",
    "    \n",
    "    # Get indices for each class\n",
    "    class_indices = {c: np.where(labels == c)[0] for c in range(n_classes)}\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "    \n",
    "    # Assign classes to clients (cycling through with only 2 classes each)\n",
    "    client_classes = []\n",
    "    for cid in range(n_clients):\n",
    "        start_class = (cid * classes_per_client) % n_classes\n",
    "        assigned = []\n",
    "        for i in range(classes_per_client):\n",
    "            assigned.append((start_class + i) % n_classes)\n",
    "        client_classes.append(assigned)\n",
    "    \n",
    "    # Count how many clients share each class\n",
    "    class_client_count = {c: sum(1 for cc in client_classes if c in cc) for c in range(n_classes)}\n",
    "    \n",
    "    # â­ UNEQUAL SAMPLE SIZES: Use Dirichlet to create unequal data distribution\n",
    "    alpha = 0.5  # Lower alpha = more unequal distribution\n",
    "    client_proportions = np.random.dirichlet([alpha] * n_clients)\n",
    "    \n",
    "    # Distribute samples to clients with UNEQUAL sizes\n",
    "    client_indices = [[] for _ in range(n_clients)]\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        clients_with_class = [cid for cid in range(n_clients) if c in client_classes[cid]]\n",
    "        \n",
    "        if len(clients_with_class) > 0:\n",
    "            class_samples = class_indices[c]\n",
    "            total_for_class = len(class_samples)\n",
    "            \n",
    "            relevant_props = np.array([client_proportions[cid] for cid in clients_with_class])\n",
    "            relevant_props = relevant_props / relevant_props.sum()\n",
    "            \n",
    "            start_idx = 0\n",
    "            for i, cid in enumerate(clients_with_class):\n",
    "                if i == len(clients_with_class) - 1:\n",
    "                    end_idx = total_for_class\n",
    "                else:\n",
    "                    n_samples = int(total_for_class * relevant_props[i])\n",
    "                    end_idx = min(start_idx + n_samples, total_for_class)\n",
    "                \n",
    "                if start_idx < end_idx:\n",
    "                    client_indices[cid].extend(class_samples[start_idx:end_idx].tolist())\n",
    "                start_idx = end_idx\n",
    "    \n",
    "    # Shuffle each client's data and ensure minimum samples\n",
    "    result = []\n",
    "    for cid in range(n_clients):\n",
    "        if len(client_indices[cid]) > 0:\n",
    "            indices = np.array(client_indices[cid])\n",
    "            np.random.shuffle(indices)\n",
    "            result.append(indices)\n",
    "        else:\n",
    "            fallback_samples = np.random.choice(len(dataset), size=50, replace=False)\n",
    "            result.append(fallback_samples)\n",
    "    \n",
    "    return result, client_classes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model accuracy with AMP\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for d, t in loader:\n",
    "        d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                preds = model(d).argmax(1)\n",
    "        else:\n",
    "            preds = model(d).argmax(1)\n",
    "        \n",
    "        correct += (preds == t).sum().item()\n",
    "        total += len(t)\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_client(model, client_loaders, device):\n",
    "    \"\"\"Evaluate model accuracy on EACH client's data.\"\"\"\n",
    "    model.eval()\n",
    "    client_accuracies = []\n",
    "    \n",
    "    for loader in client_loaders:\n",
    "        correct, total = 0, 0\n",
    "        for d, t in loader:\n",
    "            d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "            \n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    preds = model(d).argmax(1)\n",
    "            else:\n",
    "                preds = model(d).argmax(1)\n",
    "            \n",
    "            correct += (preds == t).sum().item()\n",
    "            total += len(t)\n",
    "        \n",
    "        acc = 100 * correct / total if total > 0 else 0\n",
    "        client_accuracies.append(acc)\n",
    "    \n",
    "    return client_accuracies\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FAIRNESS METRICS (Based on Per-Client Performance!)\n",
    "# ============================================================\n",
    "\n",
    "print(\"   â­ FIX 4: GAN uses SOFT LABELS (0.9-1.0 for real, 0.0-0.1 for fake)\")\n",
    "print(\"âœ… Helper functions defined (2 classes per client, unequal sizes)\")\n",
    "\n",
    "def calculate_jfi(performances):\n",
    "    \"\"\"Jain's Fairness Index: (Î£páµ¢)Â² / (N Ã— Î£páµ¢Â²)\"\"\"\n",
    "    p = np.array(performances)\n",
    "    n = len(p)\n",
    "    if np.sum(p ** 2) == 0:\n",
    "        return 1.0\n",
    "    return (np.sum(p) ** 2) / (n * np.sum(p ** 2))\n",
    "\n",
    "def calculate_accuracy_gap(performances):\n",
    "    \"\"\"Accuracy gap: max(acc) - min(acc)\"\"\"\n",
    "    return np.max(performances) - np.min(performances)\n",
    "\n",
    "def calculate_variance(performances):\n",
    "    \"\"\"Variance of per-client accuracies\"\"\"\n",
    "    return np.var(performances)\n",
    "\n",
    "def calculate_max_min_fairness(performances):\n",
    "    \"\"\"Max-Min Fairness: min(acc) / max(acc)\"\"\"\n",
    "    p = np.array(performances)\n",
    "    if np.max(p) == 0:\n",
    "        return 0.0\n",
    "    return np.min(p) / np.max(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION (V4 - Linear Formula with Multiple Gammas)\n",
    "# ============================================================\n",
    "\n",
    "# Training Parameters\n",
    "N_ROUNDS = 50           # Total training rounds\n",
    "N_CLIENTS = 20          # Number of federated clients\n",
    "N_GAN_EPOCHS = 15       # GAN training epochs per round\n",
    "N_PROBES = 500          # Number of probe samples\n",
    "LOCAL_EPOCHS = 3        # Local training epochs per client\n",
    "\n",
    "# â­ Stability Fixes (kept from V3)\n",
    "WARMUP_ROUNDS = 10      # Extended warm-up\n",
    "GRAD_CLIP_NORM = 1.0    # Gradient clipping\n",
    "AUDIT_FREQUENCY = 2     # Audit every 2nd round\n",
    "\n",
    "# Fed-Audit-GAN Parameters\n",
    "MOMENTUM = 0.8          # EMA momentum for fairness scores\n",
    "MU = 0.01               # FedProx proximal term\n",
    "\n",
    "# â­ V4: Test MULTIPLE gamma values to prove they produce different results!\n",
    "GAMMA_VALUES = [0.3, 0.7]  # Low fairness vs High fairness\n",
    "\n",
    "# DataLoader Parameters\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "PREFETCH_FACTOR = 2\n",
    "\n",
    "# PATHOLOGICAL NON-IID\n",
    "CLASSES_PER_CLIENT = 2\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ”§ Fed-Audit-GAN v4.0 - CIFAR-10 (V2 LINEAR FORMULA)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPUs: {NUM_GPUS}\")\n",
    "print(f\"AMP Enabled: {USE_AMP}\")\n",
    "print(f\"Rounds: {N_ROUNDS}, Clients: {N_CLIENTS}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ EXPERIMENTS TO RUN:\")\n",
    "print(f\"   ðŸ”µ 1. FedAvg (SEPARATE BASELINE)\")\n",
    "for i, g in enumerate(GAMMA_VALUES, 2):\n",
    "    print(f\"   ðŸŸ¢ {i}. Fed-Audit-GAN Î³={g}\")\n",
    "\n",
    "print(f\"\\nâ­ V4 KEY CHANGE: LINEAR AGGREGATION FORMULA\")\n",
    "print(f\"   Weight = (1-Î³) Ã— Accuracy_norm + Î³ Ã— Fairness_norm\")\n",
    "print(f\"   âž¡ï¸ Changing Î³ MUST change the result!\")\n",
    "\n",
    "print(f\"\\nâ­ PATHOLOGICAL NON-IID (UNEQUAL SIZES):\")\n",
    "print(f\"   Each client gets ONLY {CLASSES_PER_CLIENT}/10 classes\")\n",
    "print(f\"   Clients have DIFFERENT sample sizes (Dirichlet Î±=0.5)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe5206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING (CIFAR-10 with GPU Optimizations)\n",
    "# ============================================================\n",
    "\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create PATHOLOGICAL Non-IID partitions\n",
    "np.random.seed(42)\n",
    "client_idx, client_classes = partition_data_pathological_non_iid(\n",
    "    train_data, N_CLIENTS, classes_per_client=CLASSES_PER_CLIENT\n",
    ")\n",
    "\n",
    "# Calculate data weights for each client\n",
    "client_data_sizes = [len(idx) for idx in client_idx]\n",
    "total_samples = sum(client_data_sizes)\n",
    "CLIENT_DATA_WEIGHTS = [size / total_samples for size in client_data_sizes]\n",
    "\n",
    "# DataLoader kwargs\n",
    "dataloader_kwargs = {\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY,\n",
    "    'persistent_workers': True,\n",
    "    'prefetch_factor': PREFETCH_FACTOR,\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=VAL_BATCH_SIZE, shuffle=False, **dataloader_kwargs)\n",
    "val_loader = DataLoader(\n",
    "    Subset(train_data, np.random.choice(len(train_data), 2000, replace=False)),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **dataloader_kwargs\n",
    ")\n",
    "\n",
    "client_loaders = [\n",
    "    DataLoader(Subset(train_data, client_idx[c]), batch_size=BATCH_SIZE, shuffle=True, **dataloader_kwargs)\n",
    "    for c in range(N_CLIENTS)\n",
    "]\n",
    "\n",
    "print(f\"âœ… Data loaded!\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"â­ PATHOLOGICAL NON-IID + UNEQUAL SIZES ({CLASSES_PER_CLIENT} classes per client)\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(min(10, N_CLIENTS)):\n",
    "    class_names = [CIFAR10_CLASSES[c] for c in client_classes[i]]\n",
    "    print(f\"   Client {i:2d}: {client_classes[i]} â†’ {class_names} ({client_data_sizes[i]} samples)\")\n",
    "\n",
    "if N_CLIENTS > 10:\n",
    "    print(f\"   ... ({N_CLIENTS - 10} more clients)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸš€ SEPARATE IMPLEMENTATIONS: FedAvg vs Fed-Audit-GAN v4.0\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# PART A: FedAvg (Federated Averaging) - SEPARATE BASELINE\n",
    "# ============================================================\n",
    "\n",
    "def run_fedavg(n_rounds, n_clients, train_data, client_idx, val_loader, test_loader, \n",
    "               client_loaders, local_epochs, device, use_amp, client_data_weights):\n",
    "    \"\"\"\n",
    "    Standard FedAvg (McMahan et al., 2017)\n",
    "    - No GAN, no fairness scoring\n",
    "    - Pure data-weighted averaging of model updates\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸ”µ RUNNING: FedAvg (Federated Averaging) - BASELINE\")\n",
    "    print(\"   This is a SEPARATE algorithm, NOT gamma=0 of Fed-Audit-GAN!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=\"FedAvg\"):\n",
    "        local_weights = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            loss = criterion(output, target)\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            local_weights.append(copy.deepcopy(local_model.state_dict()))\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Data-Weighted Averaging\n",
    "        avg_weights = copy.deepcopy(local_weights[0])\n",
    "        for key in avg_weights.keys():\n",
    "            avg_weights[key] = avg_weights[key] * client_data_weights[0]\n",
    "            for i in range(1, len(local_weights)):\n",
    "                avg_weights[key] += local_weights[i][key] * client_data_weights[i]\n",
    "        \n",
    "        model.load_state_dict(avg_weights)\n",
    "        \n",
    "        # Evaluation\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PART B: Fed-Audit-GAN v4.0 (V2 LINEAR FORMULA!)\n",
    "# ============================================================\n",
    "\n",
    "def run_fed_audit_gan_v4(gamma, n_rounds, n_clients, warmup_rounds, momentum, mu,\n",
    "                         grad_clip_norm, audit_frequency,\n",
    "                         train_data, client_idx, val_loader, test_loader, client_loaders,\n",
    "                         n_gan_epochs, n_probes, local_epochs, device, use_amp,\n",
    "                         client_data_weights):\n",
    "    \"\"\"\n",
    "    Fed-Audit-GAN v4.0 with V2 LINEAR AGGREGATION FORMULA\n",
    "    \n",
    "    â­ KEY CHANGE: Uses Linear Formula instead of Softmax!\n",
    "    Weight = (1 - Î³) Ã— Accuracy_norm + Î³ Ã— Fairness_norm\n",
    "    \n",
    "    This GUARANTEES that changing gamma changes the result!\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ðŸŸ¢ RUNNING: Fed-Audit-GAN v4.0 (Î³={gamma}) - V2 LINEAR FORMULA\")\n",
    "    print(f\"   â­ Weight = (1-Î³) Ã— Accuracy_norm + Î³ Ã— Fairness_norm\")\n",
    "    print(f\"   â­ Î³ = 0.0 â†’ Only Data Size matters\")\n",
    "    print(f\"   â­ Î³ = 1.0 â†’ Only Fairness matters\")\n",
    "    print(f\"   â­ Current Î³ = {gamma}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    fairness_history = {i: 0.0 for i in range(n_clients)}\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'bias': [], 'alphas': [],\n",
    "        'raw_scores': [], 'smoothed_scores': [],\n",
    "        'normalized_fairness': [], 'normalized_accuracy': [],  # NEW: Track normalized scores\n",
    "        'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': [],\n",
    "        'audit_active': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=f\"Fed-Audit-GAN Î³={gamma}\"):\n",
    "        \n",
    "        # Determine if we should audit this round\n",
    "        should_audit = (rnd >= warmup_rounds) and (rnd % audit_frequency == 0)\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 1: Local Client Training (with FedProx + GRADIENT CLIPPING)\n",
    "        # ================================================================\n",
    "        updates = []\n",
    "        global_params = [p.clone().detach() for p in model.parameters()]\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            before_state = copy.deepcopy(model.state_dict())\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            ce_loss = F.cross_entropy(output, target)\n",
    "                            prox_loss = sum(((lp - gp) ** 2).sum() \n",
    "                                          for lp, gp in zip(local_model.parameters(), global_params))\n",
    "                            loss = ce_loss + (mu / 2) * prox_loss\n",
    "                        scaler.scale(loss).backward()\n",
    "                        \n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=grad_clip_norm)\n",
    "                        \n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        ce_loss = F.cross_entropy(output, target)\n",
    "                        prox_loss = sum(((lp - gp) ** 2).sum() \n",
    "                                      for lp, gp in zip(local_model.parameters(), global_params))\n",
    "                        loss = ce_loss + (mu / 2) * prox_loss\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=grad_clip_norm)\n",
    "                        \n",
    "                        optimizer.step()\n",
    "            \n",
    "            update = {k: local_model.state_dict()[k] - before_state[k] for k in before_state}\n",
    "            updates.append(update)\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 2 & 3: GAN Training + Fairness Scoring\n",
    "        # ================================================================\n",
    "        B_base = 0.0\n",
    "        S_fair_raw = [0.0] * n_clients\n",
    "        S_fair_smoothed = [0.0] * n_clients\n",
    "        \n",
    "        if should_audit:\n",
    "            G = FairnessGenerator(img_shape=(3, 32, 32)).to(device)\n",
    "            D = Discriminator(img_shape=(3, 32, 32)).to(device)\n",
    "            G, D = train_gan(G, D, model, val_loader, epochs=n_gan_epochs, device=device)\n",
    "            \n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(n_probes, G.latent_dim, device=device)\n",
    "                labels = torch.randint(0, 10, (n_probes,), device=device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        x_probe, xp_probe = G(z, labels)\n",
    "                else:\n",
    "                    x_probe, xp_probe = G(z, labels)\n",
    "            \n",
    "            B_base = compute_bias(model, x_probe, xp_probe, device)\n",
    "            \n",
    "            S_fair_raw = []\n",
    "            S_fair_smoothed = []\n",
    "            \n",
    "            for cid, upd in enumerate(updates):\n",
    "                hyp_model = copy.deepcopy(model)\n",
    "                hyp_state = hyp_model.state_dict()\n",
    "                for k in hyp_state:\n",
    "                    hyp_state[k] = hyp_state[k] + upd[k]\n",
    "                hyp_model.load_state_dict(hyp_state)\n",
    "                \n",
    "                B_client = compute_bias(hyp_model, x_probe, xp_probe, device)\n",
    "                S_current = B_base - B_client\n",
    "                S_fair_raw.append(S_current)\n",
    "                \n",
    "                # Apply momentum (EMA smoothing)\n",
    "                S_prev = fairness_history[cid]\n",
    "                S_smoothed = (momentum * S_prev) + ((1 - momentum) * S_current)\n",
    "                fairness_history[cid] = S_smoothed\n",
    "                S_fair_smoothed.append(S_smoothed)\n",
    "                del hyp_model\n",
    "            \n",
    "            del G, D, x_probe, xp_probe\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            S_fair_smoothed = [fairness_history[i] for i in range(n_clients)]\n",
    "        \n",
    "        history['raw_scores'].append(S_fair_raw.copy())\n",
    "        history['smoothed_scores'].append(S_fair_smoothed.copy())\n",
    "        history['audit_active'].append(should_audit)\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 4: V2 LINEAR AGGREGATION (THE KEY CHANGE!)\n",
    "        # ================================================================\n",
    "        if rnd < warmup_rounds:\n",
    "            # During warm-up, use pure FedAvg (data-weighted)\n",
    "            alphas = client_data_weights.copy()\n",
    "            f_norm_list = [0.5] * n_clients  # Placeholder\n",
    "            a_norm_list = client_data_weights.copy()\n",
    "        else:\n",
    "            # â­ V2 LINEAR FORMULA with Min-Max Normalization\n",
    "            \n",
    "            # 1. Normalize Fairness Score (Min-Max Normalization)\n",
    "            f_tensor = torch.tensor(S_fair_smoothed, device=device, dtype=torch.float32)\n",
    "            f_min, f_max = f_tensor.min(), f_tensor.max()\n",
    "            if f_max != f_min:\n",
    "                f_norm = (f_tensor - f_min) / (f_max - f_min)\n",
    "            else:\n",
    "                f_norm = torch.ones_like(f_tensor) * 0.5  # Default if all equal\n",
    "            \n",
    "            # 2. Normalize Accuracy Score (Using client data weights as accuracy proxy)\n",
    "            a_tensor = torch.tensor(client_data_weights, device=device, dtype=torch.float32)\n",
    "            a_min, a_max = a_tensor.min(), a_tensor.max()\n",
    "            if a_max != a_min:\n",
    "                a_norm = (a_tensor - a_min) / (a_max - a_min)\n",
    "            else:\n",
    "                a_norm = torch.ones_like(a_tensor) * 0.5\n",
    "            \n",
    "            # 3. V2 LINEAR FORMULA:\n",
    "            # Weight = (1 - gamma) * Accuracy_norm + gamma * Fairness_norm\n",
    "            # gamma = 0.0 â†’ Only data size matters\n",
    "            # gamma = 1.0 â†’ Only fairness matters\n",
    "            raw_weights = ((1 - gamma) * a_norm) + (gamma * f_norm) + 1e-8\n",
    "            \n",
    "            # 4. Final Normalization (Ensure sum = 1.0)\n",
    "            alphas = (raw_weights / raw_weights.sum()).tolist()\n",
    "            \n",
    "            # Store normalized scores for debugging\n",
    "            f_norm_list = f_norm.tolist()\n",
    "            a_norm_list = a_norm.tolist()\n",
    "            \n",
    "            # Debug output (every 10 rounds)\n",
    "            if rnd % 10 == 0:\n",
    "                print(f\"\\n   ðŸ“Š Round {rnd+1} Debug (Î³={gamma}):\")\n",
    "                print(f\"      Fairness scores (raw): min={min(S_fair_smoothed):.4f}, max={max(S_fair_smoothed):.4f}\")\n",
    "                print(f\"      Fairness normalized: min={min(f_norm_list):.4f}, max={max(f_norm_list):.4f}\")\n",
    "                print(f\"      Accuracy normalized: min={min(a_norm_list):.4f}, max={max(a_norm_list):.4f}\")\n",
    "                print(f\"      Final alphas: min={min(alphas):.4f}, max={max(alphas):.4f}, sum={sum(alphas):.4f}\")\n",
    "        \n",
    "        history['normalized_fairness'].append(f_norm_list.copy() if 'f_norm_list' in dir() else [0.5]*n_clients)\n",
    "        history['normalized_accuracy'].append(a_norm_list.copy() if 'a_norm_list' in dir() else client_data_weights.copy())\n",
    "        \n",
    "        # Apply weighted aggregation\n",
    "        new_state = model.state_dict()\n",
    "        for k in new_state:\n",
    "            new_state[k] = new_state[k] + sum(a * u[k] for a, u in zip(alphas, updates))\n",
    "        model.load_state_dict(new_state)\n",
    "        \n",
    "        # ================================================================\n",
    "        # EVALUATION\n",
    "        # ================================================================\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['bias'].append(B_base)\n",
    "        history['alphas'].append(alphas.copy())\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'bias': B_base,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs),\n",
    "            'audit_active': int(should_audit),\n",
    "            'alpha_min': min(alphas),\n",
    "            'alpha_max': max(alphas),\n",
    "            'alpha_std': np.std(alphas)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "print(\"âœ… Training functions defined (FedAvg + Fed-Audit-GAN v4.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551581d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN ALL EXPERIMENTS\n",
    "# ============================================================\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 1: FedAvg (SEPARATE BASELINE)\n",
    "# ============================================================\n",
    "wandb.init(\n",
    "    project=\"FED_AUDIT_GAN_TEST_4_CIFAR10\",\n",
    "    name=f\"FedAvg_CIFAR10_clients{N_CLIENTS}_v4\",\n",
    "    config={\n",
    "        \"method\": \"FedAvg\",\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "        \"n_rounds\": N_ROUNDS,\n",
    "        \"n_clients\": N_CLIENTS,\n",
    "        \"non_iid_type\": \"pathological\",\n",
    "        \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "        \"device\": str(DEVICE),\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"amp_enabled\": USE_AMP\n",
    "    }\n",
    ")\n",
    "\n",
    "fedavg_model, fedavg_history = run_fedavg(\n",
    "    n_rounds=N_ROUNDS,\n",
    "    n_clients=N_CLIENTS,\n",
    "    train_data=train_data,\n",
    "    client_idx=client_idx,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    client_loaders=client_loaders,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    device=DEVICE,\n",
    "    use_amp=USE_AMP,\n",
    "    client_data_weights=CLIENT_DATA_WEIGHTS\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "all_results['FedAvg'] = {\n",
    "    'model': fedavg_model,\n",
    "    'history': fedavg_history,\n",
    "    'name': 'FedAvg'\n",
    "}\n",
    "\n",
    "print(f\"âœ… FedAvg Complete!\")\n",
    "print(f\"   Final Accuracy: {fedavg_history['acc'][-1]:.2f}%\")\n",
    "print(f\"   Final JFI: {fedavg_history['jfi'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENTS 2-3: Fed-Audit-GAN v4.0 with Î³ = 0.3 and 0.7\n",
    "# ============================================================\n",
    "for gamma in GAMMA_VALUES:\n",
    "    method_name = f\"FedAuditGAN_v4_Î³={gamma}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"FED_AUDIT_GAN_TEST_4_CIFAR10\",\n",
    "        name=f\"{method_name}_CIFAR10_clients{N_CLIENTS}_linear\",\n",
    "        config={\n",
    "            \"method\": method_name,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"n_rounds\": N_ROUNDS,\n",
    "            \"n_clients\": N_CLIENTS,\n",
    "            \"gamma\": gamma,\n",
    "            \"momentum\": MOMENTUM,\n",
    "            \"warmup_rounds\": WARMUP_ROUNDS,\n",
    "            \"mu_fedprox\": MU,\n",
    "            \"grad_clip_norm\": GRAD_CLIP_NORM,\n",
    "            \"audit_frequency\": AUDIT_FREQUENCY,\n",
    "            \"aggregation_method\": \"V2_LINEAR\",\n",
    "            \"non_iid_type\": \"pathological\",\n",
    "            \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "            \"device\": str(DEVICE),\n",
    "            \"num_gpus\": NUM_GPUS,\n",
    "            \"amp_enabled\": USE_AMP\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model, history = run_fed_audit_gan_v4(\n",
    "        gamma=gamma,\n",
    "        n_rounds=N_ROUNDS,\n",
    "        n_clients=N_CLIENTS,\n",
    "        warmup_rounds=WARMUP_ROUNDS,\n",
    "        momentum=MOMENTUM,\n",
    "        mu=MU,\n",
    "        grad_clip_norm=GRAD_CLIP_NORM,\n",
    "        audit_frequency=AUDIT_FREQUENCY,\n",
    "        train_data=train_data,\n",
    "        client_idx=client_idx,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        client_loaders=client_loaders,\n",
    "        n_gan_epochs=N_GAN_EPOCHS,\n",
    "        n_probes=N_PROBES,\n",
    "        local_epochs=LOCAL_EPOCHS,\n",
    "        device=DEVICE,\n",
    "        use_amp=USE_AMP,\n",
    "        client_data_weights=CLIENT_DATA_WEIGHTS\n",
    "    )\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    all_results[method_name] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'name': method_name\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… {method_name} Complete!\")\n",
    "    print(f\"   Final Accuracy: {history['acc'][-1]:.2f}%\")\n",
    "    print(f\"   Final JFI: {history['jfi'][-1]:.4f}\")\n",
    "    print(f\"   Accuracy Gap: {history['accuracy_gap'][-1]:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“Š Experiments Run:\")\n",
    "print(\"   1. FedAvg (Separate Baseline)\")\n",
    "for i, g in enumerate(GAMMA_VALUES, 2):\n",
    "    print(f\"   {i}. Fed-Audit-GAN v4.0 Î³={g} (V2 Linear Formula)\")\n",
    "print(\"ðŸ“Š Check your WandB dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38316b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š RESULTS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"ðŸ“Š CIFAR-10: FedAvg vs Fed-Audit-GAN v4.0 (V2 LINEAR FORMULA)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(f\"\\n{'METHOD':<35} {'GLOBAL ACC':<12} {'JFI':<10} {'MAX-MIN':<10} {'GAP':<10} {'MIN ACC':<10} {'MAX ACC':<10}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "best_acc = max(all_results[m]['history']['acc'][-1] for m in method_names)\n",
    "best_jfi = max(all_results[m]['history']['jfi'][-1] for m in method_names)\n",
    "lowest_gap = min(all_results[m]['history']['accuracy_gap'][-1] for m in method_names)\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    max_min = all_results[method]['history']['max_min_fairness'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    min_acc = all_results[method]['history']['min_client_acc'][-1]\n",
    "    max_acc = all_results[method]['history']['max_client_acc'][-1]\n",
    "    \n",
    "    acc_mark = \"ðŸ†\" if acc == best_acc else \"\"\n",
    "    jfi_mark = \"â­\" if jfi == best_jfi else \"\"\n",
    "    gap_mark = \"âœ…\" if gap == lowest_gap else \"\"\n",
    "    \n",
    "    print(f\"{name:<35} {acc:>8.2f}% {acc_mark:<2} {jfi:>8.4f} {jfi_mark:<2} {max_min:>8.4f}   {gap:>6.2f}% {gap_mark:<2} {min_acc:>8.2f}%  {max_acc:>8.2f}%\")\n",
    "\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# â­ KEY: Show that gamma values produce DIFFERENT results!\n",
    "print(f\"\\nâ­ GAMMA SENSITIVITY CHECK (V2 Linear Formula):\")\n",
    "gamma_results = {g: all_results[f'FedAuditGAN_v4_Î³={g}'] for g in GAMMA_VALUES if f'FedAuditGAN_v4_Î³={g}' in all_results}\n",
    "\n",
    "if len(gamma_results) >= 2:\n",
    "    g1, g2 = GAMMA_VALUES[0], GAMMA_VALUES[1]\n",
    "    acc1 = all_results[f'FedAuditGAN_v4_Î³={g1}']['history']['acc'][-1]\n",
    "    acc2 = all_results[f'FedAuditGAN_v4_Î³={g2}']['history']['acc'][-1]\n",
    "    jfi1 = all_results[f'FedAuditGAN_v4_Î³={g1}']['history']['jfi'][-1]\n",
    "    jfi2 = all_results[f'FedAuditGAN_v4_Î³={g2}']['history']['jfi'][-1]\n",
    "    gap1 = all_results[f'FedAuditGAN_v4_Î³={g1}']['history']['accuracy_gap'][-1]\n",
    "    gap2 = all_results[f'FedAuditGAN_v4_Î³={g2}']['history']['accuracy_gap'][-1]\n",
    "    \n",
    "    print(f\"   Î³={g1} vs Î³={g2}:\")\n",
    "    print(f\"      Accuracy difference: {abs(acc1 - acc2):.2f}%\")\n",
    "    print(f\"      JFI difference: {abs(jfi1 - jfi2):.4f}\")\n",
    "    print(f\"      Gap difference: {abs(gap1 - gap2):.2f}%\")\n",
    "    \n",
    "    if abs(acc1 - acc2) > 0.1 or abs(jfi1 - jfi2) > 0.001:\n",
    "        print(f\"   âœ… SUCCESS! Different gamma values produce DIFFERENT results!\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Results still similar - may need more rounds or larger gamma difference\")\n",
    "\n",
    "# Improvement over FedAvg\n",
    "fedavg_acc = all_results['FedAvg']['history']['acc'][-1]\n",
    "fedavg_jfi = all_results['FedAvg']['history']['jfi'][-1]\n",
    "fedavg_gap = all_results['FedAvg']['history']['accuracy_gap'][-1]\n",
    "\n",
    "print(f\"\\nðŸ“ˆ IMPROVEMENT OVER FedAvg:\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    \n",
    "    acc_diff = acc - fedavg_acc\n",
    "    jfi_diff = jfi - fedavg_jfi\n",
    "    gap_reduction = fedavg_gap - gap\n",
    "    \n",
    "    print(f\"   {name}:\")\n",
    "    print(f\"      Accuracy: {'+' if acc_diff >= 0 else ''}{acc_diff:.2f}%\")\n",
    "    print(f\"      JFI: {'+' if jfi_diff >= 0 else ''}{jfi_diff:.4f}\")\n",
    "    print(f\"      Gap Reduction: {gap_reduction:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"â­ V4 KEY CHANGE: V2 LINEAR AGGREGATION FORMULA\")\n",
    "print(\"   Weight = (1-Î³) Ã— Accuracy_norm + Î³ Ã— Fairness_norm\")\n",
    "print(\"   âž¡ï¸ Changing Î³ MECHANICALLY changes the weights!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š COMPREHENSIVE VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "colors = {\n",
    "    'FedAvg': '#e74c3c',                      # Red\n",
    "    'FedAuditGAN_v4_Î³=0.3': '#3498db',        # Blue (low fairness)\n",
    "    'FedAuditGAN_v4_Î³=0.7': '#2ecc71',        # Green (high fairness)\n",
    "}\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "rounds = range(1, N_ROUNDS + 1)\n",
    "\n",
    "# Plot 1: Global Accuracy\n",
    "ax = axes[0, 0]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    linewidth = 2.5 if method == 'FedAvg' else 2\n",
    "    ax.plot(rounds, acc, color=colors.get(method, '#95a5a6'), linestyle=linestyle, \n",
    "            linewidth=linewidth, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange', label=f'Warm-up ({WARMUP_ROUNDS} rounds)')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('CIFAR-10: Global Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: JFI\n",
    "ax = axes[0, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    jfi = all_results[method]['history']['jfi']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, jfi, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('JFI', fontsize=12)\n",
    "ax.set_title(\"Jain's Fairness Index\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy Gap\n",
    "ax = axes[0, 2]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    gap = all_results[method]['history']['accuracy_gap']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, gap, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy Gap (%)', fontsize=12)\n",
    "ax.set_title('Best-Worst Client Gap', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Variance\n",
    "ax = axes[1, 0]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    var = all_results[method]['history']['variance']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, var, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Variance', fontsize=12)\n",
    "ax.set_title('Per-Client Accuracy Variance', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Min-Max Range\n",
    "ax = axes[1, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    min_acc = all_results[method]['history']['min_client_acc']\n",
    "    max_acc = all_results[method]['history']['max_client_acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.fill_between(rounds, min_acc, max_acc, color=colors.get(method, '#95a5a6'), alpha=0.2)\n",
    "    ax.plot(rounds, min_acc, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=1.5)\n",
    "    ax.plot(rounds, max_acc, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=1.5, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Min-Max Client Accuracy Range', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final Per-Client Accuracy\n",
    "ax = axes[1, 2]\n",
    "x = np.arange(N_CLIENTS)\n",
    "width = 0.25\n",
    "for i, method in enumerate(method_names):\n",
    "    name = all_results[method]['name']\n",
    "    client_accs = all_results[method]['history']['client_accuracies'][-1]\n",
    "    ax.bar(x + i*width, client_accs, width, label=name, color=colors.get(method, '#95a5a6'), alpha=0.8)\n",
    "ax.set_xlabel('Client ID', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Per-Client Accuracy (Final Round)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cifar10_v4_linear_formula_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“ Results saved to: cifar10_v4_linear_formula_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25797b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE ALL MODELS AND RESULTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs('results_cifar10_v4_linear', exist_ok=True)\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    filename = f\"results_cifar10_v4_linear/{name.replace('=', '').replace('.', '_').replace('Î³', 'gamma')}_CIFAR10.pth\"\n",
    "    \n",
    "    save_dict = {\n",
    "        'model_state_dict': all_results[method]['model'].state_dict(),\n",
    "        'history': all_results[method]['history'],\n",
    "        'config': {\n",
    "            'n_rounds': N_ROUNDS,\n",
    "            'n_clients': N_CLIENTS,\n",
    "            'classes_per_client': CLASSES_PER_CLIENT,\n",
    "            'device': str(DEVICE),\n",
    "            'num_gpus': NUM_GPUS,\n",
    "            'aggregation_method': 'V2_LINEAR'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if method != 'FedAvg':\n",
    "        gamma_val = float(name.split('=')[1]) if '=' in name else 0.5\n",
    "        save_dict['config']['momentum'] = MOMENTUM\n",
    "        save_dict['config']['warmup_rounds'] = WARMUP_ROUNDS\n",
    "        save_dict['config']['mu'] = MU\n",
    "        save_dict['config']['gamma'] = gamma_val\n",
    "        save_dict['config']['grad_clip_norm'] = GRAD_CLIP_NORM\n",
    "        save_dict['config']['audit_frequency'] = AUDIT_FREQUENCY\n",
    "    \n",
    "    torch.save(save_dict, filename)\n",
    "    print(f\"âœ… Saved: {filename}\")\n",
    "\n",
    "with open('results_cifar10_v4_linear/all_results_summary.pkl', 'wb') as f:\n",
    "    summary = {\n",
    "        method: {\n",
    "            'name': all_results[method]['name'],\n",
    "            'history': all_results[method]['history'],\n",
    "            'final_acc': all_results[method]['history']['acc'][-1],\n",
    "            'final_jfi': all_results[method]['history']['jfi'][-1]\n",
    "        }\n",
    "        for method in method_names\n",
    "    }\n",
    "    pickle.dump(summary, f)\n",
    "print(\"âœ… Saved: results_cifar10_v4_linear/all_results_summary.pkl\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“Š FINAL RESULTS SUMMARY (Fed-Audit-GAN v4.0 - V2 Linear Formula)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ”µ BASELINE:\")\n",
    "print(f\"   FedAvg: {all_results['FedAvg']['history']['acc'][-1]:.2f}% accuracy, JFI={all_results['FedAvg']['history']['jfi'][-1]:.4f}\")\n",
    "print(\"\\nðŸŸ¢ FED-AUDIT-GAN v4.0 (V2 LINEAR FORMULA):\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    print(f\"   {name}: {acc:.2f}% accuracy, JFI={jfi:.4f}, Gap={gap:.2f}%\")\n",
    "\n",
    "print(\"\\nâ­ V4 KEY CHANGE:\")\n",
    "print(\"   Weight = (1-Î³) Ã— Accuracy_norm + Î³ Ã— Fairness_norm\")\n",
    "print(\"   âž¡ï¸ Min-Max normalization ensures both scores are in [0, 1]\")\n",
    "print(\"   âž¡ï¸ Changing Î³ MUST change the aggregation weights!\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nðŸ“Š Check WandB dashboard: https://wandb.ai\")\n",
    "print(f\"   Project: FED_AUDIT_GAN_TEST_4_CIFAR10\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
