{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0825cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb torch torchvision tqdm matplotlib numpy -q\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb7cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()\n",
    "print(\"WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9384023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Configuration with optimizations\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "# Enable cuDNN optimizations\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Using device: {DEVICE} | GPUs available: {NUM_GPUS}\")\n",
    "\n",
    "# ========== AGGRESSIVE GPU UTILIZATION ==========\n",
    "# With 2x T4 (15GB each), we can use MUCH larger batches\n",
    "NUM_WORKERS = 8  # More CPU workers\n",
    "PIN_MEMORY = True\n",
    "BATCH_SIZE = 1024 if NUM_GPUS > 1 else 512  # 4x increase!\n",
    "VAL_BATCH_SIZE = 2048 if NUM_GPUS > 1 else 1024  # 4x increase!\n",
    "PREFETCH_FACTOR = 4\n",
    "GAN_BATCH_SIZE = 512  # Larger batches for GAN training\n",
    "N_PROBES = 2000  # More probes for better fairness estimation\n",
    "\n",
    "# Experiment settings\n",
    "N_ROUNDS = 50\n",
    "N_CLIENTS = 20\n",
    "\n",
    "# CIFAR-10 specific\n",
    "IMG_SHAPE = (3, 32, 32)  # 3 channels, 32x32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Pre-load CIFAR-10 dataset\n",
    "print(\"Pre-loading CIFAR-10 dataset...\")\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "TRAIN_DATA = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "TEST_DATA = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Non-IID partition: each client gets only 2-3 classes (more extreme heterogeneity)\n",
    "def partition_non_iid_extreme(dataset, n_clients, classes_per_client=2, seed=42):\n",
    "    \"\"\"Extreme non-IID: each client gets only a few classes\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    \n",
    "    # Group indices by class\n",
    "    class_indices = {c: np.where(labels == c)[0] for c in range(NUM_CLASSES)}\n",
    "    \n",
    "    client_data = []\n",
    "    classes_assigned = []\n",
    "    \n",
    "    for cid in range(n_clients):\n",
    "        # Assign 2-3 random classes to each client\n",
    "        n_classes = np.random.randint(classes_per_client, classes_per_client + 2)\n",
    "        client_classes = np.random.choice(NUM_CLASSES, n_classes, replace=False)\n",
    "        classes_assigned.append(client_classes.tolist())\n",
    "        \n",
    "        # Gather indices for those classes\n",
    "        client_idx = []\n",
    "        for c in client_classes:\n",
    "            # Take a random subset of this class\n",
    "            n_samples = len(class_indices[c]) // (n_clients // 2)\n",
    "            start = np.random.randint(0, max(1, len(class_indices[c]) - n_samples))\n",
    "            client_idx.extend(class_indices[c][start:start + n_samples])\n",
    "        \n",
    "        np.random.shuffle(client_idx)\n",
    "        client_data.append(np.array(client_idx))\n",
    "    \n",
    "    return client_data, classes_assigned\n",
    "\n",
    "CLIENT_IDX, CLIENT_CLASSES = partition_non_iid_extreme(TRAIN_DATA, N_CLIENTS)\n",
    "print(f\"Client data sizes: {[len(idx) for idx in CLIENT_IDX]}\")\n",
    "print(f\"Client classes: {CLIENT_CLASSES}\")\n",
    "\n",
    "# Pre-create DataLoaders\n",
    "TEST_LOADER = DataLoader(TEST_DATA, batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                         persistent_workers=True, prefetch_factor=PREFETCH_FACTOR)\n",
    "\n",
    "VAL_IDX = np.random.choice(len(TRAIN_DATA), 2000, replace=False)\n",
    "VAL_LOADER = DataLoader(Subset(TRAIN_DATA, VAL_IDX), batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                        persistent_workers=True, prefetch_factor=PREFETCH_FACTOR)\n",
    "\n",
    "# Client evaluation loaders\n",
    "CLIENT_EVAL_LOADERS = []\n",
    "for cid in range(N_CLIENTS):\n",
    "    loader = DataLoader(Subset(TRAIN_DATA, CLIENT_IDX[cid]), batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                       num_workers=4, pin_memory=PIN_MEMORY, persistent_workers=True)\n",
    "    CLIENT_EVAL_LOADERS.append(loader)\n",
    "\n",
    "# ========== LARGER CIFAR-10 CNN Model (uses more GPU memory) ==========\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"Deeper ResNet-style CNN for CIFAR-10 - optimized for T4 GPUs\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        # Wider channels to use more GPU memory\n",
    "        self.conv1 = nn.Conv2d(3, 128, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.conv3 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        self.conv4 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32->16\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16->8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8->4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 4->2\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# ========== LARGER Fairness Generator for CIFAR-10 ==========\n",
    "class FairnessGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4  # 8\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 512 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(-1, 512, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        x_prime = torch.clamp(x + delta, -1, 1)\n",
    "        return x, x_prime\n",
    "\n",
    "# ========== LARGER Discriminator for CIFAR-10 ==========\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 128, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 1)  # No sigmoid for BCEWithLogitsLoss\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "# ========== Helper Functions ==========\n",
    "def make_parallel(model):\n",
    "    if NUM_GPUS > 1:\n",
    "        return nn.DataParallel(model)\n",
    "    return model\n",
    "\n",
    "def get_base_model(model):\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        return model.module\n",
    "    return model\n",
    "\n",
    "# ========== FAIRNESS METRICS ==========\n",
    "def compute_jfi(performances):\n",
    "    p = np.array(performances)\n",
    "    n = len(p)\n",
    "    if np.sum(p**2) == 0:\n",
    "        return 1.0\n",
    "    return (np.sum(p)**2) / (n * np.sum(p**2))\n",
    "\n",
    "def compute_max_min_fairness(performances):\n",
    "    p = np.array(performances)\n",
    "    if np.max(p) == 0:\n",
    "        return 0.0\n",
    "    return np.min(p) / np.max(p)\n",
    "\n",
    "def compute_fairness_variance(performances):\n",
    "    return np.var(np.array(performances))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_client(model, client_loaders):\n",
    "    model.eval()\n",
    "    client_accuracies = []\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        for loader in client_loaders:\n",
    "            correct, total = 0, 0\n",
    "            for d, t in loader:\n",
    "                d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "                correct += (model(d).argmax(1) == t).sum().item()\n",
    "                total += len(t)\n",
    "            client_accuracies.append(100 * correct / total if total > 0 else 0)\n",
    "    return client_accuracies\n",
    "\n",
    "def compute_all_fairness_metrics(client_accuracies):\n",
    "    return {\n",
    "        'jfi': compute_jfi(client_accuracies),\n",
    "        'max_min_fairness': compute_max_min_fairness(client_accuracies),\n",
    "        'fairness_variance': compute_fairness_variance(client_accuracies),\n",
    "        'min_accuracy': np.min(client_accuracies),\n",
    "        'max_accuracy': np.max(client_accuracies),\n",
    "        'mean_accuracy': np.mean(client_accuracies)\n",
    "    }\n",
    "\n",
    "# ========== Training Functions ==========\n",
    "def train_fairness_gan(G, D, model, loader, epochs=15, alpha=1.0, beta=1.0):\n",
    "    model.eval()\n",
    "    G = make_parallel(G)\n",
    "    D = make_parallel(D)\n",
    "    \n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    scaler = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    latent_dim = get_base_model(G).latent_dim\n",
    "    num_classes = get_base_model(G).num_classes\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            batch_size = imgs.size(0)\n",
    "            real_t = torch.ones(batch_size, 1, device=DEVICE)\n",
    "            fake_t = torch.zeros(batch_size, 1, device=DEVICE)\n",
    "            imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "            labels = labels.to(DEVICE, non_blocking=True)\n",
    "            z = torch.randn(batch_size, latent_dim, device=DEVICE)\n",
    "            gen_labels = torch.randint(0, num_classes, (batch_size,), device=DEVICE)\n",
    "            \n",
    "            # Generator\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                x, x_prime = G(z, gen_labels)\n",
    "                with torch.no_grad():\n",
    "                    pred_x, pred_xp = model(x), model(x_prime)\n",
    "                pred_diff = -beta * torch.mean((pred_x - pred_xp) ** 2)\n",
    "                realism = alpha * torch.mean((x - x_prime) ** 2)\n",
    "                gan_loss = (bce(D(x, gen_labels), real_t) + bce(D(x_prime, gen_labels), real_t)) / 2\n",
    "                g_loss = pred_diff + realism + gan_loss\n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.step(opt_G)\n",
    "            \n",
    "            # Discriminator\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                x, x_prime = G(z, gen_labels)\n",
    "                d_real = bce(D(imgs, labels), real_t)\n",
    "                d_fake = (bce(D(x.detach(), gen_labels), fake_t) + bce(D(x_prime.detach(), gen_labels), fake_t)) / 2\n",
    "                d_loss = (d_real + d_fake) / 2\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(opt_D)\n",
    "            scaler.update()\n",
    "    \n",
    "    return get_base_model(G), get_base_model(D)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bias(model, x, x_prime):\n",
    "    model.eval()\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        pred_x = model(x)\n",
    "        pred_xp = model(x_prime)\n",
    "    return torch.abs(pred_x - pred_xp).sum(dim=1).mean().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_acc_score(model, update, val_loader):\n",
    "    model.eval()\n",
    "    loss_before, count = 0.0, 0\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        for d, t in val_loader:\n",
    "            d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "            loss_before += F.cross_entropy(model(d), t, reduction='sum').item()\n",
    "            count += len(t)\n",
    "    loss_before /= count\n",
    "    \n",
    "    base_model = get_base_model(model)\n",
    "    hyp = copy.deepcopy(base_model)\n",
    "    sd = hyp.state_dict()\n",
    "    \n",
    "    for k in sd:\n",
    "        update_key = k\n",
    "        if k not in update:\n",
    "            update_key = k.replace('module.', '') if k.startswith('module.') else 'module.' + k\n",
    "        if update_key in update:\n",
    "            sd[k] = sd[k] + update[update_key]\n",
    "        elif k in update:\n",
    "            sd[k] = sd[k] + update[k]\n",
    "    \n",
    "    hyp.load_state_dict(sd)\n",
    "    hyp = make_parallel(hyp.to(DEVICE))\n",
    "    hyp.eval()\n",
    "    \n",
    "    loss_after = 0.0\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        for d, t in val_loader:\n",
    "            d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "            loss_after += F.cross_entropy(hyp(d), t, reduction='sum').item()\n",
    "    return loss_before - loss_after / count\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        for d, t in loader:\n",
    "            d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "            correct += (model(d).argmax(1) == t).sum().item()\n",
    "            total += len(t)\n",
    "    return 100 * correct / total\n",
    "\n",
    "def normalize(scores):\n",
    "    s = np.array(scores)\n",
    "    if s.max() - s.min() < 1e-8:\n",
    "        return np.ones_like(s) / len(s)\n",
    "    return (s - s.min()) / (s.max() - s.min())\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CIFAR-10 Fed-AuditGAN Experiment - AGGRESSIVE GPU MODE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"GPUs: {NUM_GPUS} x T4 (15GB each)\")\n",
    "print(f\"Batch Size: {BATCH_SIZE} | Val Batch: {VAL_BATCH_SIZE}\")\n",
    "print(f\"Workers: {NUM_WORKERS} | Probes: {N_PROBES}\")\n",
    "print(f\"Rounds: {N_ROUNDS} | Clients: {N_CLIENTS}\")\n",
    "print(f\"Non-IID: Each client has 2-3 classes only (EXTREME)\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec0ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fedavg(n_rounds=N_ROUNDS, n_clients=N_CLIENTS, local_epochs=3, lr=0.01):\n",
    "    \"\"\"Standard FedAvg baseline\"\"\"\n",
    "    wandb.init(project=\"fed-audit-gan-cifar10\", name=\"FedAvg\", \n",
    "               config={'method': 'FedAvg', 'n_rounds': n_rounds, 'n_clients': n_clients})\n",
    "    \n",
    "    global_model = make_parallel(CNN().to(DEVICE))\n",
    "    scaler = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    history = {'accuracy': [], 'jfi': [], 'max_min_fairness': [], 'fairness_variance': [],\n",
    "               'min_accuracy': [], 'max_accuracy': [], 'client_accuracies': [], 'weights': []}\n",
    "    \n",
    "    # Pre-create client loaders\n",
    "    client_loaders = []\n",
    "    for cid in range(n_clients):\n",
    "        loader = DataLoader(Subset(TRAIN_DATA, CLIENT_IDX[cid]), batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                           persistent_workers=True, prefetch_factor=PREFETCH_FACTOR)\n",
    "        client_loaders.append(loader)\n",
    "    \n",
    "    print(f\"FedAvg | Rounds: {n_rounds} | Clients: {n_clients} | GPUs: {NUM_GPUS}\")\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=\"FedAvg\"):\n",
    "        updates = []\n",
    "        client_sizes = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local = make_parallel(copy.deepcopy(get_base_model(global_model)).to(DEVICE))\n",
    "            before = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "            opt = optim.SGD(local.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "            local.train()\n",
    "            \n",
    "            for _ in range(local_epochs):\n",
    "                for d, t in client_loaders[cid]:\n",
    "                    d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        loss = F.cross_entropy(local(d), t)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "            \n",
    "            after = local.state_dict()\n",
    "            updates.append({k: after[k] - before[k] for k in before})\n",
    "            client_sizes.append(len(CLIENT_IDX[cid]))\n",
    "            del local\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # FedAvg aggregation\n",
    "        total_size = sum(client_sizes)\n",
    "        alphas = [size / total_size for size in client_sizes]\n",
    "        \n",
    "        sd = global_model.state_dict()\n",
    "        for k in sd:\n",
    "            sd[k] = sd[k] + sum(alphas[i] * updates[i][k] for i in range(n_clients))\n",
    "        global_model.load_state_dict(sd)\n",
    "        \n",
    "        # Evaluate\n",
    "        acc = evaluate(global_model, TEST_LOADER)\n",
    "        client_accs = evaluate_per_client(global_model, CLIENT_EVAL_LOADERS)\n",
    "        fairness = compute_all_fairness_metrics(client_accs)\n",
    "        \n",
    "        history['accuracy'].append(acc)\n",
    "        history['jfi'].append(fairness['jfi'])\n",
    "        history['max_min_fairness'].append(fairness['max_min_fairness'])\n",
    "        history['fairness_variance'].append(fairness['fairness_variance'])\n",
    "        history['min_accuracy'].append(fairness['min_accuracy'])\n",
    "        history['max_accuracy'].append(fairness['max_accuracy'])\n",
    "        history['client_accuracies'].append(client_accs)\n",
    "        history['weights'].append(alphas)\n",
    "        \n",
    "        wandb.log({'round': rnd+1, 'accuracy': acc, 'jfi': fairness['jfi'],\n",
    "                   'max_min_fairness': fairness['max_min_fairness'],\n",
    "                   'fairness_variance': fairness['fairness_variance'],\n",
    "                   'min_client_acc': fairness['min_accuracy'],\n",
    "                   'max_client_acc': fairness['max_accuracy']})\n",
    "    \n",
    "    print(f\"FedAvg Done | Accuracy: {history['accuracy'][-1]:.2f}% | JFI: {history['jfi'][-1]:.4f}\")\n",
    "    wandb.finish()\n",
    "    return history\n",
    "\n",
    "def run_fed_audit_gan(gamma, n_rounds=N_ROUNDS, n_clients=N_CLIENTS, local_epochs=3, lr=0.01, gan_epochs=20, n_probes=N_PROBES):\n",
    "    \"\"\"Fed-AuditGAN with fairness-aware aggregation\"\"\"\n",
    "    wandb.init(project=\"fed-audit-gan-cifar10\", name=f\"FedAuditGAN_gamma{gamma}\",\n",
    "               config={'method': 'FedAuditGAN', 'gamma': gamma, 'n_rounds': n_rounds, 'n_clients': n_clients})\n",
    "    \n",
    "    global_model = make_parallel(CNN().to(DEVICE))\n",
    "    scaler = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    history = {'accuracy': [], 'bias': [], 'jfi': [], 'max_min_fairness': [], 'fairness_variance': [],\n",
    "               'min_accuracy': [], 'max_accuracy': [], 'client_accuracies': [], 'weights': []}\n",
    "    \n",
    "    # Pre-create client loaders\n",
    "    client_loaders = []\n",
    "    for cid in range(n_clients):\n",
    "        loader = DataLoader(Subset(TRAIN_DATA, CLIENT_IDX[cid]), batch_size=BATCH_SIZE, shuffle=True,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                           persistent_workers=True, prefetch_factor=PREFETCH_FACTOR)\n",
    "        client_loaders.append(loader)\n",
    "    \n",
    "    print(f\"Fed-AuditGAN | gamma={gamma} | Rounds: {n_rounds} | Clients: {n_clients} | GPUs: {NUM_GPUS}\")\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=f\"gamma={gamma}\"):\n",
    "        # Phase 1: Client training\n",
    "        updates = []\n",
    "        for cid in range(n_clients):\n",
    "            local = make_parallel(copy.deepcopy(get_base_model(global_model)).to(DEVICE))\n",
    "            before = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "            opt = optim.SGD(local.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "            local.train()\n",
    "            \n",
    "            for _ in range(local_epochs):\n",
    "                for d, t in client_loaders[cid]:\n",
    "                    d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        loss = F.cross_entropy(local(d), t)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "            \n",
    "            after = local.state_dict()\n",
    "            updates.append({k: after[k] - before[k] for k in before})\n",
    "            del local\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Phase 2: GAN auditing\n",
    "        G = FairnessGenerator(img_shape=IMG_SHAPE).to(DEVICE)\n",
    "        D = Discriminator(img_shape=IMG_SHAPE).to(DEVICE)\n",
    "        G, D = train_fairness_gan(G, D, global_model, VAL_LOADER, epochs=gan_epochs)\n",
    "        G.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_probes, G.latent_dim, device=DEVICE)\n",
    "            lbls = torch.randint(0, NUM_CLASSES, (n_probes,), device=DEVICE)\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                G_parallel = make_parallel(G)\n",
    "                x_p, xp_p = G_parallel(z, lbls)\n",
    "        \n",
    "        # Phase 3: Scoring\n",
    "        B_base = compute_bias(global_model, x_p, xp_p)\n",
    "        S_fair, S_acc = [], []\n",
    "        for upd in updates:\n",
    "            hyp = make_parallel(copy.deepcopy(get_base_model(global_model)).to(DEVICE))\n",
    "            sd = hyp.state_dict()\n",
    "            for k in sd:\n",
    "                if k in upd:\n",
    "                    sd[k] = sd[k] + upd[k]\n",
    "            hyp.load_state_dict(sd)\n",
    "            S_fair.append(B_base - compute_bias(hyp, x_p, xp_p))\n",
    "            S_acc.append(compute_acc_score(global_model, upd, VAL_LOADER))\n",
    "            del hyp\n",
    "        \n",
    "        del G, D, x_p, xp_p\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Phase 4: Aggregation\n",
    "        S_fair_n, S_acc_n = normalize(S_fair), normalize(S_acc)\n",
    "        alphas = [(1-gamma)*S_acc_n[i] + gamma*S_fair_n[i] for i in range(n_clients)]\n",
    "        a_sum = sum(alphas)\n",
    "        alphas = [a/a_sum if a_sum > 0 else 1/n_clients for a in alphas]\n",
    "        \n",
    "        sd = global_model.state_dict()\n",
    "        for k in sd:\n",
    "            sd[k] = sd[k] + sum(alphas[i] * updates[i][k] for i in range(n_clients))\n",
    "        global_model.load_state_dict(sd)\n",
    "        \n",
    "        # Evaluate\n",
    "        acc = evaluate(global_model, TEST_LOADER)\n",
    "        client_accs = evaluate_per_client(global_model, CLIENT_EVAL_LOADERS)\n",
    "        fairness = compute_all_fairness_metrics(client_accs)\n",
    "        \n",
    "        history['accuracy'].append(acc)\n",
    "        history['bias'].append(B_base)\n",
    "        history['jfi'].append(fairness['jfi'])\n",
    "        history['max_min_fairness'].append(fairness['max_min_fairness'])\n",
    "        history['fairness_variance'].append(fairness['fairness_variance'])\n",
    "        history['min_accuracy'].append(fairness['min_accuracy'])\n",
    "        history['max_accuracy'].append(fairness['max_accuracy'])\n",
    "        history['client_accuracies'].append(client_accs)\n",
    "        history['weights'].append(alphas)\n",
    "        \n",
    "        wandb.log({'round': rnd+1, 'accuracy': acc, 'bias': B_base, 'jfi': fairness['jfi'],\n",
    "                   'max_min_fairness': fairness['max_min_fairness'],\n",
    "                   'fairness_variance': fairness['fairness_variance'],\n",
    "                   'min_client_acc': fairness['min_accuracy'],\n",
    "                   'max_client_acc': fairness['max_accuracy']})\n",
    "    \n",
    "    print(f\"gamma={gamma} Done | Accuracy: {history['accuracy'][-1]:.2f}% | JFI: {history['jfi'][-1]:.4f}\")\n",
    "    wandb.finish()\n",
    "    return history\n",
    "\n",
    "print(\"Training functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd01842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments: FedAvg + gamma 0.3 + gamma 0.7\n",
    "results = {}\n",
    "\n",
    "# 1. Run FedAvg baseline\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Running FedAvg Baseline\")\n",
    "print(\"=\"*50)\n",
    "results['FedAvg'] = run_fedavg(n_rounds=N_ROUNDS, n_clients=N_CLIENTS)\n",
    "\n",
    "# 2. Run Fed-AuditGAN with gamma = 0.3 and 0.7\n",
    "for gamma in [0.3, 0.7]:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Running Fed-AuditGAN gamma = {gamma}\")\n",
    "    print(\"=\"*50)\n",
    "    results[f'gamma{gamma}'] = run_fed_audit_gan(gamma=gamma, n_rounds=N_ROUNDS, n_clients=N_CLIENTS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"All experiments complete!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ade30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "colors = {'FedAvg': 'black', 'gamma0.3': 'blue', 'gamma0.7': 'red'}\n",
    "linestyles = {'FedAvg': '--', 'gamma0.3': '-', 'gamma0.7': '-'}\n",
    "rounds = list(range(1, N_ROUNDS + 1))\n",
    "\n",
    "# Plot 1: Global Accuracy\n",
    "for method in results:\n",
    "    axes[0, 0].plot(rounds, results[method]['accuracy'], linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[0, 0].set_xlabel('Round'); axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "axes[0, 0].set_title('CIFAR-10: Global Test Accuracy'); axes[0, 0].legend(); axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: JFI\n",
    "for method in results:\n",
    "    axes[0, 1].plot(rounds, results[method]['jfi'], linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[0, 1].set_xlabel('Round'); axes[0, 1].set_ylabel('JFI')\n",
    "axes[0, 1].set_title(\"Jain's Fairness Index (higher=fairer)\"); axes[0, 1].legend(); axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Max-Min Fairness\n",
    "for method in results:\n",
    "    axes[0, 2].plot(rounds, results[method]['max_min_fairness'], linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[0, 2].set_xlabel('Round'); axes[0, 2].set_ylabel('Min/Max Ratio')\n",
    "axes[0, 2].set_title('Max-Min Fairness (higher=fairer)'); axes[0, 2].legend(); axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Variance\n",
    "for method in results:\n",
    "    axes[1, 0].plot(rounds, results[method]['fairness_variance'], linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[1, 0].set_xlabel('Round'); axes[1, 0].set_ylabel('Variance')\n",
    "axes[1, 0].set_title('Per-Client Accuracy Variance (lower=fairer)'); axes[1, 0].legend(); axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Accuracy Gap\n",
    "for method in results:\n",
    "    gap = [results[method]['max_accuracy'][i] - results[method]['min_accuracy'][i] for i in range(len(rounds))]\n",
    "    axes[1, 1].plot(rounds, gap, linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[1, 1].set_xlabel('Round'); axes[1, 1].set_ylabel('Gap (%)')\n",
    "axes[1, 1].set_title('Best-Worst Client Gap (lower=fairer)'); axes[1, 1].legend(); axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final Per-Client Accuracy\n",
    "final_client_accs = {m: results[m]['client_accuracies'][-1] for m in results}\n",
    "x = np.arange(N_CLIENTS)\n",
    "width = 0.25\n",
    "for i, method in enumerate(results):\n",
    "    axes[1, 2].bar(x + i*width, final_client_accs[method], width, label=method, color=colors[method], alpha=0.8)\n",
    "axes[1, 2].set_xlabel('Client ID'); axes[1, 2].set_ylabel('Accuracy (%)')\n",
    "axes[1, 2].set_title(f'Per-Client Accuracy (Round {N_ROUNDS})'); axes[1, 2].legend(); axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cifar10_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Summary Table\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(\"CIFAR-10 FINAL RESULTS SUMMARY (Round {})\".format(N_ROUNDS))\n",
    "print(\"=\"*110)\n",
    "print(f\"{'Method':<15} {'Global Acc':>10} {'JFI':>8} {'Max-Min':>10} {'Variance':>12} {'Min Acc':>10} {'Max Acc':>10} {'Gap':>8}\")\n",
    "print(\"-\"*110)\n",
    "for method in results:\n",
    "    acc = results[method]['accuracy'][-1]\n",
    "    jfi = results[method]['jfi'][-1]\n",
    "    mmf = results[method]['max_min_fairness'][-1]\n",
    "    var = results[method]['fairness_variance'][-1]\n",
    "    min_acc = results[method]['min_accuracy'][-1]\n",
    "    max_acc = results[method]['max_accuracy'][-1]\n",
    "    gap = max_acc - min_acc\n",
    "    print(f\"{method:<15} {acc:>10.2f}% {jfi:>8.4f} {mmf:>10.4f} {var:>12.2f} {min_acc:>10.2f}% {max_acc:>10.2f}% {gap:>8.2f}%\")\n",
    "print(\"=\"*110)\n",
    "\n",
    "# Bias summary\n",
    "print(\"\\nBias Summary (Fed-AuditGAN only):\")\n",
    "for method in ['gamma0.3', 'gamma0.7']:\n",
    "    if method in results and 'bias' in results[method]:\n",
    "        print(f\"  {method}: Final Bias = {results[method]['bias'][-1]:.4f}\")\n",
    "\n",
    "# Winner\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FAIRNESS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "best_jfi = max(results.items(), key=lambda x: x[1]['jfi'][-1])\n",
    "best_mmf = max(results.items(), key=lambda x: x[1]['max_min_fairness'][-1])\n",
    "lowest_var = min(results.items(), key=lambda x: x[1]['fairness_variance'][-1])\n",
    "print(f\"Best JFI:        {best_jfi[0]} ({best_jfi[1]['jfi'][-1]:.4f})\")\n",
    "print(f\"Best Max-Min:    {best_mmf[0]} ({best_mmf[1]['max_min_fairness'][-1]:.4f})\")\n",
    "print(f\"Lowest Variance: {lowest_var[0]} ({lowest_var[1]['fairness_variance'][-1]:.2f})\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
