{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d46a77d",
   "metadata": {},
   "source": [
    "# üîß Fed-Audit-GAN v5.0 - CIFAR-10 (Simplified - No FedProx/Grad Clip)\n",
    "\n",
    "## üéØ Experiments Run:\n",
    "\n",
    "### üîµ Baseline: FedAvg (Federated Averaging)\n",
    "- Standard data-weighted averaging (McMahan et al., 2017)\n",
    "- NO GAN, NO fairness scoring\n",
    "\n",
    "### üü¢ Our Method: Fed-Audit-GAN (V2 Linear Formula)\n",
    "- **Fed-Audit-GAN Œ≥ = 0.3** - Accuracy-weighted\n",
    "- **Fed-Audit-GAN Œ≥ = 0.7** - Fairness-weighted\n",
    "\n",
    "## üîß Fed-Audit-GAN 4-Phase Architecture:\n",
    "1. **Phase 1**: Local Client Training (Standard SGD - NO FedProx)\n",
    "2. **Phase 2**: GAN Training (with Soft Labels)\n",
    "3. **Phase 3**: Fairness Scoring (with EMA)\n",
    "4. **Phase 4**: V2 Linear Aggregation (Min-Max Normalized)\n",
    "\n",
    "## ‚≠ê KEY CHANGES FROM V4:\n",
    "- ‚ùå **REMOVED**: FedProx proximal term\n",
    "- ‚ùå **REMOVED**: Gradient clipping\n",
    "- ‚úÖ **KEPT**: V2 Linear Formula for aggregation\n",
    "- ‚úÖ **KEPT**: Soft labels for GAN stability\n",
    "- ‚úÖ **KEPT**: Extended warm-up period\n",
    "\n",
    "## ‚≠ê V2 Linear Aggregation Formula:\n",
    "```\n",
    "Weight = (1 - Œ≥) √ó Accuracy_norm + Œ≥ √ó Fairness_norm\n",
    "```\n",
    "\n",
    "## ‚≠ê PATHOLOGICAL NON-IID:\n",
    "- Each client ONLY has 2 out of 10 classes\n",
    "- Clients have DIFFERENT sample sizes (Dirichlet distribution)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee69b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies\n",
    "!pip install -q torch torchvision tqdm matplotlib numpy wandb\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"‚úÖ WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a018c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Imports and GPU Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# üöÄ GPU SETUP\n",
    "# ============================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    \n",
    "    # Enable cuDNN optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # Enable TF32 for faster matrix operations\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"‚úÖ GPU(s) detected: {NUM_GPUS}\")\n",
    "    for i in range(NUM_GPUS):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"   GPU {i}: {props.name}\")\n",
    "        print(f\"      Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    NUM_GPUS = 0\n",
    "    print(\"‚ö†Ô∏è  No GPU detected. Using CPU.\")\n",
    "\n",
    "# Mixed Precision Training\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "if USE_AMP:\n",
    "    print(\"\\n‚úÖ Mixed Precision Training (AMP) enabled.\")\n",
    "\n",
    "print(f\"\\nüìç Device: {DEVICE}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15edb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEFINITIONS (CIFAR-10)\n",
    "# ============================================================\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"CNN for CIFAR-10 classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32->16\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16->8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8->4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 4->2\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    \"\"\"Generator that produces paired samples (x, x') for fairness testing\"\"\"\n",
    "    def __init__(self, latent_dim=128, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4  # 8\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 512 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input).view(-1, 512, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        return x, torch.clamp(x + delta, -1, 1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Conditional Discriminator\"\"\"\n",
    "    def __init__(self, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 1)\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Models defined: CNN, FairnessGenerator, Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def train_gan(G, D, model, loader, epochs=15, device='cuda', l1=1.0, l2=1.0):\n",
    "    \"\"\"\n",
    "    Train the Fairness GAN with Soft Labels for stability\n",
    "    \"\"\"\n",
    "    G, D, model = G.to(device), D.to(device), model.to(device)\n",
    "    model.eval()\n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    if USE_AMP:\n",
    "        scaler_G = torch.amp.GradScaler(device='cuda')\n",
    "        scaler_D = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            bs = imgs.size(0)\n",
    "            \n",
    "            # Soft Labels for stability\n",
    "            real_labels = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "            fake_labels = torch.empty(bs, 1, device=device).uniform_(0.0, 0.1)\n",
    "            \n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            z = torch.randn(bs, G.latent_dim, device=device)\n",
    "            gl = torch.randint(0, G.num_classes, (bs,), device=device)\n",
    "            \n",
    "            # Generator\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    with torch.no_grad():\n",
    "                        px, pxp = model(x), model(xp)\n",
    "                    t1 = -torch.mean((px - pxp) ** 2)\n",
    "                    t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                    g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                    t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                    g_loss = t1 + t2 + t3\n",
    "                scaler_G.scale(g_loss).backward()\n",
    "                scaler_G.step(opt_G)\n",
    "                scaler_G.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                with torch.no_grad():\n",
    "                    px, pxp = model(x), model(xp)\n",
    "                t1 = -torch.mean((px - pxp) ** 2)\n",
    "                t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                g_loss = t1 + t2 + t3\n",
    "                g_loss.backward()\n",
    "                opt_G.step()\n",
    "            \n",
    "            # Discriminator\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                              bce(D(x.detach(), gl), fake_labels) + \n",
    "                              bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                scaler_D.scale(d_loss).backward()\n",
    "                scaler_D.step(opt_D)\n",
    "                scaler_D.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                          bce(D(x.detach(), gl), fake_labels) + \n",
    "                          bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                d_loss.backward()\n",
    "                opt_D.step()\n",
    "    \n",
    "    return G, D\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bias(model, x, xp, device):\n",
    "    \"\"\"Compute bias as difference in model predictions between x and x'\"\"\"\n",
    "    model.eval()\n",
    "    x, xp = x.to(device), xp.to(device)\n",
    "    \n",
    "    if USE_AMP:\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    else:\n",
    "        diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    \n",
    "    return diff.item()\n",
    "\n",
    "\n",
    "def partition_data_pathological_non_iid(dataset, n_clients, classes_per_client=2):\n",
    "    \"\"\"Pathological Non-IID with unequal sample sizes\"\"\"\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    class_indices = {c: np.where(labels == c)[0] for c in range(n_classes)}\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "    \n",
    "    client_classes = []\n",
    "    for cid in range(n_clients):\n",
    "        start_class = (cid * classes_per_client) % n_classes\n",
    "        assigned = [(start_class + i) % n_classes for i in range(classes_per_client)]\n",
    "        client_classes.append(assigned)\n",
    "    \n",
    "    # Unequal sample sizes using Dirichlet\n",
    "    alpha = 0.5\n",
    "    client_proportions = np.random.dirichlet([alpha] * n_clients)\n",
    "    \n",
    "    client_indices = [[] for _ in range(n_clients)]\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        clients_with_class = [cid for cid in range(n_clients) if c in client_classes[cid]]\n",
    "        \n",
    "        if len(clients_with_class) > 0:\n",
    "            class_samples = class_indices[c]\n",
    "            total_for_class = len(class_samples)\n",
    "            \n",
    "            relevant_props = np.array([client_proportions[cid] for cid in clients_with_class])\n",
    "            relevant_props = relevant_props / relevant_props.sum()\n",
    "            \n",
    "            start_idx = 0\n",
    "            for i, cid in enumerate(clients_with_class):\n",
    "                if i == len(clients_with_class) - 1:\n",
    "                    end_idx = total_for_class\n",
    "                else:\n",
    "                    n_samples = int(total_for_class * relevant_props[i])\n",
    "                    end_idx = min(start_idx + n_samples, total_for_class)\n",
    "                \n",
    "                if start_idx < end_idx:\n",
    "                    client_indices[cid].extend(class_samples[start_idx:end_idx].tolist())\n",
    "                start_idx = end_idx\n",
    "    \n",
    "    result = []\n",
    "    for cid in range(n_clients):\n",
    "        if len(client_indices[cid]) > 0:\n",
    "            indices = np.array(client_indices[cid])\n",
    "            np.random.shuffle(indices)\n",
    "            result.append(indices)\n",
    "        else:\n",
    "            fallback_samples = np.random.choice(len(dataset), size=50, replace=False)\n",
    "            result.append(fallback_samples)\n",
    "    \n",
    "    return result, client_classes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for d, t in loader:\n",
    "        d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                preds = model(d).argmax(1)\n",
    "        else:\n",
    "            preds = model(d).argmax(1)\n",
    "        \n",
    "        correct += (preds == t).sum().item()\n",
    "        total += len(t)\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_client(model, client_loaders, device):\n",
    "    \"\"\"Evaluate model accuracy on each client's data\"\"\"\n",
    "    model.eval()\n",
    "    client_accuracies = []\n",
    "    \n",
    "    for loader in client_loaders:\n",
    "        correct, total = 0, 0\n",
    "        for d, t in loader:\n",
    "            d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "            \n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    preds = model(d).argmax(1)\n",
    "            else:\n",
    "                preds = model(d).argmax(1)\n",
    "            \n",
    "            correct += (preds == t).sum().item()\n",
    "            total += len(t)\n",
    "        \n",
    "        acc = 100 * correct / total if total > 0 else 0\n",
    "        client_accuracies.append(acc)\n",
    "    \n",
    "    return client_accuracies\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FAIRNESS METRICS\n",
    "# ============================================================\n",
    "\n",
    "def calculate_jfi(performances):\n",
    "    \"\"\"Jain's Fairness Index\"\"\"\n",
    "    p = np.array(performances)\n",
    "    n = len(p)\n",
    "    if np.sum(p ** 2) == 0:\n",
    "        return 1.0\n",
    "    return (np.sum(p) ** 2) / (n * np.sum(p ** 2))\n",
    "\n",
    "def calculate_accuracy_gap(performances):\n",
    "    return np.max(performances) - np.min(performances)\n",
    "\n",
    "def calculate_variance(performances):\n",
    "    return np.var(performances)\n",
    "\n",
    "def calculate_max_min_fairness(performances):\n",
    "    p = np.array(performances)\n",
    "    if np.max(p) == 0:\n",
    "        return 0.0\n",
    "    return np.min(p) / np.max(p)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4a9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION (V5 - Simplified: No FedProx, No Grad Clip)\n",
    "# ============================================================\n",
    "\n",
    "# Training Parameters\n",
    "N_ROUNDS = 50\n",
    "N_CLIENTS = 20\n",
    "N_GAN_EPOCHS = 15\n",
    "N_PROBES = 500\n",
    "LOCAL_EPOCHS = 3\n",
    "\n",
    "# Warm-up (kept for stability)\n",
    "WARMUP_ROUNDS = 10\n",
    "AUDIT_FREQUENCY = 2\n",
    "\n",
    "# Fed-Audit-GAN Parameters\n",
    "MOMENTUM = 0.8  # EMA momentum for fairness scores\n",
    "\n",
    "# ‚≠ê REMOVED: FedProx (MU = 0) and Gradient Clipping\n",
    "# MU = 0.0  # No FedProx!\n",
    "# GRAD_CLIP_NORM = None  # No gradient clipping!\n",
    "\n",
    "# Test multiple gamma values\n",
    "GAMMA_VALUES = [0.3, 0.7]\n",
    "\n",
    "# DataLoader Parameters\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "PREFETCH_FACTOR = 2\n",
    "\n",
    "# PATHOLOGICAL NON-IID\n",
    "CLASSES_PER_CLIENT = 2\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üîß Fed-Audit-GAN v5.0 - CIFAR-10 (SIMPLIFIED)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPUs: {NUM_GPUS}\")\n",
    "print(f\"AMP Enabled: {USE_AMP}\")\n",
    "print(f\"Rounds: {N_ROUNDS}, Clients: {N_CLIENTS}\")\n",
    "\n",
    "print(f\"\\nüéØ EXPERIMENTS TO RUN:\")\n",
    "print(f\"   üîµ 1. FedAvg (BASELINE)\")\n",
    "for i, g in enumerate(GAMMA_VALUES, 2):\n",
    "    print(f\"   üü¢ {i}. Fed-Audit-GAN Œ≥={g}\")\n",
    "\n",
    "print(f\"\\n‚≠ê V5 SIMPLIFICATIONS:\")\n",
    "print(f\"   ‚ùå REMOVED: FedProx proximal term\")\n",
    "print(f\"   ‚ùå REMOVED: Gradient clipping\")\n",
    "print(f\"   ‚úÖ KEPT: V2 Linear aggregation formula\")\n",
    "print(f\"   ‚úÖ KEPT: Soft labels for GAN\")\n",
    "print(f\"   ‚úÖ KEPT: Warm-up period ({WARMUP_ROUNDS} rounds)\")\n",
    "\n",
    "print(f\"\\n‚≠ê PATHOLOGICAL NON-IID:\")\n",
    "print(f\"   Each client gets ONLY {CLASSES_PER_CLIENT}/10 classes\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f253e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create PATHOLOGICAL Non-IID partitions\n",
    "np.random.seed(42)\n",
    "client_idx, client_classes = partition_data_pathological_non_iid(\n",
    "    train_data, N_CLIENTS, classes_per_client=CLASSES_PER_CLIENT\n",
    ")\n",
    "\n",
    "# Calculate data weights\n",
    "client_data_sizes = [len(idx) for idx in client_idx]\n",
    "total_samples = sum(client_data_sizes)\n",
    "CLIENT_DATA_WEIGHTS = [size / total_samples for size in client_data_sizes]\n",
    "\n",
    "# DataLoader kwargs\n",
    "dataloader_kwargs = {\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY,\n",
    "    'persistent_workers': True,\n",
    "    'prefetch_factor': PREFETCH_FACTOR,\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=VAL_BATCH_SIZE, shuffle=False, **dataloader_kwargs)\n",
    "val_loader = DataLoader(\n",
    "    Subset(train_data, np.random.choice(len(train_data), 2000, replace=False)),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **dataloader_kwargs\n",
    ")\n",
    "\n",
    "client_loaders = [\n",
    "    DataLoader(Subset(train_data, client_idx[c]), batch_size=BATCH_SIZE, shuffle=True, **dataloader_kwargs)\n",
    "    for c in range(N_CLIENTS)\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Data loaded!\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"‚≠ê PATHOLOGICAL NON-IID ({CLASSES_PER_CLIENT} classes per client)\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(min(10, N_CLIENTS)):\n",
    "    class_names = [CIFAR10_CLASSES[c] for c in client_classes[i]]\n",
    "    print(f\"   Client {i:2d}: {client_classes[i]} ‚Üí {class_names} ({client_data_sizes[i]} samples)\")\n",
    "\n",
    "if N_CLIENTS > 10:\n",
    "    print(f\"   ... ({N_CLIENTS - 10} more clients)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c0d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTIONS (SIMPLIFIED - No FedProx, No Grad Clip)\n",
    "# ============================================================\n",
    "\n",
    "def run_fedavg(n_rounds, n_clients, train_data, client_idx, val_loader, test_loader, \n",
    "               client_loaders, local_epochs, device, use_amp, client_data_weights):\n",
    "    \"\"\"\n",
    "    Standard FedAvg (McMahan et al., 2017)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üîµ RUNNING: FedAvg (Federated Averaging) - BASELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=\"FedAvg\"):\n",
    "        local_weights = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            loss = criterion(output, target)\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            local_weights.append(copy.deepcopy(local_model.state_dict()))\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Data-Weighted Averaging\n",
    "        avg_weights = copy.deepcopy(local_weights[0])\n",
    "        for key in avg_weights.keys():\n",
    "            avg_weights[key] = avg_weights[key] * client_data_weights[0]\n",
    "            for i in range(1, len(local_weights)):\n",
    "                avg_weights[key] += local_weights[i][key] * client_data_weights[i]\n",
    "        \n",
    "        model.load_state_dict(avg_weights)\n",
    "        \n",
    "        # Evaluation\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def run_fed_audit_gan_v5(gamma, n_rounds, n_clients, warmup_rounds, momentum,\n",
    "                         audit_frequency, train_data, client_idx, val_loader, \n",
    "                         test_loader, client_loaders, n_gan_epochs, n_probes, \n",
    "                         local_epochs, device, use_amp, client_data_weights):\n",
    "    \"\"\"\n",
    "    Fed-Audit-GAN v5.0 - SIMPLIFIED\n",
    "    \n",
    "    ‚ùå REMOVED: FedProx proximal term\n",
    "    ‚ùå REMOVED: Gradient clipping\n",
    "    ‚úÖ KEPT: V2 Linear aggregation formula\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"üü¢ RUNNING: Fed-Audit-GAN v5.0 (Œ≥={gamma}) - SIMPLIFIED\")\n",
    "    print(f\"   ‚ùå No FedProx | ‚ùå No Gradient Clipping\")\n",
    "    print(f\"   ‚úÖ V2 Linear Formula: Weight = (1-Œ≥)√óAcc + Œ≥√óFair\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    fairness_history = {i: 0.0 for i in range(n_clients)}\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'bias': [], 'alphas': [],\n",
    "        'raw_scores': [], 'smoothed_scores': [],\n",
    "        'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': [],\n",
    "        'audit_active': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=f\"Fed-Audit-GAN Œ≥={gamma}\"):\n",
    "        \n",
    "        should_audit = (rnd >= warmup_rounds) and (rnd % audit_frequency == 0)\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 1: Local Client Training (STANDARD SGD - No FedProx!)\n",
    "        # ================================================================\n",
    "        updates = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            before_state = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # Standard SGD - NO FedProx, NO Gradient Clipping\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            # Just CrossEntropy - NO FedProx proximal term!\n",
    "                            loss = F.cross_entropy(output, target)\n",
    "                        scaler.scale(loss).backward()\n",
    "                        # NO gradient clipping!\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        loss = F.cross_entropy(output, target)\n",
    "                        loss.backward()\n",
    "                        # NO gradient clipping!\n",
    "                        optimizer.step()\n",
    "            \n",
    "            update = {k: local_model.state_dict()[k] - before_state[k] for k in before_state}\n",
    "            updates.append(update)\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 2 & 3: GAN Training + Fairness Scoring\n",
    "        # ================================================================\n",
    "        B_base = 0.0\n",
    "        S_fair_raw = [0.0] * n_clients\n",
    "        S_fair_smoothed = [0.0] * n_clients\n",
    "        \n",
    "        if should_audit:\n",
    "            G = FairnessGenerator(img_shape=(3, 32, 32)).to(device)\n",
    "            D = Discriminator(img_shape=(3, 32, 32)).to(device)\n",
    "            G, D = train_gan(G, D, model, val_loader, epochs=n_gan_epochs, device=device)\n",
    "            \n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(n_probes, G.latent_dim, device=device)\n",
    "                labels = torch.randint(0, 10, (n_probes,), device=device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        x_probe, xp_probe = G(z, labels)\n",
    "                else:\n",
    "                    x_probe, xp_probe = G(z, labels)\n",
    "            \n",
    "            B_base = compute_bias(model, x_probe, xp_probe, device)\n",
    "            \n",
    "            S_fair_raw = []\n",
    "            S_fair_smoothed = []\n",
    "            \n",
    "            for cid, upd in enumerate(updates):\n",
    "                hyp_model = copy.deepcopy(model)\n",
    "                hyp_state = hyp_model.state_dict()\n",
    "                for k in hyp_state:\n",
    "                    hyp_state[k] = hyp_state[k] + upd[k]\n",
    "                hyp_model.load_state_dict(hyp_state)\n",
    "                \n",
    "                B_client = compute_bias(hyp_model, x_probe, xp_probe, device)\n",
    "                S_current = B_base - B_client\n",
    "                S_fair_raw.append(S_current)\n",
    "                \n",
    "                # EMA smoothing\n",
    "                S_prev = fairness_history[cid]\n",
    "                S_smoothed = (momentum * S_prev) + ((1 - momentum) * S_current)\n",
    "                fairness_history[cid] = S_smoothed\n",
    "                S_fair_smoothed.append(S_smoothed)\n",
    "                del hyp_model\n",
    "            \n",
    "            del G, D, x_probe, xp_probe\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            S_fair_smoothed = [fairness_history[i] for i in range(n_clients)]\n",
    "        \n",
    "        history['raw_scores'].append(S_fair_raw.copy())\n",
    "        history['smoothed_scores'].append(S_fair_smoothed.copy())\n",
    "        history['audit_active'].append(should_audit)\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 4: V2 LINEAR AGGREGATION\n",
    "        # ================================================================\n",
    "        if rnd < warmup_rounds:\n",
    "            alphas = client_data_weights.copy()\n",
    "        else:\n",
    "            # V2 Linear Formula with Min-Max Normalization\n",
    "            \n",
    "            # Normalize Fairness Score\n",
    "            f_tensor = torch.tensor(S_fair_smoothed, device=device, dtype=torch.float32)\n",
    "            f_min, f_max = f_tensor.min(), f_tensor.max()\n",
    "            if f_max != f_min:\n",
    "                f_norm = (f_tensor - f_min) / (f_max - f_min)\n",
    "            else:\n",
    "                f_norm = torch.ones_like(f_tensor) * 0.5\n",
    "            \n",
    "            # Normalize Accuracy Score (using data weights as proxy)\n",
    "            a_tensor = torch.tensor(client_data_weights, device=device, dtype=torch.float32)\n",
    "            a_min, a_max = a_tensor.min(), a_tensor.max()\n",
    "            if a_max != a_min:\n",
    "                a_norm = (a_tensor - a_min) / (a_max - a_min)\n",
    "            else:\n",
    "                a_norm = torch.ones_like(a_tensor) * 0.5\n",
    "            \n",
    "            # V2 Linear Formula\n",
    "            raw_weights = ((1 - gamma) * a_norm) + (gamma * f_norm) + 1e-8\n",
    "            \n",
    "            # Final Normalization\n",
    "            alphas = (raw_weights / raw_weights.sum()).tolist()\n",
    "            \n",
    "            # Debug output every 10 rounds\n",
    "            if rnd % 10 == 0:\n",
    "                print(f\"\\n   üìä Round {rnd+1} Debug (Œ≥={gamma}):\")\n",
    "                print(f\"      Fairness: min={min(S_fair_smoothed):.4f}, max={max(S_fair_smoothed):.4f}\")\n",
    "                print(f\"      Alphas: min={min(alphas):.4f}, max={max(alphas):.4f}\")\n",
    "        \n",
    "        # Apply weighted aggregation\n",
    "        new_state = model.state_dict()\n",
    "        for k in new_state:\n",
    "            new_state[k] = new_state[k] + sum(a * u[k] for a, u in zip(alphas, updates))\n",
    "        model.load_state_dict(new_state)\n",
    "        \n",
    "        # ================================================================\n",
    "        # EVALUATION\n",
    "        # ================================================================\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['bias'].append(B_base)\n",
    "        history['alphas'].append(alphas.copy())\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'bias': B_base,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs),\n",
    "            'audit_active': int(should_audit),\n",
    "            'alpha_min': min(alphas),\n",
    "            'alpha_max': max(alphas)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "print(\"‚úÖ Training functions defined (FedAvg + Fed-Audit-GAN v5.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b822e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN ALL EXPERIMENTS\n",
    "# ============================================================\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 1: FedAvg (BASELINE)\n",
    "# ============================================================\n",
    "wandb.init(\n",
    "    project=\"FED_AUDIT_GAN_TEST_5_CIFAR10\",\n",
    "    name=f\"FedAvg_CIFAR10_clients{N_CLIENTS}_v5\",\n",
    "    config={\n",
    "        \"method\": \"FedAvg\",\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "        \"n_rounds\": N_ROUNDS,\n",
    "        \"n_clients\": N_CLIENTS,\n",
    "        \"non_iid_type\": \"pathological\",\n",
    "        \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "        \"device\": str(DEVICE),\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"amp_enabled\": USE_AMP\n",
    "    }\n",
    ")\n",
    "\n",
    "fedavg_model, fedavg_history = run_fedavg(\n",
    "    n_rounds=N_ROUNDS,\n",
    "    n_clients=N_CLIENTS,\n",
    "    train_data=train_data,\n",
    "    client_idx=client_idx,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    client_loaders=client_loaders,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    device=DEVICE,\n",
    "    use_amp=USE_AMP,\n",
    "    client_data_weights=CLIENT_DATA_WEIGHTS\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "all_results['FedAvg'] = {\n",
    "    'model': fedavg_model,\n",
    "    'history': fedavg_history,\n",
    "    'name': 'FedAvg'\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ FedAvg Complete!\")\n",
    "print(f\"   Final Accuracy: {fedavg_history['acc'][-1]:.2f}%\")\n",
    "print(f\"   Final JFI: {fedavg_history['jfi'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENTS 2-3: Fed-Audit-GAN v5.0 with Œ≥ = 0.3 and 0.7\n",
    "# ============================================================\n",
    "for gamma in GAMMA_VALUES:\n",
    "    method_name = f\"FedAuditGAN_v5_Œ≥={gamma}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"FED_AUDIT_GAN_TEST_5_CIFAR10\",\n",
    "        name=f\"{method_name}_CIFAR10_clients{N_CLIENTS}_simplified\",\n",
    "        config={\n",
    "            \"method\": method_name,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"n_rounds\": N_ROUNDS,\n",
    "            \"n_clients\": N_CLIENTS,\n",
    "            \"gamma\": gamma,\n",
    "            \"momentum\": MOMENTUM,\n",
    "            \"warmup_rounds\": WARMUP_ROUNDS,\n",
    "            \"fedprox\": False,\n",
    "            \"gradient_clipping\": False,\n",
    "            \"aggregation_method\": \"V2_LINEAR\",\n",
    "            \"non_iid_type\": \"pathological\",\n",
    "            \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "            \"device\": str(DEVICE),\n",
    "            \"num_gpus\": NUM_GPUS,\n",
    "            \"amp_enabled\": USE_AMP\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model, history = run_fed_audit_gan_v5(\n",
    "        gamma=gamma,\n",
    "        n_rounds=N_ROUNDS,\n",
    "        n_clients=N_CLIENTS,\n",
    "        warmup_rounds=WARMUP_ROUNDS,\n",
    "        momentum=MOMENTUM,\n",
    "        audit_frequency=AUDIT_FREQUENCY,\n",
    "        train_data=train_data,\n",
    "        client_idx=client_idx,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        client_loaders=client_loaders,\n",
    "        n_gan_epochs=N_GAN_EPOCHS,\n",
    "        n_probes=N_PROBES,\n",
    "        local_epochs=LOCAL_EPOCHS,\n",
    "        device=DEVICE,\n",
    "        use_amp=USE_AMP,\n",
    "        client_data_weights=CLIENT_DATA_WEIGHTS\n",
    "    )\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    all_results[method_name] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'name': method_name\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {method_name} Complete!\")\n",
    "    print(f\"   Final Accuracy: {history['acc'][-1]:.2f}%\")\n",
    "    print(f\"   Final JFI: {history['jfi'][-1]:.4f}\")\n",
    "    print(f\"   Accuracy Gap: {history['accuracy_gap'][-1]:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä RESULTS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"üìä CIFAR-10: FedAvg vs Fed-Audit-GAN v5.0 (SIMPLIFIED - No FedProx/Grad Clip)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(f\"\\n{'METHOD':<35} {'GLOBAL ACC':<12} {'JFI':<10} {'MAX-MIN':<10} {'GAP':<10} {'MIN ACC':<10} {'MAX ACC':<10}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "best_acc = max(all_results[m]['history']['acc'][-1] for m in method_names)\n",
    "best_jfi = max(all_results[m]['history']['jfi'][-1] for m in method_names)\n",
    "lowest_gap = min(all_results[m]['history']['accuracy_gap'][-1] for m in method_names)\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    max_min = all_results[method]['history']['max_min_fairness'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    min_acc = all_results[method]['history']['min_client_acc'][-1]\n",
    "    max_acc = all_results[method]['history']['max_client_acc'][-1]\n",
    "    \n",
    "    acc_mark = \"üèÜ\" if acc == best_acc else \"\"\n",
    "    jfi_mark = \"‚≠ê\" if jfi == best_jfi else \"\"\n",
    "    gap_mark = \"‚úÖ\" if gap == lowest_gap else \"\"\n",
    "    \n",
    "    print(f\"{name:<35} {acc:>8.2f}% {acc_mark:<2} {jfi:>8.4f} {jfi_mark:<2} {max_min:>8.4f}   {gap:>6.2f}% {gap_mark:<2} {min_acc:>8.2f}%  {max_acc:>8.2f}%\")\n",
    "\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Gamma sensitivity check\n",
    "print(f\"\\n‚≠ê GAMMA SENSITIVITY CHECK:\")\n",
    "if len(GAMMA_VALUES) >= 2:\n",
    "    g1, g2 = GAMMA_VALUES[0], GAMMA_VALUES[1]\n",
    "    acc1 = all_results[f'FedAuditGAN_v5_Œ≥={g1}']['history']['acc'][-1]\n",
    "    acc2 = all_results[f'FedAuditGAN_v5_Œ≥={g2}']['history']['acc'][-1]\n",
    "    jfi1 = all_results[f'FedAuditGAN_v5_Œ≥={g1}']['history']['jfi'][-1]\n",
    "    jfi2 = all_results[f'FedAuditGAN_v5_Œ≥={g2}']['history']['jfi'][-1]\n",
    "    \n",
    "    print(f\"   Œ≥={g1} vs Œ≥={g2}:\")\n",
    "    print(f\"      Accuracy difference: {abs(acc1 - acc2):.2f}%\")\n",
    "    print(f\"      JFI difference: {abs(jfi1 - jfi2):.4f}\")\n",
    "    \n",
    "    if abs(acc1 - acc2) > 0.1 or abs(jfi1 - jfi2) > 0.001:\n",
    "        print(f\"   ‚úÖ SUCCESS! Different gamma values produce DIFFERENT results!\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Results still similar\")\n",
    "\n",
    "# Improvement over FedAvg\n",
    "fedavg_acc = all_results['FedAvg']['history']['acc'][-1]\n",
    "fedavg_jfi = all_results['FedAvg']['history']['jfi'][-1]\n",
    "fedavg_gap = all_results['FedAvg']['history']['accuracy_gap'][-1]\n",
    "\n",
    "print(f\"\\nüìà IMPROVEMENT OVER FedAvg:\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    \n",
    "    print(f\"   {name}:\")\n",
    "    print(f\"      Accuracy: {'+' if acc >= fedavg_acc else ''}{acc - fedavg_acc:.2f}%\")\n",
    "    print(f\"      JFI: {'+' if jfi >= fedavg_jfi else ''}{jfi - fedavg_jfi:.4f}\")\n",
    "    print(f\"      Gap Reduction: {fedavg_gap - gap:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚≠ê V5 SIMPLIFICATIONS:\")\n",
    "print(\"   ‚ùå REMOVED: FedProx proximal term\")\n",
    "print(\"   ‚ùå REMOVED: Gradient clipping\")\n",
    "print(\"   ‚úÖ KEPT: V2 Linear aggregation formula\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcade3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "colors = {\n",
    "    'FedAvg': '#e74c3c',\n",
    "    'FedAuditGAN_v5_Œ≥=0.3': '#3498db',\n",
    "    'FedAuditGAN_v5_Œ≥=0.7': '#2ecc71',\n",
    "}\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "rounds = range(1, N_ROUNDS + 1)\n",
    "\n",
    "# Plot 1: Global Accuracy\n",
    "ax = axes[0, 0]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, acc, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange', label=f'Warm-up')\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('CIFAR-10: Global Test Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: JFI\n",
    "ax = axes[0, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    jfi = all_results[method]['history']['jfi']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, jfi, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('JFI')\n",
    "ax.set_title(\"Jain's Fairness Index\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy Gap\n",
    "ax = axes[0, 2]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    gap = all_results[method]['history']['accuracy_gap']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, gap, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Accuracy Gap (%)')\n",
    "ax.set_title('Best-Worst Client Gap')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Variance\n",
    "ax = axes[1, 0]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    var = all_results[method]['history']['variance']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, var, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Variance')\n",
    "ax.set_title('Per-Client Accuracy Variance')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Min-Max Range\n",
    "ax = axes[1, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    min_acc = all_results[method]['history']['min_client_acc']\n",
    "    max_acc = all_results[method]['history']['max_client_acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.fill_between(rounds, min_acc, max_acc, color=colors.get(method, '#95a5a6'), alpha=0.2)\n",
    "    ax.plot(rounds, min_acc, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=1.5)\n",
    "    ax.plot(rounds, max_acc, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=1.5, label=name)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Min-Max Client Accuracy Range')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final Per-Client Accuracy\n",
    "ax = axes[1, 2]\n",
    "x = np.arange(N_CLIENTS)\n",
    "width = 0.25\n",
    "for i, method in enumerate(method_names):\n",
    "    name = all_results[method]['name']\n",
    "    client_accs = all_results[method]['history']['client_accuracies'][-1]\n",
    "    ax.bar(x + i*width, client_accs, width, label=name, color=colors.get(method, '#95a5a6'), alpha=0.8)\n",
    "ax.set_xlabel('Client ID')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Per-Client Accuracy (Final Round)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cifar10_v5_simplified_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìÅ Results saved to: cifar10_v5_simplified_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff87913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODELS AND RESULTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs('results_cifar10_v5_simplified', exist_ok=True)\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    filename = f\"results_cifar10_v5_simplified/{name.replace('=', '').replace('.', '_').replace('Œ≥', 'gamma')}_CIFAR10.pth\"\n",
    "    \n",
    "    save_dict = {\n",
    "        'model_state_dict': all_results[method]['model'].state_dict(),\n",
    "        'history': all_results[method]['history'],\n",
    "        'config': {\n",
    "            'n_rounds': N_ROUNDS,\n",
    "            'n_clients': N_CLIENTS,\n",
    "            'classes_per_client': CLASSES_PER_CLIENT,\n",
    "            'device': str(DEVICE),\n",
    "            'fedprox': False,\n",
    "            'gradient_clipping': False,\n",
    "            'aggregation_method': 'V2_LINEAR'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if method != 'FedAvg':\n",
    "        gamma_val = float(name.split('=')[1]) if '=' in name else 0.5\n",
    "        save_dict['config']['momentum'] = MOMENTUM\n",
    "        save_dict['config']['warmup_rounds'] = WARMUP_ROUNDS\n",
    "        save_dict['config']['gamma'] = gamma_val\n",
    "    \n",
    "    torch.save(save_dict, filename)\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "\n",
    "with open('results_cifar10_v5_simplified/all_results_summary.pkl', 'wb') as f:\n",
    "    summary = {\n",
    "        method: {\n",
    "            'name': all_results[method]['name'],\n",
    "            'history': all_results[method]['history'],\n",
    "            'final_acc': all_results[method]['history']['acc'][-1],\n",
    "            'final_jfi': all_results[method]['history']['jfi'][-1]\n",
    "        }\n",
    "        for method in method_names\n",
    "    }\n",
    "    pickle.dump(summary, f)\n",
    "print(\"‚úÖ Saved: results_cifar10_v5_simplified/all_results_summary.pkl\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"üìä FINAL SUMMARY (Fed-Audit-GAN v5.0 - SIMPLIFIED)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üîµ BASELINE:\")\n",
    "print(f\"   FedAvg: {all_results['FedAvg']['history']['acc'][-1]:.2f}% accuracy, JFI={all_results['FedAvg']['history']['jfi'][-1]:.4f}\")\n",
    "print(\"\\nüü¢ FED-AUDIT-GAN v5.0:\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    print(f\"   {name}: {acc:.2f}% accuracy, JFI={jfi:.4f}, Gap={gap:.2f}%\")\n",
    "\n",
    "print(\"\\n‚≠ê V5 SIMPLIFICATIONS:\")\n",
    "print(\"   ‚ùå REMOVED: FedProx\")\n",
    "print(\"   ‚ùå REMOVED: Gradient clipping\")\n",
    "print(\"   ‚úÖ V2 Linear: Weight = (1-Œ≥)√óAcc + Œ≥√óFair\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nüìä Check WandB: https://wandb.ai\")\n",
    "print(f\"   Project: FED_AUDIT_GAN_TEST_5_CIFAR10\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
