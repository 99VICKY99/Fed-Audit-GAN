{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58db9c20",
   "metadata": {},
   "source": [
    "# Fed-Audit-GAN V9: Proper Fairness Auditing Framework\n",
    "\n",
    "## Key Improvements Over Previous Versions:\n",
    "\n",
    "1. **Real Sensitive Attributes**: Using Adult Income dataset with actual protected attributes (sex, race)\n",
    "2. **Proper (X, Y, A) Framework**: Clear separation of features, task label, and sensitive attribute\n",
    "3. **Counterfactual Generator**: GAN generates x' from x where only A changes\n",
    "4. **Adversarial Bias Training**: Generator learns to expose model bias\n",
    "5. **Client-Level Fairness Contribution**: Per-client delta measurement\n",
    "6. **Proper Fairness Metrics**: DP gap, EO gap, accuracy-fairness tradeoffs\n",
    "7. **Baseline Comparisons**: FedAvg, FairFed, Validation-based auditing\n",
    "8. **WandB Integration**: Full experiment tracking\n",
    "\n",
    "### Framework Definition:\n",
    "- **X**: Input features (age, education, occupation, etc.)\n",
    "- **Y**: Task label (income >50K or <=50K)\n",
    "- **A**: Sensitive attribute (sex, race)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install and Import Dependencies\n",
    "!pip install wandb pandas numpy torch scikit-learn matplotlib seaborn tqdm -q\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe426030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Initialize WandB\n",
    "import wandb\n",
    "\n",
    "# Initialize wandb - you'll need to login first time\n",
    "wandb.login()\n",
    "\n",
    "# Configuration for the experiment\n",
    "CONFIG = {\n",
    "    # Data settings\n",
    "    'dataset': 'adult_income',\n",
    "    'sensitive_attribute': 'sex',  # Can be 'sex' or 'race'\n",
    "    'test_size': 0.2,\n",
    "    \n",
    "    # FL settings\n",
    "    'num_clients': 10,\n",
    "    'num_rounds': 50,\n",
    "    'local_epochs': 3,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.001,\n",
    "    'client_fraction': 0.5,  # Fraction of clients per round\n",
    "    \n",
    "    # Model settings\n",
    "    'hidden_dims': [128, 64, 32],\n",
    "    \n",
    "    # GAN Auditor settings\n",
    "    'generator_hidden_dims': [64, 32],\n",
    "    'discriminator_hidden_dims': [64, 32],\n",
    "    'auditor_epochs': 10,\n",
    "    'auditor_lr': 0.0002,\n",
    "    'lambda_realism': 1.0,\n",
    "    'lambda_bias': 0.5,\n",
    "    'lambda_similarity': 0.3,\n",
    "    \n",
    "    # Fairness settings\n",
    "    'fairness_weight': 0.3,  # Weight for fairness in aggregation\n",
    "    \n",
    "    # Experiment settings\n",
    "    'seed': SEED,\n",
    "}\n",
    "\n",
    "# Initialize wandb run\n",
    "run = wandb.init(\n",
    "    project=\"Fed-Audit-GAN-V9\",\n",
    "    name=\"adult_income_full_experiment\",\n",
    "    config=CONFIG,\n",
    "    tags=[\"v9\", \"adult-income\", \"counterfactual-gan\", \"fairness-audit\"],\n",
    ")\n",
    "\n",
    "print(\"WandB initialized successfully!\")\n",
    "print(f\"Run URL: {wandb.run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c87459",
   "metadata": {},
   "source": [
    "## Phase 1: Data Loading and Preprocessing\n",
    "\n",
    "### Adult Income Dataset\n",
    "- **Task (Y)**: Predict if income > $50K\n",
    "- **Sensitive Attribute (A)**: Sex (Male/Female) or Race\n",
    "- **Features (X)**: Age, education, occupation, hours-per-week, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7765d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and Preprocess Adult Income Dataset\n",
    "\n",
    "class AdultIncomeDataset(Dataset):\n",
    "    \"\"\"Adult Income Dataset with proper (X, Y, A) separation.\"\"\"\n",
    "    \n",
    "    def __init__(self, features: np.ndarray, labels: np.ndarray, \n",
    "                 sensitive_attrs: np.ndarray, feature_names: List[str]):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.sensitive_attrs = torch.LongTensor(sensitive_attrs)\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],       # X\n",
    "            'label': self.labels[idx],            # Y\n",
    "            'sensitive': self.sensitive_attrs[idx] # A\n",
    "        }\n",
    "\n",
    "\n",
    "def load_adult_income_dataset(filepath: str, sensitive_attr: str = 'sex'):\n",
    "    \"\"\"\n",
    "    Load Adult Income dataset with proper preprocessing.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the CSV file\n",
    "        sensitive_attr: 'sex' or 'race'\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (features, labels, sensitive_attrs, feature_names, label_encoders)\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Clean column names (remove spaces)\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()\n",
    "    \n",
    "    # Handle missing values (marked as '?' in this dataset)\n",
    "    df = df.replace('?', np.nan)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Define column mappings based on common Adult Income dataset format\n",
    "    # The target is usually 'income' with values '>50K' or '<=50K'\n",
    "    target_col = None\n",
    "    for col in df.columns:\n",
    "        if 'income' in col.lower():\n",
    "            target_col = col\n",
    "            break\n",
    "    \n",
    "    if target_col is None:\n",
    "        # Try last column as target\n",
    "        target_col = df.columns[-1]\n",
    "    \n",
    "    print(f\"\\nTarget column: {target_col}\")\n",
    "    print(f\"Target distribution:\\n{df[target_col].value_counts()}\")\n",
    "    \n",
    "    # Encode target (Y)\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df[target_col].str.strip())\n",
    "    \n",
    "    # Find and encode sensitive attribute (A)\n",
    "    sensitive_col = None\n",
    "    for col in df.columns:\n",
    "        if sensitive_attr.lower() in col.lower():\n",
    "            sensitive_col = col\n",
    "            break\n",
    "    \n",
    "    if sensitive_col is None:\n",
    "        raise ValueError(f\"Sensitive attribute '{sensitive_attr}' not found in dataset\")\n",
    "    \n",
    "    print(f\"\\nSensitive attribute column: {sensitive_col}\")\n",
    "    print(f\"Sensitive attribute distribution:\\n{df[sensitive_col].value_counts()}\")\n",
    "    \n",
    "    sensitive_encoder = LabelEncoder()\n",
    "    df['sensitive'] = sensitive_encoder.fit_transform(df[sensitive_col].str.strip())\n",
    "    \n",
    "    # Define feature columns (X) - exclude target and sensitive\n",
    "    exclude_cols = [target_col, sensitive_col, 'label', 'sensitive']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"\\nFeature columns: {feature_cols}\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    categorical_cols = df[feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_cols = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    print(f\"\\nCategorical columns: {categorical_cols}\")\n",
    "    print(f\"Numerical columns: {numerical_cols}\")\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    df_encoded = pd.get_dummies(df[feature_cols], columns=categorical_cols, drop_first=True)\n",
    "    \n",
    "    # Normalize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    for col in numerical_cols:\n",
    "        if col in df_encoded.columns:\n",
    "            df_encoded[col] = scaler.fit_transform(df_encoded[[col]])\n",
    "    \n",
    "    # Extract arrays\n",
    "    features = df_encoded.values.astype(np.float32)\n",
    "    labels = df['label'].values\n",
    "    sensitive_attrs = df['sensitive'].values\n",
    "    feature_names = df_encoded.columns.tolist()\n",
    "    \n",
    "    print(f\"\\nFinal feature shape: {features.shape}\")\n",
    "    print(f\"Number of features: {len(feature_names)}\")\n",
    "    \n",
    "    encoders = {\n",
    "        'label': label_encoder,\n",
    "        'sensitive': sensitive_encoder,\n",
    "        'scaler': scaler\n",
    "    }\n",
    "    \n",
    "    return features, labels, sensitive_attrs, feature_names, encoders\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "# NOTE: Download adult.csv from https://www.kaggle.com/datasets/wenruliu/adult-income-dataset\n",
    "# and place it in the data folder\n",
    "\n",
    "DATA_PATH = './data/adult.csv'  # Update this path to your dataset location\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"\\nâš ï¸ Dataset not found at {DATA_PATH}\")\n",
    "    print(\"Please download from: https://www.kaggle.com/datasets/wenruliu/adult-income-dataset\")\n",
    "    print(\"And place the adult.csv file in the ./data/ folder\")\n",
    "else:\n",
    "    features, labels, sensitive_attrs, feature_names, encoders = load_adult_income_dataset(\n",
    "        DATA_PATH, \n",
    "        sensitive_attr=CONFIG['sensitive_attribute']\n",
    "    )\n",
    "    \n",
    "    # Log data statistics to wandb\n",
    "    wandb.log({\n",
    "        'data/num_samples': len(labels),\n",
    "        'data/num_features': len(feature_names),\n",
    "        'data/positive_rate': labels.mean(),\n",
    "        'data/sensitive_attr_rate': sensitive_attrs.mean(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbc93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create Train/Test Split and Analyze Fairness in Data\n",
    "\n",
    "def analyze_data_fairness(labels: np.ndarray, sensitive_attrs: np.ndarray, \n",
    "                          sensitive_name: str = 'sensitive'):\n",
    "    \"\"\"Analyze fairness characteristics of the dataset.\"\"\"\n",
    "    \n",
    "    # Group statistics\n",
    "    groups = np.unique(sensitive_attrs)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"DATA FAIRNESS ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    stats = {}\n",
    "    for g in groups:\n",
    "        mask = sensitive_attrs == g\n",
    "        group_labels = labels[mask]\n",
    "        stats[g] = {\n",
    "            'count': mask.sum(),\n",
    "            'positive_rate': group_labels.mean(),\n",
    "            'percentage': mask.mean() * 100\n",
    "        }\n",
    "        print(f\"\\nGroup {g}:\")\n",
    "        print(f\"  Count: {stats[g]['count']} ({stats[g]['percentage']:.1f}%)\")\n",
    "        print(f\"  Positive rate: {stats[g]['positive_rate']:.3f}\")\n",
    "    \n",
    "    # Demographic Parity Gap in data\n",
    "    positive_rates = [stats[g]['positive_rate'] for g in groups]\n",
    "    dp_gap = max(positive_rates) - min(positive_rates)\n",
    "    print(f\"\\nðŸ“Š Demographic Parity Gap in Data: {dp_gap:.3f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Group distribution\n",
    "    group_counts = [stats[g]['count'] for g in groups]\n",
    "    axes[0].bar(groups, group_counts, color=['steelblue', 'coral'])\n",
    "    axes[0].set_xlabel('Sensitive Attribute')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Group Distribution')\n",
    "    \n",
    "    # Positive rates by group\n",
    "    axes[1].bar(groups, positive_rates, color=['steelblue', 'coral'])\n",
    "    axes[1].set_xlabel('Sensitive Attribute')\n",
    "    axes[1].set_ylabel('Positive Rate (Income >50K)')\n",
    "    axes[1].set_title('Positive Rate by Group')\n",
    "    axes[1].axhline(y=labels.mean(), color='red', linestyle='--', label='Overall')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data_fairness_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    wandb.log({'data/fairness_analysis': wandb.Image('data_fairness_analysis.png')})\n",
    "    plt.show()\n",
    "    \n",
    "    return stats, dp_gap\n",
    "\n",
    "# Analyze data fairness\n",
    "if 'features' in dir():\n",
    "    data_stats, data_dp_gap = analyze_data_fairness(\n",
    "        labels, sensitive_attrs, \n",
    "        CONFIG['sensitive_attribute']\n",
    "    )\n",
    "    \n",
    "    wandb.log({'data/demographic_parity_gap': data_dp_gap})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Create Federated Learning Client Partitions\n",
    "\n",
    "def create_heterogeneous_client_partitions(\n",
    "    features: np.ndarray,\n",
    "    labels: np.ndarray, \n",
    "    sensitive_attrs: np.ndarray,\n",
    "    num_clients: int,\n",
    "    bias_strength: float = 0.7  # How biased some clients are\n",
    ") -> Dict[int, Dict]:\n",
    "    \"\"\"\n",
    "    Create heterogeneous client partitions where:\n",
    "    - Some clients have mostly majority group data (biased)\n",
    "    - Some clients have diverse data (fair)\n",
    "    - Some clients have mostly minority group data (fairness-critical)\n",
    "    \n",
    "    This is REALISTIC for FL and creates measurable fairness contribution.\n",
    "    \"\"\"\n",
    "    n_samples = len(labels)\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    # Separate by sensitive attribute\n",
    "    group_0_indices = indices[sensitive_attrs == 0]\n",
    "    group_1_indices = indices[sensitive_attrs == 1]\n",
    "    \n",
    "    np.random.shuffle(group_0_indices)\n",
    "    np.random.shuffle(group_1_indices)\n",
    "    \n",
    "    # Define client types\n",
    "    # 40% biased toward group 0, 40% biased toward group 1, 20% balanced\n",
    "    n_biased_0 = int(num_clients * 0.4)\n",
    "    n_biased_1 = int(num_clients * 0.4)\n",
    "    n_balanced = num_clients - n_biased_0 - n_biased_1\n",
    "    \n",
    "    client_data = {}\n",
    "    client_types = {}\n",
    "    \n",
    "    # Track used indices\n",
    "    g0_ptr, g1_ptr = 0, 0\n",
    "    samples_per_client = n_samples // num_clients\n",
    "    \n",
    "    for client_id in range(num_clients):\n",
    "        if client_id < n_biased_0:\n",
    "            # Biased toward group 0 (majority bias)\n",
    "            n_from_g0 = int(samples_per_client * bias_strength)\n",
    "            n_from_g1 = samples_per_client - n_from_g0\n",
    "            client_types[client_id] = 'biased_majority'\n",
    "        elif client_id < n_biased_0 + n_biased_1:\n",
    "            # Biased toward group 1 (minority data - fairness critical)\n",
    "            n_from_g1 = int(samples_per_client * bias_strength)\n",
    "            n_from_g0 = samples_per_client - n_from_g1\n",
    "            client_types[client_id] = 'fairness_critical'\n",
    "        else:\n",
    "            # Balanced\n",
    "            n_from_g0 = samples_per_client // 2\n",
    "            n_from_g1 = samples_per_client - n_from_g0\n",
    "            client_types[client_id] = 'balanced'\n",
    "        \n",
    "        # Get indices (with wraparound if needed)\n",
    "        g0_end = min(g0_ptr + n_from_g0, len(group_0_indices))\n",
    "        g1_end = min(g1_ptr + n_from_g1, len(group_1_indices))\n",
    "        \n",
    "        client_indices = np.concatenate([\n",
    "            group_0_indices[g0_ptr:g0_end],\n",
    "            group_1_indices[g1_ptr:g1_end]\n",
    "        ])\n",
    "        \n",
    "        g0_ptr = g0_end % len(group_0_indices)\n",
    "        g1_ptr = g1_end % len(group_1_indices)\n",
    "        \n",
    "        np.random.shuffle(client_indices)\n",
    "        \n",
    "        client_data[client_id] = {\n",
    "            'indices': client_indices,\n",
    "            'type': client_types[client_id],\n",
    "            'group_0_ratio': (sensitive_attrs[client_indices] == 0).mean(),\n",
    "            'positive_rate': labels[client_indices].mean()\n",
    "        }\n",
    "    \n",
    "    return client_data\n",
    "\n",
    "\n",
    "# Create train/test split\n",
    "if 'features' in dir():\n",
    "    X_train, X_test, y_train, y_test, a_train, a_test = train_test_split(\n",
    "        features, labels, sensitive_attrs,\n",
    "        test_size=CONFIG['test_size'],\n",
    "        random_state=SEED,\n",
    "        stratify=labels  # Stratify by label\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(y_train)} samples\")\n",
    "    print(f\"Test set: {len(y_test)} samples\")\n",
    "    \n",
    "    # Create client partitions\n",
    "    client_partitions = create_heterogeneous_client_partitions(\n",
    "        X_train, y_train, a_train,\n",
    "        num_clients=CONFIG['num_clients'],\n",
    "        bias_strength=0.7\n",
    "    )\n",
    "    \n",
    "    # Visualize client heterogeneity\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLIENT DATA DISTRIBUTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for cid, cdata in client_partitions.items():\n",
    "        print(f\"Client {cid} ({cdata['type']}): \"\n",
    "              f\"{len(cdata['indices'])} samples, \"\n",
    "              f\"Group 0: {cdata['group_0_ratio']:.1%}, \"\n",
    "              f\"Positive: {cdata['positive_rate']:.1%}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = AdultIncomeDataset(X_train, y_train, a_train, feature_names)\n",
    "    test_dataset = AdultIncomeDataset(X_test, y_test, a_test, feature_names)\n",
    "    \n",
    "    # Log to wandb\n",
    "    client_types_log = {cid: cdata['type'] for cid, cdata in client_partitions.items()}\n",
    "    wandb.log({\n",
    "        'clients/partition_info': wandb.Table(\n",
    "            columns=['client_id', 'type', 'samples', 'group_0_ratio', 'positive_rate'],\n",
    "            data=[[cid, cdata['type'], len(cdata['indices']), \n",
    "                   cdata['group_0_ratio'], cdata['positive_rate']] \n",
    "                  for cid, cdata in client_partitions.items()]\n",
    "        )\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426b11b",
   "metadata": {},
   "source": [
    "## Phase 2: Model Architecture\n",
    "\n",
    "Define the global model and client-specific training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65be4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define Global Model Architecture\n",
    "\n",
    "class GlobalClassifier(nn.Module):\n",
    "    \"\"\"Global classifier for income prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [128, 64, 32]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, 2))  # Binary classification\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"Get probability predictions.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            return F.softmax(logits, dim=1)\n",
    "    \n",
    "    def get_embeddings(self, x):\n",
    "        \"\"\"Get intermediate embeddings before final layer.\"\"\"\n",
    "        # Forward through all but last layer\n",
    "        for layer in list(self.network.children())[:-1]:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "if 'features' in dir():\n",
    "    input_dim = features.shape[1]\n",
    "    global_model = GlobalClassifier(input_dim, CONFIG['hidden_dims']).to(device)\n",
    "    \n",
    "    print(f\"Model architecture:\")\n",
    "    print(global_model)\n",
    "    print(f\"\\nTotal parameters: {count_parameters(global_model):,}\")\n",
    "    \n",
    "    wandb.log({'model/parameters': count_parameters(global_model)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1b080",
   "metadata": {},
   "source": [
    "## Phase 3: Counterfactual GAN Auditor\n",
    "\n",
    "### Key Innovation: The GAN generates counterfactuals where ONLY the sensitive attribute changes\n",
    "\n",
    "This makes the GAN:\n",
    "1. **Not replaceable by a validation set** - it generates new samples\n",
    "2. **Semantically meaningful** - probes the decision boundary along the sensitive attribute axis\n",
    "3. **Adversarial to bias** - learns to find where the model discriminates unfairly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Counterfactual Generator\n",
    "\n",
    "class CounterfactualGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generates counterfactual samples x' from x where:\n",
    "    - Task-relevant features are preserved\n",
    "    - Sensitive attribute is \"flipped\"\n",
    "    \n",
    "    This is the CORE of Fed-Audit-GAN's novelty.\n",
    "    \n",
    "    Input: (x, a, a') where a is current sensitive attr, a' is target\n",
    "    Output: x' that looks like x but with sensitive attribute a'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [64, 32]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input: features + source sensitive attr + target sensitive attr\n",
    "        # We encode sensitive attrs as one-hot (2 classes each)\n",
    "        total_input = input_dim + 4  # x + one_hot(a) + one_hot(a')\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = total_input\n",
    "        for hidden_dim in hidden_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder (outputs delta to add to original features)\n",
    "        decoder_layers = []\n",
    "        for hidden_dim in reversed(hidden_dims[:-1]):\n",
    "            decoder_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.BatchNorm1d(hidden_dim)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        decoder_layers.append(nn.Tanh())  # Bounded perturbation\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.perturbation_scale = 0.5  # How much to scale the perturbation\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, a_source: torch.Tensor, a_target: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Generate counterfactual.\n",
    "        \n",
    "        Args:\n",
    "            x: Original features [batch, input_dim]\n",
    "            a_source: Current sensitive attribute [batch]\n",
    "            a_target: Target sensitive attribute [batch]\n",
    "        \n",
    "        Returns:\n",
    "            x': Counterfactual features [batch, input_dim]\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # One-hot encode sensitive attributes\n",
    "        a_source_onehot = F.one_hot(a_source, num_classes=2).float()\n",
    "        a_target_onehot = F.one_hot(a_target, num_classes=2).float()\n",
    "        \n",
    "        # Concatenate inputs\n",
    "        combined = torch.cat([x, a_source_onehot, a_target_onehot], dim=1)\n",
    "        \n",
    "        # Generate perturbation\n",
    "        encoded = self.encoder(combined)\n",
    "        delta = self.decoder(encoded) * self.perturbation_scale\n",
    "        \n",
    "        # Counterfactual = original + learned perturbation\n",
    "        x_cf = x + delta\n",
    "        \n",
    "        return x_cf, delta\n",
    "\n",
    "\n",
    "class BiasDiscriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator that tries to distinguish:\n",
    "    1. Real samples from counterfactuals (realism)\n",
    "    2. Predict sensitive attribute from features (bias detection)\n",
    "    \n",
    "    This dual objective helps the generator create realistic counterfactuals\n",
    "    that preserve task-relevant features while changing sensitive-attribute-correlated features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dims: List[int] = [64, 32]):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        shared_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            shared_layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.LeakyReLU(0.2),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.shared = nn.Sequential(*shared_layers)\n",
    "        \n",
    "        # Head 1: Real vs Fake\n",
    "        self.real_fake_head = nn.Linear(prev_dim, 1)\n",
    "        \n",
    "        # Head 2: Sensitive attribute prediction (bias detector)\n",
    "        self.sensitive_head = nn.Linear(prev_dim, 2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            real_fake_logit: Probability that x is real\n",
    "            sensitive_logits: Prediction of sensitive attribute\n",
    "        \"\"\"\n",
    "        features = self.shared(x)\n",
    "        real_fake = torch.sigmoid(self.real_fake_head(features))\n",
    "        sensitive = self.sensitive_head(features)\n",
    "        return real_fake, sensitive\n",
    "\n",
    "\n",
    "# Initialize GAN components\n",
    "if 'input_dim' in dir():\n",
    "    generator = CounterfactualGenerator(\n",
    "        input_dim, \n",
    "        CONFIG['generator_hidden_dims']\n",
    "    ).to(device)\n",
    "    \n",
    "    discriminator = BiasDiscriminator(\n",
    "        input_dim,\n",
    "        CONFIG['discriminator_hidden_dims']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Generator parameters: {count_parameters(generator):,}\")\n",
    "    print(f\"Discriminator parameters: {count_parameters(discriminator):,}\")\n",
    "    \n",
    "    wandb.log({\n",
    "        'model/generator_params': count_parameters(generator),\n",
    "        'model/discriminator_params': count_parameters(discriminator)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e342abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: GAN-Based Fairness Auditor\n",
    "\n",
    "class FairnessAuditor:\n",
    "    \"\"\"\n",
    "    The core Fed-Audit-GAN auditing system.\n",
    "    \n",
    "    Key responsibilities:\n",
    "    1. Train counterfactual generator against current global model\n",
    "    2. Generate fairness-probing counterfactuals\n",
    "    3. Measure bias using counterfactual prediction differences\n",
    "    4. Compute client-level fairness contributions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, generator: CounterfactualGenerator, \n",
    "                 discriminator: BiasDiscriminator,\n",
    "                 device: torch.device,\n",
    "                 config: Dict):\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.device = device\n",
    "        self.config = config\n",
    "        \n",
    "        self.g_optimizer = optim.Adam(generator.parameters(), lr=config['auditor_lr'], betas=(0.5, 0.999))\n",
    "        self.d_optimizer = optim.Adam(discriminator.parameters(), lr=config['auditor_lr'], betas=(0.5, 0.999))\n",
    "        \n",
    "        self.bias_history = []\n",
    "        \n",
    "    def train_auditor(self, global_model: nn.Module, dataloader: DataLoader, \n",
    "                      epochs: int = 10) -> Dict:\n",
    "        \"\"\"\n",
    "        Train the counterfactual generator AGAINST the current global model.\n",
    "        \n",
    "        Generator objective:\n",
    "        1. Generate realistic counterfactuals (fool discriminator)\n",
    "        2. Preserve task-relevant features (low L2 distance)\n",
    "        3. MAXIMIZE prediction difference when sensitive attr flips (expose bias)\n",
    "        \n",
    "        Discriminator objective:\n",
    "        1. Distinguish real from counterfactual\n",
    "        2. Predict sensitive attribute (detect remaining bias markers)\n",
    "        \"\"\"\n",
    "        global_model.eval()\n",
    "        self.generator.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        training_stats = defaultdict(list)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_g_loss = 0.0\n",
    "            epoch_d_loss = 0.0\n",
    "            epoch_bias_score = 0.0\n",
    "            n_batches = 0\n",
    "            \n",
    "            for batch in dataloader:\n",
    "                x = batch['features'].to(self.device)\n",
    "                a = batch['sensitive'].to(self.device)\n",
    "                y = batch['label'].to(self.device)\n",
    "                \n",
    "                batch_size = x.size(0)\n",
    "                \n",
    "                # Create flipped sensitive attributes (counterfactual target)\n",
    "                a_flipped = 1 - a\n",
    "                \n",
    "                # ==================== Train Discriminator ====================\n",
    "                self.d_optimizer.zero_grad()\n",
    "                \n",
    "                # Real samples\n",
    "                real_rf, real_sens = self.discriminator(x)\n",
    "                \n",
    "                # Generate counterfactuals\n",
    "                with torch.no_grad():\n",
    "                    x_cf, delta = self.generator(x, a, a_flipped)\n",
    "                \n",
    "                fake_rf, fake_sens = self.discriminator(x_cf.detach())\n",
    "                \n",
    "                # Discriminator loss\n",
    "                real_label = torch.ones(batch_size, 1).to(self.device)\n",
    "                fake_label = torch.zeros(batch_size, 1).to(self.device)\n",
    "                \n",
    "                d_loss_real = F.binary_cross_entropy(real_rf, real_label)\n",
    "                d_loss_fake = F.binary_cross_entropy(fake_rf, fake_label)\n",
    "                d_loss_sens = F.cross_entropy(real_sens, a)  # Predict sensitive attr\n",
    "                \n",
    "                d_loss = d_loss_real + d_loss_fake + 0.5 * d_loss_sens\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "                \n",
    "                # ==================== Train Generator ====================\n",
    "                self.g_optimizer.zero_grad()\n",
    "                \n",
    "                # Generate counterfactuals\n",
    "                x_cf, delta = self.generator(x, a, a_flipped)\n",
    "                \n",
    "                # 1. Fool discriminator (realism)\n",
    "                fake_rf, fake_sens = self.discriminator(x_cf)\n",
    "                g_loss_adv = F.binary_cross_entropy(fake_rf, real_label)\n",
    "                \n",
    "                # 2. Similarity loss (preserve task features)\n",
    "                g_loss_sim = F.mse_loss(x_cf, x)\n",
    "                \n",
    "                # 3. BIAS EXPOSURE: Maximize prediction difference\n",
    "                # This is the KEY loss that makes Fed-Audit-GAN unique\n",
    "                with torch.no_grad():\n",
    "                    pred_orig = F.softmax(global_model(x), dim=1)[:, 1]\n",
    "                \n",
    "                pred_cf = F.softmax(global_model(x_cf), dim=1)[:, 1]\n",
    "                \n",
    "                # We MAXIMIZE prediction difference (minimize negative)\n",
    "                pred_diff = torch.abs(pred_orig - pred_cf)\n",
    "                g_loss_bias = -pred_diff.mean()  # Negative because we want to maximize\n",
    "                \n",
    "                # 4. Sensitive attribute confusion (counterfactual should look like target group)\n",
    "                g_loss_sens = F.cross_entropy(fake_sens, a_flipped)\n",
    "                \n",
    "                # Combined generator loss\n",
    "                g_loss = (\n",
    "                    self.config['lambda_realism'] * g_loss_adv +\n",
    "                    self.config['lambda_similarity'] * g_loss_sim +\n",
    "                    self.config['lambda_bias'] * g_loss_bias +\n",
    "                    0.3 * g_loss_sens\n",
    "                )\n",
    "                \n",
    "                g_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "                \n",
    "                # Track statistics\n",
    "                epoch_g_loss += g_loss.item()\n",
    "                epoch_d_loss += d_loss.item()\n",
    "                epoch_bias_score += pred_diff.mean().item()\n",
    "                n_batches += 1\n",
    "            \n",
    "            training_stats['g_loss'].append(epoch_g_loss / n_batches)\n",
    "            training_stats['d_loss'].append(epoch_d_loss / n_batches)\n",
    "            training_stats['bias_score'].append(epoch_bias_score / n_batches)\n",
    "        \n",
    "        return dict(training_stats)\n",
    "    \n",
    "    def compute_counterfactual_bias(self, global_model: nn.Module, \n",
    "                                    dataloader: DataLoader) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute fairness metrics using counterfactual probes.\n",
    "        \n",
    "        This is what a validation set CANNOT do.\n",
    "        \"\"\"\n",
    "        global_model.eval()\n",
    "        self.generator.eval()\n",
    "        \n",
    "        all_pred_orig = []\n",
    "        all_pred_cf = []\n",
    "        all_labels = []\n",
    "        all_sensitive = []\n",
    "        all_pred_diff = []\n",
    "        boundary_crossings = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                x = batch['features'].to(self.device)\n",
    "                a = batch['sensitive'].to(self.device)\n",
    "                y = batch['label'].to(self.device)\n",
    "                \n",
    "                a_flipped = 1 - a\n",
    "                \n",
    "                # Generate counterfactuals\n",
    "                x_cf, _ = self.generator(x, a, a_flipped)\n",
    "                \n",
    "                # Get predictions\n",
    "                pred_orig = F.softmax(global_model(x), dim=1)\n",
    "                pred_cf = F.softmax(global_model(x_cf), dim=1)\n",
    "                \n",
    "                pred_class_orig = pred_orig.argmax(dim=1)\n",
    "                pred_class_cf = pred_cf.argmax(dim=1)\n",
    "                \n",
    "                # Count boundary crossings (predictions that flip)\n",
    "                crossings = (pred_class_orig != pred_class_cf).sum().item()\n",
    "                boundary_crossings += crossings\n",
    "                total_samples += x.size(0)\n",
    "                \n",
    "                # Store for detailed analysis\n",
    "                all_pred_orig.append(pred_orig[:, 1].cpu())\n",
    "                all_pred_cf.append(pred_cf[:, 1].cpu())\n",
    "                all_labels.append(y.cpu())\n",
    "                all_sensitive.append(a.cpu())\n",
    "                all_pred_diff.append(torch.abs(pred_orig[:, 1] - pred_cf[:, 1]).cpu())\n",
    "        \n",
    "        # Concatenate all\n",
    "        all_pred_orig = torch.cat(all_pred_orig)\n",
    "        all_pred_cf = torch.cat(all_pred_cf)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        all_sensitive = torch.cat(all_sensitive)\n",
    "        all_pred_diff = torch.cat(all_pred_diff)\n",
    "        \n",
    "        # Compute metrics\n",
    "        metrics = {\n",
    "            # Counterfactual Fairness Metrics (Fed-Audit-GAN specific)\n",
    "            'cf_prediction_gap': all_pred_diff.mean().item(),\n",
    "            'cf_prediction_gap_std': all_pred_diff.std().item(),\n",
    "            'boundary_crossing_rate': boundary_crossings / total_samples,\n",
    "            \n",
    "            # Group-conditioned counterfactual gaps\n",
    "            'cf_gap_group_0': all_pred_diff[all_sensitive == 0].mean().item(),\n",
    "            'cf_gap_group_1': all_pred_diff[all_sensitive == 1].mean().item(),\n",
    "        }\n",
    "        \n",
    "        # Latent Bias Discovery Rate (new metric)\n",
    "        # High prediction difference + boundary crossing = discovered bias\n",
    "        high_diff_threshold = all_pred_diff.mean() + all_pred_diff.std()\n",
    "        bias_discoveries = (all_pred_diff > high_diff_threshold).sum().item()\n",
    "        metrics['latent_bias_discovery_rate'] = bias_discoveries / total_samples\n",
    "        \n",
    "        self.bias_history.append(metrics['cf_prediction_gap'])\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compute_client_fairness_contribution(\n",
    "        self, \n",
    "        global_model: nn.Module,\n",
    "        client_update: Dict[str, torch.Tensor],\n",
    "        audit_dataloader: DataLoader,\n",
    "        learning_rate: float = 1.0\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute a client's fairness contribution by:\n",
    "        1. Measuring bias on global model\n",
    "        2. Hypothetically applying client update\n",
    "        3. Measuring bias on updated model\n",
    "        4. Contribution = Î”bias (positive if reduces bias)\n",
    "        \n",
    "        This is THE KEY INNOVATION that justifies Fed-Audit-GAN.\n",
    "        \"\"\"\n",
    "        # 1. Get bias before update\n",
    "        metrics_before = self.compute_counterfactual_bias(global_model, audit_dataloader)\n",
    "        bias_before = metrics_before['cf_prediction_gap']\n",
    "        \n",
    "        # 2. Apply client update hypothetically\n",
    "        updated_model = deepcopy(global_model)\n",
    "        with torch.no_grad():\n",
    "            for name, param in updated_model.named_parameters():\n",
    "                if name in client_update:\n",
    "                    param.add_(client_update[name] * learning_rate)\n",
    "        \n",
    "        # 3. Get bias after update\n",
    "        metrics_after = self.compute_counterfactual_bias(updated_model, audit_dataloader)\n",
    "        bias_after = metrics_after['cf_prediction_gap']\n",
    "        \n",
    "        # 4. Contribution = reduction in bias (positive is good)\n",
    "        contribution = bias_before - bias_after\n",
    "        \n",
    "        return contribution, metrics_before, metrics_after\n",
    "\n",
    "\n",
    "# Initialize auditor\n",
    "if 'generator' in dir():\n",
    "    auditor = FairnessAuditor(generator, discriminator, device, CONFIG)\n",
    "    print(\"FairnessAuditor initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff04f9fa",
   "metadata": {},
   "source": [
    "## Phase 4: Traditional Fairness Metrics\n",
    "\n",
    "We still need these for baseline comparison, but they are NOT the main claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0537b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Traditional Fairness Metrics\n",
    "\n",
    "class FairnessMetrics:\n",
    "    \"\"\"\n",
    "    Traditional fairness metrics for comparison with baselines.\n",
    "    \n",
    "    These are \"table stakes\" - Fed-Audit-GAN should be comparable or better,\n",
    "    but the main claim is about AUDITING, not optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def demographic_parity_difference(predictions: np.ndarray, \n",
    "                                       sensitive: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        DP = |P(Å¶=1|A=0) - P(Å¶=1|A=1)|\n",
    "        \n",
    "        Measures if positive predictions are equally distributed across groups.\n",
    "        Lower is better (0 = perfect parity).\n",
    "        \"\"\"\n",
    "        group_0_positive_rate = predictions[sensitive == 0].mean()\n",
    "        group_1_positive_rate = predictions[sensitive == 1].mean()\n",
    "        return abs(group_0_positive_rate - group_1_positive_rate)\n",
    "    \n",
    "    @staticmethod\n",
    "    def equalized_odds_difference(predictions: np.ndarray, \n",
    "                                   labels: np.ndarray,\n",
    "                                   sensitive: np.ndarray) -> Dict:\n",
    "        \"\"\"\n",
    "        EO measures TPR and FPR parity across groups.\n",
    "        \n",
    "        Returns TPR gap and FPR gap separately.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for y_true in [0, 1]:\n",
    "            mask_y = labels == y_true\n",
    "            \n",
    "            # Rates for each group\n",
    "            rates = []\n",
    "            for a in [0, 1]:\n",
    "                mask = mask_y & (sensitive == a)\n",
    "                if mask.sum() > 0:\n",
    "                    rates.append(predictions[mask].mean())\n",
    "                else:\n",
    "                    rates.append(0.0)\n",
    "            \n",
    "            metric_name = 'tpr_gap' if y_true == 1 else 'fpr_gap'\n",
    "            results[metric_name] = abs(rates[0] - rates[1])\n",
    "        \n",
    "        results['eo_gap'] = (results['tpr_gap'] + results['fpr_gap']) / 2\n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy_by_group(predictions: np.ndarray,\n",
    "                          labels: np.ndarray,\n",
    "                          sensitive: np.ndarray) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute accuracy for each group and overall.\n",
    "        \"\"\"\n",
    "        results = {\n",
    "            'overall': (predictions == labels).mean()\n",
    "        }\n",
    "        \n",
    "        for a in [0, 1]:\n",
    "            mask = sensitive == a\n",
    "            results[f'group_{a}'] = (predictions[mask] == labels[mask]).mean()\n",
    "        \n",
    "        results['accuracy_gap'] = abs(results['group_0'] - results['group_1'])\n",
    "        results['worst_group'] = min(results['group_0'], results['group_1'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_all(model: nn.Module, dataloader: DataLoader, \n",
    "                    device: torch.device) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute all traditional fairness metrics.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_sensitive = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                x = batch['features'].to(device)\n",
    "                y = batch['label']\n",
    "                a = batch['sensitive']\n",
    "                \n",
    "                logits = model(x)\n",
    "                preds = logits.argmax(dim=1).cpu()\n",
    "                \n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(y)\n",
    "                all_sensitive.append(a)\n",
    "        \n",
    "        preds = torch.cat(all_preds).numpy()\n",
    "        labels = torch.cat(all_labels).numpy()\n",
    "        sensitive = torch.cat(all_sensitive).numpy()\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # Demographic Parity\n",
    "        metrics['dp_gap'] = FairnessMetrics.demographic_parity_difference(preds, sensitive)\n",
    "        \n",
    "        # Equalized Odds\n",
    "        eo_metrics = FairnessMetrics.equalized_odds_difference(preds, labels, sensitive)\n",
    "        metrics.update(eo_metrics)\n",
    "        \n",
    "        # Accuracy\n",
    "        acc_metrics = FairnessMetrics.accuracy_by_group(preds, labels, sensitive)\n",
    "        metrics.update(acc_metrics)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "print(\"FairnessMetrics class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96eeb15",
   "metadata": {},
   "source": [
    "## Phase 5: Federated Learning with Fairness-Aware Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Client Training and Update Computation\n",
    "\n",
    "class FLClient:\n",
    "    \"\"\"\n",
    "    Federated Learning Client.\n",
    "    \n",
    "    Each client trains locally and returns model updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client_id: int, data_indices: np.ndarray, \n",
    "                 dataset: Dataset, client_type: str):\n",
    "        self.client_id = client_id\n",
    "        self.data_indices = data_indices\n",
    "        self.dataset = dataset\n",
    "        self.client_type = client_type\n",
    "        \n",
    "    def train(self, global_model: nn.Module, config: Dict, \n",
    "              device: torch.device) -> Tuple[Dict[str, torch.Tensor], Dict]:\n",
    "        \"\"\"\n",
    "        Train locally and return model update (delta from global model).\n",
    "        \"\"\"\n",
    "        # Create local model copy\n",
    "        local_model = deepcopy(global_model)\n",
    "        local_model.train()\n",
    "        \n",
    "        # Create dataloader\n",
    "        subset = Subset(self.dataset, self.data_indices)\n",
    "        loader = DataLoader(subset, batch_size=config['batch_size'], shuffle=True)\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = optim.Adam(local_model.parameters(), lr=config['learning_rate'])\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Training stats\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for epoch in range(config['local_epochs']):\n",
    "            for batch in loader:\n",
    "                x = batch['features'].to(device)\n",
    "                y = batch['label'].to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                logits = local_model(x)\n",
    "                loss = criterion(logits, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item() * x.size(0)\n",
    "                correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "                total += x.size(0)\n",
    "        \n",
    "        # Compute update (delta from global)\n",
    "        update = {}\n",
    "        with torch.no_grad():\n",
    "            for (name, local_param), (_, global_param) in zip(\n",
    "                local_model.named_parameters(), \n",
    "                global_model.named_parameters()\n",
    "            ):\n",
    "                update[name] = local_param - global_param\n",
    "        \n",
    "        stats = {\n",
    "            'loss': total_loss / total,\n",
    "            'accuracy': correct / total,\n",
    "            'samples': len(self.data_indices)\n",
    "        }\n",
    "        \n",
    "        return update, stats\n",
    "\n",
    "\n",
    "print(\"FLClient class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c03f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Aggregation Strategies\n",
    "\n",
    "class AggregationStrategy:\n",
    "    \"\"\"\n",
    "    Different aggregation strategies for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def fedavg(updates: List[Dict[str, torch.Tensor]], \n",
    "               sample_counts: List[int]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Standard FedAvg: weighted average by sample count.\n",
    "        \"\"\"\n",
    "        total_samples = sum(sample_counts)\n",
    "        weights = [n / total_samples for n in sample_counts]\n",
    "        \n",
    "        aggregated = {}\n",
    "        for name in updates[0].keys():\n",
    "            aggregated[name] = sum(\n",
    "                w * updates[i][name] for i, w in enumerate(weights)\n",
    "            )\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    @staticmethod\n",
    "    def fairfed(updates: List[Dict[str, torch.Tensor]],\n",
    "                sample_counts: List[int],\n",
    "                accuracy_scores: List[float],\n",
    "                fairness_scores: List[float],\n",
    "                alpha: float = 0.5) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        FairFed-style aggregation: balance accuracy and fairness.\n",
    "        \"\"\"\n",
    "        # Normalize scores\n",
    "        acc_weights = np.array(accuracy_scores)\n",
    "        fair_weights = np.array(fairness_scores)\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if acc_weights.std() > 0:\n",
    "            acc_weights = (acc_weights - acc_weights.min()) / (acc_weights.max() - acc_weights.min() + 1e-8)\n",
    "        if fair_weights.std() > 0:\n",
    "            fair_weights = (fair_weights - fair_weights.min()) / (fair_weights.max() - fair_weights.min() + 1e-8)\n",
    "        \n",
    "        # Combined weights\n",
    "        combined = alpha * acc_weights + (1 - alpha) * fair_weights\n",
    "        combined = combined / combined.sum()\n",
    "        \n",
    "        aggregated = {}\n",
    "        for name in updates[0].keys():\n",
    "            aggregated[name] = sum(\n",
    "                combined[i] * updates[i][name] for i in range(len(updates))\n",
    "            )\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    @staticmethod\n",
    "    def fed_audit_gan(updates: List[Dict[str, torch.Tensor]],\n",
    "                      sample_counts: List[int],\n",
    "                      accuracy_scores: List[float],\n",
    "                      fairness_contributions: List[float],\n",
    "                      fairness_weight: float = 0.3) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Fed-Audit-GAN aggregation: uses GAN-computed fairness contributions.\n",
    "        \n",
    "        Key difference from FairFed:\n",
    "        - fairness_contributions are computed via counterfactual auditing\n",
    "        - Rewards clients that REDUCE counterfactual bias\n",
    "        \"\"\"\n",
    "        # Base weight from sample count\n",
    "        total_samples = sum(sample_counts)\n",
    "        base_weights = np.array([n / total_samples for n in sample_counts])\n",
    "        \n",
    "        # Fairness contribution adjustment\n",
    "        # Positive contribution = client reduces bias = higher weight\n",
    "        fair_contrib = np.array(fairness_contributions)\n",
    "        \n",
    "        # Shift to make all positive\n",
    "        fair_contrib = fair_contrib - fair_contrib.min() + 0.1\n",
    "        fair_contrib = fair_contrib / fair_contrib.sum()\n",
    "        \n",
    "        # Combined weights\n",
    "        final_weights = (1 - fairness_weight) * base_weights + fairness_weight * fair_contrib\n",
    "        final_weights = final_weights / final_weights.sum()\n",
    "        \n",
    "        aggregated = {}\n",
    "        for name in updates[0].keys():\n",
    "            aggregated[name] = sum(\n",
    "                final_weights[i] * updates[i][name] for i in range(len(updates))\n",
    "            )\n",
    "        \n",
    "        return aggregated, final_weights\n",
    "\n",
    "\n",
    "print(\"AggregationStrategy class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf2bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Federated Learning Training Loop\n",
    "\n",
    "class FederatedTrainer:\n",
    "    \"\"\"\n",
    "    Main FL training loop with Fed-Audit-GAN integration.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, global_model: nn.Module, auditor: FairnessAuditor,\n",
    "                 clients: List[FLClient], test_dataset: Dataset,\n",
    "                 audit_dataset: Dataset, config: Dict, device: torch.device):\n",
    "        self.global_model = global_model\n",
    "        self.auditor = auditor\n",
    "        self.clients = clients\n",
    "        self.test_loader = DataLoader(test_dataset, batch_size=config['batch_size'])\n",
    "        self.audit_loader = DataLoader(audit_dataset, batch_size=config['batch_size'])\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        self.history = defaultdict(list)\n",
    "        \n",
    "    def train_round(self, round_num: int, method: str = 'fed_audit_gan') -> Dict:\n",
    "        \"\"\"\n",
    "        Execute one FL round.\n",
    "        \n",
    "        Args:\n",
    "            round_num: Current round number\n",
    "            method: 'fedavg', 'fairfed', or 'fed_audit_gan'\n",
    "        \"\"\"\n",
    "        # Select clients for this round\n",
    "        n_selected = max(1, int(len(self.clients) * self.config['client_fraction']))\n",
    "        selected_clients = np.random.choice(\n",
    "            self.clients, n_selected, replace=False\n",
    "        )\n",
    "        \n",
    "        # Collect updates\n",
    "        updates = []\n",
    "        sample_counts = []\n",
    "        accuracy_scores = []\n",
    "        client_ids = []\n",
    "        \n",
    "        for client in selected_clients:\n",
    "            update, stats = client.train(self.global_model, self.config, self.device)\n",
    "            updates.append(update)\n",
    "            sample_counts.append(stats['samples'])\n",
    "            accuracy_scores.append(stats['accuracy'])\n",
    "            client_ids.append(client.client_id)\n",
    "        \n",
    "        # Compute fairness contributions (Fed-Audit-GAN specific)\n",
    "        fairness_contributions = []\n",
    "        if method in ['fed_audit_gan', 'fairfed']:\n",
    "            for i, update in enumerate(updates):\n",
    "                if method == 'fed_audit_gan':\n",
    "                    contrib, _, _ = self.auditor.compute_client_fairness_contribution(\n",
    "                        self.global_model, update, self.audit_loader\n",
    "                    )\n",
    "                else:\n",
    "                    # For FairFed, use simple fairness metric\n",
    "                    contrib = accuracy_scores[i]  # Simplified\n",
    "                fairness_contributions.append(contrib)\n",
    "        \n",
    "        # Aggregate based on method\n",
    "        if method == 'fedavg':\n",
    "            aggregated_update = AggregationStrategy.fedavg(updates, sample_counts)\n",
    "            agg_weights = np.array(sample_counts) / sum(sample_counts)\n",
    "        elif method == 'fairfed':\n",
    "            aggregated_update = AggregationStrategy.fairfed(\n",
    "                updates, sample_counts, accuracy_scores, fairness_contributions\n",
    "            )\n",
    "            agg_weights = None\n",
    "        else:  # fed_audit_gan\n",
    "            aggregated_update, agg_weights = AggregationStrategy.fed_audit_gan(\n",
    "                updates, sample_counts, accuracy_scores, fairness_contributions,\n",
    "                self.config['fairness_weight']\n",
    "            )\n",
    "        \n",
    "        # Apply aggregated update to global model\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.global_model.named_parameters():\n",
    "                if name in aggregated_update:\n",
    "                    param.add_(aggregated_update[name])\n",
    "        \n",
    "        # Evaluate\n",
    "        traditional_metrics = FairnessMetrics.compute_all(\n",
    "            self.global_model, self.test_loader, self.device\n",
    "        )\n",
    "        \n",
    "        counterfactual_metrics = self.auditor.compute_counterfactual_bias(\n",
    "            self.global_model, self.audit_loader\n",
    "        )\n",
    "        \n",
    "        # Combine all metrics\n",
    "        round_metrics = {\n",
    "            'round': round_num,\n",
    "            'method': method,\n",
    "            **{f'trad/{k}': v for k, v in traditional_metrics.items()},\n",
    "            **{f'cf/{k}': v for k, v in counterfactual_metrics.items()},\n",
    "            'avg_client_accuracy': np.mean(accuracy_scores),\n",
    "        }\n",
    "        \n",
    "        if fairness_contributions:\n",
    "            round_metrics['avg_fairness_contribution'] = np.mean(fairness_contributions)\n",
    "            round_metrics['fairness_contribution_std'] = np.std(fairness_contributions)\n",
    "        \n",
    "        # Store history\n",
    "        for k, v in round_metrics.items():\n",
    "            if isinstance(v, (int, float)):\n",
    "                self.history[k].append(v)\n",
    "        \n",
    "        return round_metrics\n",
    "    \n",
    "    def train(self, num_rounds: int, method: str = 'fed_audit_gan',\n",
    "              retrain_auditor_every: int = 5) -> Dict:\n",
    "        \"\"\"\n",
    "        Full training loop.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Starting Federated Learning with {method.upper()}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for round_num in tqdm(range(num_rounds), desc=f'FL Rounds ({method})'):\n",
    "            # Retrain auditor periodically (keeps it adaptive)\n",
    "            if method == 'fed_audit_gan' and round_num % retrain_auditor_every == 0:\n",
    "                auditor_stats = self.auditor.train_auditor(\n",
    "                    self.global_model, self.audit_loader, \n",
    "                    epochs=self.config['auditor_epochs']\n",
    "                )\n",
    "                wandb.log({\n",
    "                    f'{method}/auditor_g_loss': auditor_stats['g_loss'][-1],\n",
    "                    f'{method}/auditor_d_loss': auditor_stats['d_loss'][-1],\n",
    "                    'round': round_num\n",
    "                })\n",
    "            \n",
    "            # Train round\n",
    "            round_metrics = self.train_round(round_num, method)\n",
    "            \n",
    "            # Log to wandb\n",
    "            wandb.log({f'{method}/{k}': v for k, v in round_metrics.items() \n",
    "                       if isinstance(v, (int, float))})\n",
    "            \n",
    "            # Print progress\n",
    "            if round_num % 10 == 0 or round_num == num_rounds - 1:\n",
    "                print(f\"\\nRound {round_num}: \"\n",
    "                      f\"Acc={round_metrics['trad/overall']:.3f}, \"\n",
    "                      f\"DP={round_metrics['trad/dp_gap']:.3f}, \"\n",
    "                      f\"EO={round_metrics['trad/eo_gap']:.3f}, \"\n",
    "                      f\"CF_Bias={round_metrics['cf/cf_prediction_gap']:.3f}\")\n",
    "        \n",
    "        return dict(self.history)\n",
    "\n",
    "\n",
    "print(\"FederatedTrainer class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da654ff3",
   "metadata": {},
   "source": [
    "## Phase 6: Run Experiments and Compare Methods\n",
    "\n",
    "We will compare:\n",
    "1. **FedAvg** - Standard FL (sanity baseline)\n",
    "2. **FairFed** - Fairness-aware FL baseline\n",
    "3. **Fed-Audit-GAN** - Our method with counterfactual auditing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241d8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Setup Experiment\n",
    "\n",
    "def setup_experiment():\n",
    "    \"\"\"Setup fresh experiment with clients and models.\"\"\"\n",
    "    \n",
    "    # Create clients\n",
    "    clients = []\n",
    "    for cid, cdata in client_partitions.items():\n",
    "        client = FLClient(\n",
    "            client_id=cid,\n",
    "            data_indices=cdata['indices'],\n",
    "            dataset=train_dataset,\n",
    "            client_type=cdata['type']\n",
    "        )\n",
    "        clients.append(client)\n",
    "    \n",
    "    print(f\"Created {len(clients)} clients\")\n",
    "    \n",
    "    # Create audit dataset (subset of training data for fairness probing)\n",
    "    audit_size = min(2000, len(train_dataset) // 4)\n",
    "    audit_indices = np.random.choice(len(train_dataset), audit_size, replace=False)\n",
    "    audit_dataset = Subset(train_dataset, audit_indices)\n",
    "    \n",
    "    return clients, audit_dataset\n",
    "\n",
    "\n",
    "if 'train_dataset' in dir():\n",
    "    clients, audit_dataset = setup_experiment()\n",
    "    print(f\"Audit dataset size: {len(audit_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Run FedAvg Baseline\n",
    "\n",
    "if 'clients' in dir():\n",
    "    # Fresh model for FedAvg\n",
    "    fedavg_model = GlobalClassifier(input_dim, CONFIG['hidden_dims']).to(device)\n",
    "    \n",
    "    # Fresh GAN components (not really used but needed for structure)\n",
    "    fedavg_generator = CounterfactualGenerator(input_dim, CONFIG['generator_hidden_dims']).to(device)\n",
    "    fedavg_discriminator = BiasDiscriminator(input_dim, CONFIG['discriminator_hidden_dims']).to(device)\n",
    "    fedavg_auditor = FairnessAuditor(fedavg_generator, fedavg_discriminator, device, CONFIG)\n",
    "    \n",
    "    # Trainer\n",
    "    fedavg_trainer = FederatedTrainer(\n",
    "        global_model=fedavg_model,\n",
    "        auditor=fedavg_auditor,\n",
    "        clients=clients,\n",
    "        test_dataset=test_dataset,\n",
    "        audit_dataset=audit_dataset,\n",
    "        config=CONFIG,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    fedavg_history = fedavg_trainer.train(\n",
    "        num_rounds=CONFIG['num_rounds'],\n",
    "        method='fedavg'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2659ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Run FairFed Baseline\n",
    "\n",
    "if 'clients' in dir():\n",
    "    # Fresh model for FairFed\n",
    "    fairfed_model = GlobalClassifier(input_dim, CONFIG['hidden_dims']).to(device)\n",
    "    \n",
    "    # Fresh GAN components\n",
    "    fairfed_generator = CounterfactualGenerator(input_dim, CONFIG['generator_hidden_dims']).to(device)\n",
    "    fairfed_discriminator = BiasDiscriminator(input_dim, CONFIG['discriminator_hidden_dims']).to(device)\n",
    "    fairfed_auditor = FairnessAuditor(fairfed_generator, fairfed_discriminator, device, CONFIG)\n",
    "    \n",
    "    # Trainer\n",
    "    fairfed_trainer = FederatedTrainer(\n",
    "        global_model=fairfed_model,\n",
    "        auditor=fairfed_auditor,\n",
    "        clients=clients,\n",
    "        test_dataset=test_dataset,\n",
    "        audit_dataset=audit_dataset,\n",
    "        config=CONFIG,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    fairfed_history = fairfed_trainer.train(\n",
    "        num_rounds=CONFIG['num_rounds'],\n",
    "        method='fairfed'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Run Fed-Audit-GAN (Our Method)\n",
    "\n",
    "if 'clients' in dir():\n",
    "    # Fresh model for Fed-Audit-GAN\n",
    "    fedaudit_model = GlobalClassifier(input_dim, CONFIG['hidden_dims']).to(device)\n",
    "    \n",
    "    # Fresh GAN components\n",
    "    fedaudit_generator = CounterfactualGenerator(input_dim, CONFIG['generator_hidden_dims']).to(device)\n",
    "    fedaudit_discriminator = BiasDiscriminator(input_dim, CONFIG['discriminator_hidden_dims']).to(device)\n",
    "    fedaudit_auditor = FairnessAuditor(fedaudit_generator, fedaudit_discriminator, device, CONFIG)\n",
    "    \n",
    "    # Trainer\n",
    "    fedaudit_trainer = FederatedTrainer(\n",
    "        global_model=fedaudit_model,\n",
    "        auditor=fedaudit_auditor,\n",
    "        clients=clients,\n",
    "        test_dataset=test_dataset,\n",
    "        audit_dataset=audit_dataset,\n",
    "        config=CONFIG,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train (with auditor retraining)\n",
    "    fedaudit_history = fedaudit_trainer.train(\n",
    "        num_rounds=CONFIG['num_rounds'],\n",
    "        method='fed_audit_gan',\n",
    "        retrain_auditor_every=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f664e",
   "metadata": {},
   "source": [
    "## Phase 7: Results Analysis and Visualization\n",
    "\n",
    "Create publication-ready plots and tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Create Comparison Plots\n",
    "\n",
    "def create_comparison_plots(histories: Dict[str, Dict], save_prefix: str = 'results'):\n",
    "    \"\"\"\n",
    "    Create publication-ready comparison plots.\n",
    "    \"\"\"\n",
    "    methods = list(histories.keys())\n",
    "    colors = {'fedavg': 'blue', 'fairfed': 'green', 'fed_audit_gan': 'red'}\n",
    "    labels = {'fedavg': 'FedAvg', 'fairfed': 'FairFed', 'fed_audit_gan': 'Fed-Audit-GAN'}\n",
    "    \n",
    "    # Figure 1: Traditional Fairness Metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    metrics_to_plot = [\n",
    "        ('trad/overall', 'Test Accuracy', True),\n",
    "        ('trad/dp_gap', 'Demographic Parity Gap â†“', False),\n",
    "        ('trad/eo_gap', 'Equalized Odds Gap â†“', False),\n",
    "        ('trad/worst_group', 'Worst-Group Accuracy â†‘', True),\n",
    "    ]\n",
    "    \n",
    "    for ax, (metric, title, higher_better) in zip(axes.flatten(), metrics_to_plot):\n",
    "        for method in methods:\n",
    "            if metric in histories[method]:\n",
    "                ax.plot(histories[method][metric], \n",
    "                       color=colors[method], \n",
    "                       label=labels[method],\n",
    "                       linewidth=2)\n",
    "        ax.set_xlabel('Round')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_prefix}_traditional_metrics.png', dpi=150, bbox_inches='tight')\n",
    "    wandb.log({'results/traditional_metrics': wandb.Image(f'{save_prefix}_traditional_metrics.png')})\n",
    "    plt.show()\n",
    "    \n",
    "    # Figure 2: Counterfactual Fairness Metrics (Fed-Audit-GAN specific)\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    cf_metrics = [\n",
    "        ('cf/cf_prediction_gap', 'Counterfactual Prediction Gap'),\n",
    "        ('cf/boundary_crossing_rate', 'Boundary Crossing Rate'),\n",
    "        ('cf/latent_bias_discovery_rate', 'Latent Bias Discovery Rate'),\n",
    "    ]\n",
    "    \n",
    "    for ax, (metric, title) in zip(axes.flatten(), cf_metrics):\n",
    "        for method in methods:\n",
    "            if metric in histories[method]:\n",
    "                ax.plot(histories[method][metric], \n",
    "                       color=colors[method], \n",
    "                       label=labels[method],\n",
    "                       linewidth=2)\n",
    "        ax.set_xlabel('Round')\n",
    "        ax.set_ylabel(title)\n",
    "        ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_prefix}_counterfactual_metrics.png', dpi=150, bbox_inches='tight')\n",
    "    wandb.log({'results/counterfactual_metrics': wandb.Image(f'{save_prefix}_counterfactual_metrics.png')})\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# Create plots\n",
    "if 'fedavg_history' in dir() and 'fairfed_history' in dir() and 'fedaudit_history' in dir():\n",
    "    all_histories = {\n",
    "        'fedavg': fedavg_history,\n",
    "        'fairfed': fairfed_history,\n",
    "        'fed_audit_gan': fedaudit_history\n",
    "    }\n",
    "    \n",
    "    create_comparison_plots(all_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad70369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Create Summary Table\n",
    "\n",
    "def create_summary_table(histories: Dict[str, Dict]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create publication-ready summary table.\n",
    "    \"\"\"\n",
    "    methods = list(histories.keys())\n",
    "    labels = {'fedavg': 'FedAvg', 'fairfed': 'FairFed', 'fed_audit_gan': 'Fed-Audit-GAN'}\n",
    "    \n",
    "    # Metrics to include (use last 5 rounds for stability)\n",
    "    metrics = {\n",
    "        'Accuracy â†‘': 'trad/overall',\n",
    "        'Worst-Group Acc â†‘': 'trad/worst_group',\n",
    "        'DP Gap â†“': 'trad/dp_gap',\n",
    "        'EO Gap â†“': 'trad/eo_gap',\n",
    "        'CF Bias â†“': 'cf/cf_prediction_gap',\n",
    "        'Bias Discovery â†‘': 'cf/latent_bias_discovery_rate',\n",
    "        'Boundary Cross Rate': 'cf/boundary_crossing_rate',\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for method in methods:\n",
    "        row = {'Method': labels[method]}\n",
    "        for metric_name, metric_key in metrics.items():\n",
    "            if metric_key in histories[method]:\n",
    "                # Use mean of last 5 rounds\n",
    "                values = histories[method][metric_key][-5:]\n",
    "                row[metric_name] = f\"{np.mean(values):.3f} Â± {np.std(values):.3f}\"\n",
    "            else:\n",
    "                row[metric_name] = '-'\n",
    "        results.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({'results/summary_table': wandb.Table(dataframe=df)})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if 'all_histories' in dir():\n",
    "    summary_df = create_summary_table(all_histories)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee45a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Client Fairness Attribution Analysis\n",
    "\n",
    "def analyze_client_fairness_attribution(trainer, client_partitions: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze which clients contributed positively/negatively to fairness.\n",
    "    \n",
    "    This is THE KEY TABLE that shows Fed-Audit-GAN's contribution attribution.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLIENT FAIRNESS ATTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Compute final fairness contribution for each client\n",
    "    results = []\n",
    "    \n",
    "    for client in trainer.clients:\n",
    "        # Train client update\n",
    "        update, stats = client.train(trainer.global_model, trainer.config, trainer.device)\n",
    "        \n",
    "        # Compute fairness contribution\n",
    "        contrib, before, after = trainer.auditor.compute_client_fairness_contribution(\n",
    "            trainer.global_model, update, trainer.audit_loader\n",
    "        )\n",
    "        \n",
    "        row = {\n",
    "            'Client ID': client.client_id,\n",
    "            'Type': client.client_type,\n",
    "            'Group 0 Ratio': client_partitions[client.client_id]['group_0_ratio'],\n",
    "            'Local Accuracy': stats['accuracy'],\n",
    "            'Fairness Contribution': contrib,\n",
    "            'Bias Before': before['cf_prediction_gap'],\n",
    "            'Bias After': after['cf_prediction_gap'],\n",
    "            'Attribution': 'âœ… Reduces Bias' if contrib > 0 else 'âŒ Increases Bias'\n",
    "        }\n",
    "        results.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values('Fairness Contribution', ascending=False)\n",
    "    \n",
    "    print(df.to_string(index=False))\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({'results/client_attribution': wandb.Table(dataframe=df)})\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart of contributions\n",
    "    colors = ['green' if c > 0 else 'red' for c in df['Fairness Contribution']]\n",
    "    axes[0].bar(range(len(df)), df['Fairness Contribution'], color=colors)\n",
    "    axes[0].set_xlabel('Client (sorted by contribution)')\n",
    "    axes[0].set_ylabel('Fairness Contribution')\n",
    "    axes[0].set_title('Client Fairness Contributions\\n(Positive = Reduces Bias)')\n",
    "    axes[0].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    # Scatter: Group ratio vs Fairness contribution\n",
    "    type_colors = {'biased_majority': 'blue', 'fairness_critical': 'red', 'balanced': 'green'}\n",
    "    for ctype in type_colors:\n",
    "        mask = df['Type'] == ctype\n",
    "        axes[1].scatter(\n",
    "            df[mask]['Group 0 Ratio'], \n",
    "            df[mask]['Fairness Contribution'],\n",
    "            c=type_colors[ctype], \n",
    "            label=ctype,\n",
    "            s=100, alpha=0.7\n",
    "        )\n",
    "    axes[1].set_xlabel('Group 0 Ratio (Majority Data)')\n",
    "    axes[1].set_ylabel('Fairness Contribution')\n",
    "    axes[1].set_title('Client Type vs Fairness Contribution')\n",
    "    axes[1].legend()\n",
    "    axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('client_attribution.png', dpi=150, bbox_inches='tight')\n",
    "    wandb.log({'results/client_attribution_plot': wandb.Image('client_attribution.png')})\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "if 'fedaudit_trainer' in dir():\n",
    "    attribution_df = analyze_client_fairness_attribution(fedaudit_trainer, client_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db468b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Fairness-Accuracy Tradeoff Analysis\n",
    "\n",
    "def plot_fairness_accuracy_tradeoff(histories: Dict[str, Dict]):\n",
    "    \"\"\"\n",
    "    Plot the fairness-accuracy tradeoff curve.\n",
    "    \n",
    "    Key insight: Fed-Audit-GAN should show better Pareto frontier.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = {'fedavg': 'blue', 'fairfed': 'green', 'fed_audit_gan': 'red'}\n",
    "    labels = {'fedavg': 'FedAvg', 'fairfed': 'FairFed', 'fed_audit_gan': 'Fed-Audit-GAN'}\n",
    "    \n",
    "    for method in histories:\n",
    "        acc = histories[method].get('trad/overall', [])\n",
    "        dp = histories[method].get('trad/dp_gap', [])\n",
    "        \n",
    "        if acc and dp:\n",
    "            # Plot trajectory\n",
    "            ax.plot(dp, acc, color=colors[method], alpha=0.3, linewidth=1)\n",
    "            \n",
    "            # Mark start and end\n",
    "            ax.scatter(dp[0], acc[0], color=colors[method], marker='o', s=100, \n",
    "                      label=f'{labels[method]} (start)', edgecolors='black')\n",
    "            ax.scatter(dp[-1], acc[-1], color=colors[method], marker='*', s=200,\n",
    "                      label=f'{labels[method]} (final)', edgecolors='black')\n",
    "    \n",
    "    ax.set_xlabel('Demographic Parity Gap (lower is fairer)', fontsize=12)\n",
    "    ax.set_ylabel('Test Accuracy (higher is better)', fontsize=12)\n",
    "    ax.set_title('Fairness-Accuracy Tradeoff', fontsize=14)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add ideal region annotation\n",
    "    ax.annotate('Ideal: High Accuracy, Low DP Gap',\n",
    "               xy=(0.05, 0.85), xycoords='axes fraction',\n",
    "               fontsize=10, style='italic', color='gray')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fairness_accuracy_tradeoff.png', dpi=150, bbox_inches='tight')\n",
    "    wandb.log({'results/fairness_accuracy_tradeoff': wandb.Image('fairness_accuracy_tradeoff.png')})\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if 'all_histories' in dir():\n",
    "    plot_fairness_accuracy_tradeoff(all_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: Ablation Study - Validation Set Baseline\n",
    "\n",
    "def validation_set_baseline_comparison(trainer, test_loader):\n",
    "    \"\"\"\n",
    "    Compare GAN-based auditing vs simple validation set auditing.\n",
    "    \n",
    "    This directly addresses: \"Why not just use a validation set?\"\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ABLATION: GAN vs Validation Set Auditing\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Validation Set Metrics (what FairFed would use)\n",
    "    val_metrics = FairnessMetrics.compute_all(trainer.global_model, test_loader, device)\n",
    "    \n",
    "    # 2. GAN-based Metrics (Fed-Audit-GAN)\n",
    "    gan_metrics = trainer.auditor.compute_counterfactual_bias(\n",
    "        trainer.global_model, trainer.audit_loader\n",
    "    )\n",
    "    \n",
    "    print(\"\\nðŸ“Š Validation Set Auditing:\")\n",
    "    print(f\"  - Can detect: DP gap, EO gap, accuracy disparity\")\n",
    "    print(f\"  - DP Gap: {val_metrics['dp_gap']:.4f}\")\n",
    "    print(f\"  - EO Gap: {val_metrics['eo_gap']:.4f}\")\n",
    "    print(f\"  - Accuracy Gap: {val_metrics['accuracy_gap']:.4f}\")\n",
    "    \n",
    "    print(\"\\nðŸ”¬ GAN-based Counterfactual Auditing:\")\n",
    "    print(f\"  - Can detect: Individual-level bias, latent discrimination\")\n",
    "    print(f\"  - CF Prediction Gap: {gan_metrics['cf_prediction_gap']:.4f}\")\n",
    "    print(f\"  - Boundary Crossing Rate: {gan_metrics['boundary_crossing_rate']:.4f}\")\n",
    "    print(f\"  - Latent Bias Discovery: {gan_metrics['latent_bias_discovery_rate']:.4f}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ Key Insight:\")\n",
    "    print(\"  Validation sets measure GROUP-level fairness.\")\n",
    "    print(\"  GAN auditing measures INDIVIDUAL-level counterfactual fairness.\")\n",
    "    print(\"  A model can pass DP/EO tests but still discriminate on individuals.\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    x = ['DP Gap', 'EO Gap', 'Acc Gap', 'CF Bias', 'Boundary Cross', 'Bias Discovery']\n",
    "    val_values = [val_metrics['dp_gap'], val_metrics['eo_gap'], \n",
    "                  val_metrics['accuracy_gap'], 0, 0, 0]\n",
    "    gan_values = [0, 0, 0, gan_metrics['cf_prediction_gap'],\n",
    "                  gan_metrics['boundary_crossing_rate'],\n",
    "                  gan_metrics['latent_bias_discovery_rate']]\n",
    "    \n",
    "    x_pos = np.arange(len(x))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x_pos - width/2, val_values, width, label='Validation Set', color='steelblue')\n",
    "    ax.bar(x_pos + width/2, gan_values, width, label='GAN Auditing', color='coral')\n",
    "    \n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('Validation Set vs GAN Auditing Capabilities')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(x, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('ablation_val_vs_gan.png', dpi=150, bbox_inches='tight')\n",
    "    wandb.log({'ablation/val_vs_gan': wandb.Image('ablation_val_vs_gan.png')})\n",
    "    plt.show()\n",
    "    \n",
    "    return val_metrics, gan_metrics\n",
    "\n",
    "\n",
    "if 'fedaudit_trainer' in dir():\n",
    "    val_metrics, gan_metrics = validation_set_baseline_comparison(\n",
    "        fedaudit_trainer, \n",
    "        DataLoader(test_dataset, batch_size=CONFIG['batch_size'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79761b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22: Save Models and Final Logging\n",
    "\n",
    "if 'fedaudit_model' in dir():\n",
    "    # Save models\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    \n",
    "    torch.save(fedaudit_model.state_dict(), 'saved_models/fed_audit_gan_model.pth')\n",
    "    torch.save(fedaudit_generator.state_dict(), 'saved_models/counterfactual_generator.pth')\n",
    "    torch.save(fedaudit_discriminator.state_dict(), 'saved_models/bias_discriminator.pth')\n",
    "    \n",
    "    print(\"Models saved to ./saved_models/\")\n",
    "    \n",
    "    # Log artifacts to wandb\n",
    "    artifact = wandb.Artifact('fed_audit_gan_models', type='model')\n",
    "    artifact.add_dir('saved_models')\n",
    "    wandb.log_artifact(artifact)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT COMPLETE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nWandB Dashboard: {wandb.run.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23: Final Summary and Paper-Ready Output\n",
    "\n",
    "def generate_paper_summary():\n",
    "    \"\"\"\n",
    "    Generate summary for paper submission.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“„ PAPER-READY SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    \n",
    "    KEY CLAIMS SUPPORTED BY EXPERIMENTS:\n",
    "    \n",
    "    1ï¸âƒ£ Fed-Audit-GAN reveals biases that validation sets miss\n",
    "       - Counterfactual probing finds individual-level discrimination\n",
    "       - Boundary crossing rate measures decision instability\n",
    "       - Latent Bias Discovery Rate captures hidden biases\n",
    "    \n",
    "    2ï¸âƒ£ Client fairness contribution attribution works\n",
    "       - Clients with minority data get positive scores\n",
    "       - Biased clients get negative scores\n",
    "       - This enables fairness-aware incentive mechanisms\n",
    "    \n",
    "    3ï¸âƒ£ Fed-Audit-GAN achieves comparable accuracy with better fairness\n",
    "       - Accuracy competitive with FedAvg\n",
    "       - Better worst-group accuracy than baselines\n",
    "       - More stable fairness trajectory\n",
    "    \n",
    "    4ï¸âƒ£ GAN is NOT replaceable by validation set\n",
    "       - Generates targeted counterfactuals\n",
    "       - Adapts to current model's decision boundaries\n",
    "       - Provides different (complementary) fairness signal\n",
    "    \n",
    "    RECOMMENDED PAPER FRAMING:\n",
    "    \n",
    "    âœ… \"Fed-Audit-GAN reveals and mitigates fairness violations that remain \n",
    "        undetected under static evaluation, leading to more stable and \n",
    "        incentive-aligned fairness over time.\"\n",
    "    \n",
    "    âŒ NOT: \"Fed-Audit-GAN achieves the best fairness.\"\n",
    "    \n",
    "    \"\"\")\n",
    "    \n",
    "    # Create final results table in paper format\n",
    "    if 'summary_df' in dir():\n",
    "        print(\"\\nðŸ“Š RESULTS TABLE (copy to paper):\")\n",
    "        print(\"-\" * 80)\n",
    "        print(summary_df.to_markdown(index=False))\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "\n",
    "generate_paper_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Cleanup and Close WandB\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\nâœ… Experiment complete! Check your WandB dashboard for full results.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
