{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f84a358",
   "metadata": {},
   "source": [
    "# üîß Fed-Audit-GAN v2.0 - CIFAR-10 (Multi-Gamma Ablation Study) - TPU Edition\n",
    "\n",
    "## üéØ Experiments Run:\n",
    "- **FedAvg** (Œ≥ = 0.0) - Baseline with data-weighted aggregation (NO GAN)\n",
    "- **Fed-Audit-GAN Œ≥ = 0.3** - Mild fairness weighting\n",
    "- **Fed-Audit-GAN Œ≥ = 0.7** - Strong fairness weighting\n",
    "\n",
    "## üîß 4-Phase Architecture:\n",
    "1. **Phase 1**: Local Client Training (FedProx)\n",
    "2. **Phase 2**: GAN Training (Fairness Generator)\n",
    "3. **Phase 3**: Fairness Scoring (with Momentum/EMA)\n",
    "4. **Phase 4**: Aggregation (with Warm-up)\n",
    "\n",
    "## üìä Fairness Metrics (Per-Client Accuracy Based!):\n",
    "- **JFI**: Jain's Fairness Index\n",
    "- **Max-Min Fairness**: min(acc)/max(acc)\n",
    "- **Accuracy Gap**: max(acc) - min(acc)\n",
    "\n",
    "## üöÄ TPU Optimizations (Kaggle TPU v3-8):\n",
    "- PyTorch XLA for TPU acceleration\n",
    "- 8-core TPU with parallel data loading\n",
    "- Batch size scaled for TPU (128 per core)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c25a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies (TPU Edition)\n",
    "# Install PyTorch XLA for TPU support\n",
    "!pip install -q torch torchvision tqdm matplotlib numpy wandb\n",
    "!pip install -q cloud-tpu-client==0.10 torch==2.0.0 torchvision==0.15.1 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl\n",
    "\n",
    "print(\"‚úÖ Dependencies installed (including PyTorch XLA for TPU)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc412ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"‚úÖ WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044cb9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Imports and TPU Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# üöÄ TPU SETUP (Kaggle TPU v3-8)\n",
    "# ============================================================\n",
    "\n",
    "# Check for TPU availability\n",
    "try:\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.parallel_loader as pl\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    \n",
    "    TPU_AVAILABLE = True\n",
    "    DEVICE = xm.xla_device()\n",
    "    NUM_TPU_CORES = 8  # TPU v3-8 has 8 cores\n",
    "    \n",
    "    print(\"‚úÖ TPU detected! Using PyTorch XLA\")\n",
    "    print(f\"   TPU Device: {DEVICE}\")\n",
    "    print(f\"   TPU Cores: {NUM_TPU_CORES}\")\n",
    "    \n",
    "except ImportError:\n",
    "    TPU_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  PyTorch XLA not available. Checking for GPU...\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = torch.device('cuda')\n",
    "        print(f\"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.enabled = True\n",
    "    else:\n",
    "        DEVICE = torch.device('cpu')\n",
    "        print(\"‚ö†Ô∏è  No accelerator detected. Using CPU (training will be slow).\")\n",
    "\n",
    "# Note: We disable AMP for TPU (XLA handles optimization internally)\n",
    "USE_AMP = not TPU_AVAILABLE and torch.cuda.is_available()\n",
    "\n",
    "print(f\"\\nüìç Device: {DEVICE}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   TPU Mode: {TPU_AVAILABLE}\")\n",
    "print(f\"   AMP Enabled: {USE_AMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feff122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEFINITIONS (CIFAR-10 - Deeper CNN)\n",
    "# ============================================================\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"Deeper CNN for CIFAR-10 classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32->16\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16->8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8->4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 4->2\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    \"\"\"Generator that produces paired samples (x, x') for fairness testing\"\"\"\n",
    "    def __init__(self, latent_dim=128, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4  # 8\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 512 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input).view(-1, 512, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        return x, torch.clamp(x + delta, -1, 1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Conditional Discriminator - outputs logits for BCEWithLogitsLoss\"\"\"\n",
    "    def __init__(self, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 1)  # No Sigmoid\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "\n",
    "print(\"‚úÖ Models defined: CNN, FairnessGenerator, Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d416b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS (TPU/GPU Compatible)\n",
    "# ============================================================\n",
    "\n",
    "def train_gan(G, D, model, loader, epochs=20, device=None, l1=1.0, l2=1.0):\n",
    "    \"\"\"Train the Fairness GAN (TPU/GPU compatible)\"\"\"\n",
    "    if device is None:\n",
    "        device = DEVICE\n",
    "    G, D, model = G.to(device), D.to(device), model.to(device)\n",
    "    model.eval()\n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # AMP only for CUDA (not for TPU - XLA handles optimization)\n",
    "    use_amp_local = USE_AMP and not TPU_AVAILABLE and 'cuda' in str(device)\n",
    "    if use_amp_local:\n",
    "        scaler_G = torch.amp.GradScaler(device='cuda')\n",
    "        scaler_D = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            bs = imgs.size(0)\n",
    "            real = torch.ones(bs, 1, device=device)\n",
    "            fake_t = torch.zeros(bs, 1, device=device)\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            z = torch.randn(bs, G.latent_dim, device=device)\n",
    "            gl = torch.randint(0, G.num_classes, (bs,), device=device)\n",
    "            \n",
    "            # Generator\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            if use_amp_local:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    with torch.no_grad():\n",
    "                        px, pxp = model(x), model(xp)\n",
    "                    t1 = -torch.mean((px - pxp) ** 2)\n",
    "                    t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                    t3 = l2 * (bce(D(x, gl), real) + bce(D(xp, gl), real)) / 2\n",
    "                    g_loss = t1 + t2 + t3\n",
    "                scaler_G.scale(g_loss).backward()\n",
    "                scaler_G.step(opt_G)\n",
    "                scaler_G.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                with torch.no_grad():\n",
    "                    px, pxp = model(x), model(xp)\n",
    "                t1 = -torch.mean((px - pxp) ** 2)\n",
    "                t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                t3 = l2 * (bce(D(x, gl), real) + bce(D(xp, gl), real)) / 2\n",
    "                g_loss = t1 + t2 + t3\n",
    "                g_loss.backward()\n",
    "                opt_G.step()\n",
    "            \n",
    "            # TPU: Mark step for XLA optimization\n",
    "            if TPU_AVAILABLE:\n",
    "                xm.mark_step()\n",
    "            \n",
    "            # Discriminator\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            if use_amp_local:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    d_loss = (bce(D(imgs, labels), real) + bce(D(x.detach(), gl), fake_t) + bce(D(xp.detach(), gl), fake_t)) / 3\n",
    "                scaler_D.scale(d_loss).backward()\n",
    "                scaler_D.step(opt_D)\n",
    "                scaler_D.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                d_loss = (bce(D(imgs, labels), real) + bce(D(x.detach(), gl), fake_t) + bce(D(xp.detach(), gl), fake_t)) / 3\n",
    "                d_loss.backward()\n",
    "                opt_D.step()\n",
    "            \n",
    "            # TPU: Mark step for XLA optimization\n",
    "            if TPU_AVAILABLE:\n",
    "                xm.mark_step()\n",
    "    \n",
    "    return G, D\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bias(model, x, xp, device):\n",
    "    \"\"\"Compute bias as difference in model predictions between x and x'\"\"\"\n",
    "    model.eval()\n",
    "    x, xp = x.to(device), xp.to(device)\n",
    "    \n",
    "    if USE_AMP and not TPU_AVAILABLE:\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    else:\n",
    "        diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    \n",
    "    return diff.item()\n",
    "\n",
    "\n",
    "def partition_data_non_iid_unequal(dataset, n_clients, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Create Non-IID partition with UNEQUAL data sizes per client.\n",
    "    Uses Dirichlet distribution for label and size heterogeneity.\n",
    "    \"\"\"\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    class_indices = {c: np.where(labels == c)[0] for c in range(n_classes)}\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "    \n",
    "    client_indices = [[] for _ in range(n_clients)]\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, n_clients))\n",
    "        proportions = (proportions * len(class_indices[c])).astype(int)\n",
    "        proportions[-1] = len(class_indices[c]) - proportions[:-1].sum()\n",
    "        \n",
    "        start = 0\n",
    "        for client_id, num_samples in enumerate(proportions):\n",
    "            if num_samples > 0:\n",
    "                client_indices[client_id].extend(\n",
    "                    class_indices[c][start:start + num_samples].tolist()\n",
    "                )\n",
    "            start += num_samples\n",
    "    \n",
    "    result = []\n",
    "    for i in range(n_clients):\n",
    "        indices = np.array(client_indices[i])\n",
    "        np.random.shuffle(indices)\n",
    "        result.append(indices)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model accuracy (TPU/GPU compatible)\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for d, t in loader:\n",
    "        d, t = d.to(device), t.to(device)\n",
    "        \n",
    "        if USE_AMP and not TPU_AVAILABLE:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                preds = model(d).argmax(1)\n",
    "        else:\n",
    "            preds = model(d).argmax(1)\n",
    "        \n",
    "        correct += (preds == t).sum().item()\n",
    "        total += len(t)\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_client(model, client_loaders, device):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy on EACH client's data.\n",
    "    This measures how fairly the model performs across clients.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    client_accuracies = []\n",
    "    \n",
    "    for loader in client_loaders:\n",
    "        correct, total = 0, 0\n",
    "        for d, t in loader:\n",
    "            d, t = d.to(device), t.to(device)\n",
    "            \n",
    "            if USE_AMP and not TPU_AVAILABLE:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    preds = model(d).argmax(1)\n",
    "            else:\n",
    "                preds = model(d).argmax(1)\n",
    "            \n",
    "            correct += (preds == t).sum().item()\n",
    "            total += len(t)\n",
    "        \n",
    "        acc = 100 * correct / total if total > 0 else 0\n",
    "        client_accuracies.append(acc)\n",
    "    \n",
    "    return client_accuracies\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FAIRNESS METRICS (Based on Per-Client Performance!)\n",
    "# ============================================================\n",
    "\n",
    "def calculate_jfi(performances):\n",
    "    \"\"\"Jain's Fairness Index: (Œ£p·µ¢)¬≤ / (N √ó Œ£p·µ¢¬≤)\"\"\"\n",
    "    p = np.array(performances)\n",
    "    n = len(p)\n",
    "    if np.sum(p ** 2) == 0:\n",
    "        return 1.0\n",
    "    return (np.sum(p) ** 2) / (n * np.sum(p ** 2))\n",
    "\n",
    "\n",
    "def calculate_max_min_fairness(performances):\n",
    "    \"\"\"Max-Min Fairness: min(acc) / max(acc)\"\"\"\n",
    "    p = np.array(performances)\n",
    "    if np.max(p) == 0:\n",
    "        return 0.0\n",
    "    return np.min(p) / np.max(p)\n",
    "\n",
    "\n",
    "def calculate_variance(performances):\n",
    "    return np.var(performances)\n",
    "\n",
    "\n",
    "def calculate_accuracy_gap(performances):\n",
    "    return np.max(performances) - np.min(performances)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions defined (TPU/GPU compatible)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "# Training Parameters\n",
    "N_ROUNDS = 50           # Total training rounds\n",
    "N_CLIENTS = 20          # Number of federated clients\n",
    "N_GAN_EPOCHS = 15       # GAN training epochs per round\n",
    "N_PROBES = 500          # Number of probe samples\n",
    "LOCAL_EPOCHS = 3        # Local training epochs per client\n",
    "\n",
    "# ‚≠ê MULTI-GAMMA ABLATION STUDY\n",
    "GAMMA_VALUES = [0.0, 0.3, 0.7]  # 0.0 = FedAvg baseline\n",
    "\n",
    "# ‚≠ê Oscillation Fix Parameters\n",
    "MOMENTUM = 0.8          # EMA momentum for fairness scores\n",
    "WARMUP_ROUNDS = 5       # Rounds before activating fairness scoring\n",
    "MU = 0.01               # FedProx proximal term strength\n",
    "\n",
    "# DataLoader Parameters (TPU/GPU Optimized)\n",
    "# TPU: batch size of 128 per core is optimal (128*8 = 1024 for TPU v3-8)\n",
    "# But for federated learning with per-client training, we use smaller batches\n",
    "if TPU_AVAILABLE:\n",
    "    BATCH_SIZE = 128        # Per-client batch\n",
    "    VAL_BATCH_SIZE = 512    # Larger for validation\n",
    "    NUM_WORKERS = 4         # TPU can handle parallel loading\n",
    "    PIN_MEMORY = False      # Not needed for TPU\n",
    "    PREFETCH_FACTOR = 2\n",
    "elif torch.cuda.is_available():\n",
    "    BATCH_SIZE = 128\n",
    "    VAL_BATCH_SIZE = 256\n",
    "    NUM_WORKERS = 4\n",
    "    PIN_MEMORY = True\n",
    "    PREFETCH_FACTOR = 2\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "    VAL_BATCH_SIZE = 128\n",
    "    NUM_WORKERS = 0\n",
    "    PIN_MEMORY = False\n",
    "    PREFETCH_FACTOR = None\n",
    "\n",
    "# Non-IID Parameters\n",
    "DIRICHLET_ALPHA = 0.2  # Lower = more heterogeneous\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîß Fed-Audit-GAN v2.0 - CIFAR-10 ABLATION STUDY (TPU Edition)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"TPU Mode: {TPU_AVAILABLE}\")\n",
    "print(f\"Rounds: {N_ROUNDS}, Clients: {N_CLIENTS}\")\n",
    "print(f\"\\nüéØ GAMMA VALUES TO TEST: {GAMMA_VALUES}\")\n",
    "print(f\"   Œ≥=0.0 ‚Üí FedAvg (NO GAN, data-weighted)\")\n",
    "print(f\"   Œ≥=0.3 ‚Üí Mild fairness weighting\")\n",
    "print(f\"   Œ≥=0.7 ‚Üí Strong fairness weighting\")\n",
    "print(f\"\\n‚≠ê OSCILLATION FIX PARAMETERS:\")\n",
    "print(f\"   Momentum (Œ≤): {MOMENTUM}\")\n",
    "print(f\"   Warm-up Rounds: {WARMUP_ROUNDS}\")\n",
    "print(f\"   FedProx (Œº): {MU}\")\n",
    "print(f\"\\nüì¶ BATCH SIZES:\")\n",
    "print(f\"   Training: {BATCH_SIZE}\")\n",
    "print(f\"   Validation: {VAL_BATCH_SIZE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02429db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING (CIFAR-10 with TPU/GPU Optimizations)\n",
    "# ============================================================\n",
    "\n",
    "# CIFAR-10 transforms with augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create Non-IID partitions with UNEQUAL sizes\n",
    "np.random.seed(42)\n",
    "client_idx = partition_data_non_iid_unequal(train_data, N_CLIENTS, alpha=DIRICHLET_ALPHA)\n",
    "\n",
    "# Calculate data weights for each client\n",
    "client_data_sizes = [len(idx) for idx in client_idx]\n",
    "total_samples = sum(client_data_sizes)\n",
    "CLIENT_DATA_WEIGHTS = [size / total_samples for size in client_data_sizes]\n",
    "\n",
    "# DataLoader kwargs (TPU compatible)\n",
    "dataloader_kwargs = {\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY,\n",
    "}\n",
    "# persistent_workers and prefetch_factor only for GPU/CPU\n",
    "if not TPU_AVAILABLE and NUM_WORKERS > 0:\n",
    "    dataloader_kwargs['persistent_workers'] = True\n",
    "    if PREFETCH_FACTOR:\n",
    "        dataloader_kwargs['prefetch_factor'] = PREFETCH_FACTOR\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=VAL_BATCH_SIZE, shuffle=False, **dataloader_kwargs)\n",
    "val_loader = DataLoader(\n",
    "    Subset(train_data, np.random.choice(len(train_data), 2000, replace=False)),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **dataloader_kwargs\n",
    ")\n",
    "\n",
    "# Client data loaders\n",
    "client_loaders = [\n",
    "    DataLoader(Subset(train_data, client_idx[c]), batch_size=BATCH_SIZE, shuffle=True, **dataloader_kwargs)\n",
    "    for c in range(N_CLIENTS)\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Data loaded!\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"\\nüìä NON-IID DATA DISTRIBUTION (Dirichlet Œ±={DIRICHLET_ALPHA}):\")\n",
    "print(f\"   Samples per client: {client_data_sizes}\")\n",
    "print(f\"   Min: {min(client_data_sizes)}, Max: {max(client_data_sizes)}, Ratio: {max(client_data_sizes)/max(1, min(client_data_sizes)):.1f}x\")\n",
    "print(f\"\\n‚öñÔ∏è CLIENT DATA WEIGHTS (for FedAvg):\")\n",
    "for i in range(min(5, N_CLIENTS)):  # Show first 5\n",
    "    print(f\"   Client {i}: {client_data_sizes[i]:5d} samples ‚Üí weight = {CLIENT_DATA_WEIGHTS[i]:.4f}\")\n",
    "print(f\"   ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08abc424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üöÄ MULTI-GAMMA ABLATION STUDY (4-PHASE ARCHITECTURE)\n",
    "# FedAvg (Œ≥=0) + Fed-Audit-GAN with Œ≥=0.3, 0.7\n",
    "# TPU/GPU Compatible\n",
    "# ============================================================\n",
    "\n",
    "def run_fed_audit_gan(gamma, n_rounds, n_clients, warmup_rounds, momentum, mu,\n",
    "                      train_data, client_idx, val_loader, test_loader, client_loaders,\n",
    "                      n_gan_epochs, n_probes, local_epochs, device, use_amp,\n",
    "                      client_data_weights, tpu_available):\n",
    "    \"\"\"\n",
    "    Run Fed-Audit-GAN v2.0 with specified gamma value.\n",
    "    gamma=0 is FedAvg (data-weighted aggregation) - NO GAN training.\n",
    "    TPU/GPU compatible.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize model\n",
    "    model = CNN().to(device)\n",
    "    \n",
    "    # AMP scaler only for CUDA (not TPU)\n",
    "    scaler = None\n",
    "    if use_amp and not tpu_available:\n",
    "        scaler = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    # Fairness score history for momentum\n",
    "    fairness_history = {i: 0.0 for i in range(n_clients)}\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'acc': [], 'bias': [], 'alphas': [],\n",
    "        'raw_scores': [], 'smoothed_scores': [],\n",
    "        'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': []\n",
    "    }\n",
    "    \n",
    "    is_fedavg = (gamma == 0)\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=f\"{'FedAvg' if is_fedavg else f'Œ≥={gamma}'}\"):\n",
    "        # ================================================================\n",
    "        # PHASE 1: Local Client Training (with FedProx)\n",
    "        # ================================================================\n",
    "        updates = []\n",
    "        global_params = [p.clone().detach() for p in model.parameters()]\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            before_state = copy.deepcopy(model.state_dict())\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device)\n",
    "                    target = target.to(device)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp and not tpu_available:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            ce_loss = F.cross_entropy(output, target)\n",
    "                            prox_loss = sum(((lp - gp) ** 2).sum() \n",
    "                                          for lp, gp in zip(local_model.parameters(), global_params))\n",
    "                            loss = ce_loss + (mu / 2) * prox_loss\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        ce_loss = F.cross_entropy(output, target)\n",
    "                        prox_loss = sum(((lp - gp) ** 2).sum() \n",
    "                                      for lp, gp in zip(local_model.parameters(), global_params))\n",
    "                        loss = ce_loss + (mu / 2) * prox_loss\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # TPU: Mark step for XLA optimization\n",
    "                    if tpu_available:\n",
    "                        xm.mark_step()\n",
    "            \n",
    "            update = {k: local_model.state_dict()[k] - before_state[k] for k in before_state}\n",
    "            updates.append(update)\n",
    "            del local_model\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 2 & 3: GAN Training + Fairness Scoring (SKIP for FedAvg!)\n",
    "        # ================================================================\n",
    "        B_base = 0.0\n",
    "        S_fair_raw = [0.0] * n_clients\n",
    "        S_fair_smoothed = [0.0] * n_clients\n",
    "        \n",
    "        if not is_fedavg:\n",
    "            G = FairnessGenerator(img_shape=(3, 32, 32)).to(device)\n",
    "            D = Discriminator(img_shape=(3, 32, 32)).to(device)\n",
    "            G, D = train_gan(G, D, model, val_loader, epochs=n_gan_epochs, device=device)\n",
    "            \n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(n_probes, G.latent_dim, device=device)\n",
    "                labels = torch.randint(0, 10, (n_probes,), device=device)\n",
    "                \n",
    "                if use_amp and not tpu_available:\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        x_probe, xp_probe = G(z, labels)\n",
    "                else:\n",
    "                    x_probe, xp_probe = G(z, labels)\n",
    "            \n",
    "            B_base = compute_bias(model, x_probe, xp_probe, device)\n",
    "            \n",
    "            S_fair_raw = []\n",
    "            S_fair_smoothed = []\n",
    "            \n",
    "            for cid, upd in enumerate(updates):\n",
    "                hyp_model = copy.deepcopy(model)\n",
    "                hyp_state = hyp_model.state_dict()\n",
    "                for k in hyp_state:\n",
    "                    hyp_state[k] = hyp_state[k] + upd[k]\n",
    "                hyp_model.load_state_dict(hyp_state)\n",
    "                \n",
    "                B_client = compute_bias(hyp_model, x_probe, xp_probe, device)\n",
    "                S_current = B_base - B_client\n",
    "                S_fair_raw.append(S_current)\n",
    "                \n",
    "                S_prev = fairness_history[cid]\n",
    "                S_smoothed = (momentum * S_prev) + ((1 - momentum) * S_current)\n",
    "                fairness_history[cid] = S_smoothed\n",
    "                S_fair_smoothed.append(S_smoothed)\n",
    "                del hyp_model\n",
    "            \n",
    "            del G, D, x_probe, xp_probe\n",
    "        \n",
    "        history['raw_scores'].append(S_fair_raw.copy() if not is_fedavg else [0.0] * n_clients)\n",
    "        history['smoothed_scores'].append(S_fair_smoothed.copy() if not is_fedavg else [0.0] * n_clients)\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 4: Aggregation\n",
    "        # ================================================================\n",
    "        if is_fedavg:\n",
    "            alphas = client_data_weights.copy()\n",
    "        elif rnd < warmup_rounds:\n",
    "            alphas = client_data_weights.copy()\n",
    "        else:\n",
    "            alphas = F.softmax(torch.tensor(S_fair_smoothed) * gamma, dim=0).tolist()\n",
    "        \n",
    "        new_state = model.state_dict()\n",
    "        for k in new_state:\n",
    "            new_state[k] = new_state[k] + sum(a * u[k] for a, u in zip(alphas, updates))\n",
    "        model.load_state_dict(new_state)\n",
    "        \n",
    "        # TPU: Mark step after aggregation\n",
    "        if tpu_available:\n",
    "            xm.mark_step()\n",
    "        \n",
    "        # ================================================================\n",
    "        # EVALUATION - Per-Client Fairness\n",
    "        # ================================================================\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['bias'].append(B_base)\n",
    "        history['alphas'].append(alphas.copy())\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'bias': B_base,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN ALL EXPERIMENTS\n",
    "# ============================================================\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for gamma in GAMMA_VALUES:\n",
    "    method_name = \"FedAvg\" if gamma == 0 else f\"FedAuditGAN_Œ≥={gamma}\"\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üöÄ RUNNING: {method_name}\")\n",
    "    if gamma == 0:\n",
    "        print(f\"   (Pure FedAvg - NO GAN, data-weighted aggregation)\")\n",
    "    else:\n",
    "        print(f\"   (Fed-Audit-GAN with fairness-aware aggregation)\")\n",
    "    print(f\"   Device: {DEVICE} | TPU Mode: {TPU_AVAILABLE}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"FED_AUDIT_GAN_TEST_1_CIFAR10\",\n",
    "        name=f\"{method_name}_CIFAR10_clients{N_CLIENTS}_TPU\" if TPU_AVAILABLE else f\"{method_name}_CIFAR10_clients{N_CLIENTS}\",\n",
    "        config={\n",
    "            \"method\": method_name,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"n_rounds\": N_ROUNDS,\n",
    "            \"n_clients\": N_CLIENTS,\n",
    "            \"gamma\": gamma,\n",
    "            \"momentum\": MOMENTUM,\n",
    "            \"warmup_rounds\": WARMUP_ROUNDS,\n",
    "            \"mu_fedprox\": MU,\n",
    "            \"dirichlet_alpha\": DIRICHLET_ALPHA,\n",
    "            \"device\": str(DEVICE),\n",
    "            \"tpu_enabled\": TPU_AVAILABLE\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model, history = run_fed_audit_gan(\n",
    "        gamma=gamma,\n",
    "        n_rounds=N_ROUNDS,\n",
    "        n_clients=N_CLIENTS,\n",
    "        warmup_rounds=WARMUP_ROUNDS,\n",
    "        momentum=MOMENTUM,\n",
    "        mu=MU,\n",
    "        train_data=train_data,\n",
    "        client_idx=client_idx,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        client_loaders=client_loaders,\n",
    "        n_gan_epochs=N_GAN_EPOCHS,\n",
    "        n_probes=N_PROBES,\n",
    "        local_epochs=LOCAL_EPOCHS,\n",
    "        device=DEVICE,\n",
    "        use_amp=USE_AMP,\n",
    "        client_data_weights=CLIENT_DATA_WEIGHTS,\n",
    "        tpu_available=TPU_AVAILABLE\n",
    "    )\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    all_results[gamma] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'name': method_name\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ {method_name} Complete!\")\n",
    "    print(f\"   Final Accuracy: {history['acc'][-1]:.2f}%\")\n",
    "    print(f\"   Final JFI (per-client): {history['jfi'][-1]:.4f}\")\n",
    "    print(f\"   Accuracy Gap: {history['accuracy_gap'][-1]:.2f}%\")\n",
    "    if gamma > 0:\n",
    "        print(f\"   Final Bias: {history['bias'][-1]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä Check your WandB dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9253e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä RESULTS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"üìä CIFAR-10 MULTI-GAMMA ABLATION STUDY RESULTS\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "print(f\"\\n{'METHOD':<25} {'GLOBAL ACC':<12} {'JFI':<10} {'MAX-MIN':<10} {'GAP':<10} {'MIN ACC':<10} {'MAX ACC':<10}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "best_acc = max(all_results[g]['history']['acc'][-1] for g in GAMMA_VALUES)\n",
    "best_jfi = max(all_results[g]['history']['jfi'][-1] for g in GAMMA_VALUES)\n",
    "lowest_gap = min(all_results[g]['history']['accuracy_gap'][-1] for g in GAMMA_VALUES)\n",
    "\n",
    "for gamma in GAMMA_VALUES:\n",
    "    name = all_results[gamma]['name']\n",
    "    acc = all_results[gamma]['history']['acc'][-1]\n",
    "    jfi = all_results[gamma]['history']['jfi'][-1]\n",
    "    max_min = all_results[gamma]['history']['max_min_fairness'][-1]\n",
    "    gap = all_results[gamma]['history']['accuracy_gap'][-1]\n",
    "    min_acc = all_results[gamma]['history']['min_client_acc'][-1]\n",
    "    max_acc = all_results[gamma]['history']['max_client_acc'][-1]\n",
    "    \n",
    "    acc_mark = \"üèÜ\" if acc == best_acc else \"\"\n",
    "    jfi_mark = \"‚≠ê\" if jfi == best_jfi else \"\"\n",
    "    gap_mark = \"‚úÖ\" if gap == lowest_gap else \"\"\n",
    "    \n",
    "    print(f\"{name:<25} {acc:>8.2f}% {acc_mark:<2} {jfi:>8.4f} {jfi_mark:<2} {max_min:>8.4f}   {gap:>6.2f}% {gap_mark:<2} {min_acc:>8.2f}%  {max_acc:>8.2f}%\")\n",
    "\n",
    "print(\"=\" * 110)\n",
    "\n",
    "# Improvement over FedAvg\n",
    "fedavg_acc = all_results[0.0]['history']['acc'][-1]\n",
    "fedavg_jfi = all_results[0.0]['history']['jfi'][-1]\n",
    "fedavg_gap = all_results[0.0]['history']['accuracy_gap'][-1]\n",
    "\n",
    "print(f\"\\nüìà IMPROVEMENT OVER FedAvg:\")\n",
    "for gamma in GAMMA_VALUES:\n",
    "    if gamma == 0:\n",
    "        continue\n",
    "    name = all_results[gamma]['name']\n",
    "    acc = all_results[gamma]['history']['acc'][-1]\n",
    "    jfi = all_results[gamma]['history']['jfi'][-1]\n",
    "    gap = all_results[gamma]['history']['accuracy_gap'][-1]\n",
    "    \n",
    "    acc_diff = acc - fedavg_acc\n",
    "    jfi_diff = jfi - fedavg_jfi\n",
    "    gap_reduction = fedavg_gap - gap\n",
    "    \n",
    "    print(f\"   {name}:\")\n",
    "    print(f\"      Accuracy: {'+' if acc_diff >= 0 else ''}{acc_diff:.2f}%\")\n",
    "    print(f\"      JFI: {'+' if jfi_diff >= 0 else ''}{jfi_diff:.4f}\")\n",
    "    print(f\"      Gap Reduction: {gap_reduction:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a17c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üìä COMPREHENSIVE VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "colors = {\n",
    "    0.0: '#e74c3c',   # Red - FedAvg\n",
    "    0.3: '#3498db',   # Blue\n",
    "    0.7: '#2ecc71',   # Green\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "rounds = range(1, N_ROUNDS + 1)\n",
    "\n",
    "# Plot 1: Global Accuracy\n",
    "ax = axes[0, 0]\n",
    "for gamma in GAMMA_VALUES:\n",
    "    name = all_results[gamma]['name']\n",
    "    acc = all_results[gamma]['history']['acc']\n",
    "    linestyle = '--' if gamma == 0 else '-'\n",
    "    ax.plot(rounds, acc, color=colors[gamma], linestyle=linestyle, \n",
    "            linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='gray', label='Warm-up')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('CIFAR-10: Global Test Accuracy', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: JFI\n",
    "ax = axes[0, 1]\n",
    "for gamma in GAMMA_VALUES:\n",
    "    name = all_results[gamma]['name']\n",
    "    jfi = all_results[gamma]['history']['jfi']\n",
    "    linestyle = '--' if gamma == 0 else '-'\n",
    "    ax.plot(rounds, jfi, color=colors[gamma], linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='gray')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('JFI', fontsize=12)\n",
    "ax.set_title(\"Jain's Fairness Index (Higher = Fairer)\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy Gap\n",
    "ax = axes[0, 2]\n",
    "for gamma in GAMMA_VALUES:\n",
    "    name = all_results[gamma]['name']\n",
    "    gap = all_results[gamma]['history']['accuracy_gap']\n",
    "    linestyle = '--' if gamma == 0 else '-'\n",
    "    ax.plot(rounds, gap, color=colors[gamma], linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='gray')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy Gap (%)', fontsize=12)\n",
    "ax.set_title('Best-Worst Client Gap (Lower = Fairer)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Variance\n",
    "ax = axes[1, 0]\n",
    "for gamma in GAMMA_VALUES:\n",
    "    name = all_results[gamma]['name']\n",
    "    var = all_results[gamma]['history']['variance']\n",
    "    linestyle = '--' if gamma == 0 else '-'\n",
    "    ax.plot(rounds, var, color=colors[gamma], linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='gray')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Variance', fontsize=12)\n",
    "ax.set_title('Per-Client Accuracy Variance (Lower = Fairer)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Min-Max Range\n",
    "ax = axes[1, 1]\n",
    "for gamma in GAMMA_VALUES:\n",
    "    name = all_results[gamma]['name']\n",
    "    min_acc = all_results[gamma]['history']['min_client_acc']\n",
    "    max_acc = all_results[gamma]['history']['max_client_acc']\n",
    "    linestyle = '--' if gamma == 0 else '-'\n",
    "    ax.fill_between(rounds, min_acc, max_acc, color=colors[gamma], alpha=0.2)\n",
    "    ax.plot(rounds, min_acc, color=colors[gamma], linestyle=linestyle, linewidth=1.5)\n",
    "    ax.plot(rounds, max_acc, color=colors[gamma], linestyle=linestyle, linewidth=1.5, label=name)\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Min-Max Client Accuracy Range', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final Per-Client Accuracy\n",
    "ax = axes[1, 2]\n",
    "x = np.arange(N_CLIENTS)\n",
    "width = 0.25\n",
    "for i, gamma in enumerate(GAMMA_VALUES):\n",
    "    name = all_results[gamma]['name']\n",
    "    client_accs = all_results[gamma]['history']['client_accuracies'][-1]\n",
    "    ax.bar(x + i*width, client_accs, width, label=name, color=colors[gamma], alpha=0.8)\n",
    "ax.set_xlabel('Client ID', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Per-Client Accuracy (Final Round)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cifar10_fed_audit_gan_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìÅ Results saved to: cifar10_fed_audit_gan_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ba0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE ALL MODELS AND RESULTS (TPU Compatible)\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs('results_cifar10_v2', exist_ok=True)\n",
    "\n",
    "for gamma in GAMMA_VALUES:\n",
    "    name = all_results[gamma]['name']\n",
    "    filename = f\"results_cifar10_v2/{name.replace('=', '').replace('.', '_')}_CIFAR10.pth\"\n",
    "    \n",
    "    # Move model to CPU before saving (required for TPU)\n",
    "    model_state = all_results[gamma]['model'].cpu().state_dict()\n",
    "    \n",
    "    # For TPU, use xm.save for proper serialization\n",
    "    if TPU_AVAILABLE:\n",
    "        import torch_xla.utils.serialization as xser\n",
    "        xser.save({\n",
    "            'model_state_dict': model_state,\n",
    "            'history': all_results[gamma]['history'],\n",
    "            'config': {\n",
    "                'n_rounds': N_ROUNDS,\n",
    "                'n_clients': N_CLIENTS,\n",
    "                'gamma': gamma,\n",
    "                'momentum': MOMENTUM,\n",
    "                'warmup_rounds': WARMUP_ROUNDS,\n",
    "                'mu': MU,\n",
    "                'tpu_trained': True\n",
    "            }\n",
    "        }, filename)\n",
    "    else:\n",
    "        torch.save({\n",
    "            'model_state_dict': model_state,\n",
    "            'history': all_results[gamma]['history'],\n",
    "            'config': {\n",
    "                'n_rounds': N_ROUNDS,\n",
    "                'n_clients': N_CLIENTS,\n",
    "                'gamma': gamma,\n",
    "                'momentum': MOMENTUM,\n",
    "                'warmup_rounds': WARMUP_ROUNDS,\n",
    "                'mu': MU,\n",
    "                'tpu_trained': False\n",
    "            }\n",
    "        }, filename)\n",
    "    print(f\"‚úÖ Saved: {filename}\")\n",
    "\n",
    "with open('results_cifar10_v2/all_results_summary.pkl', 'wb') as f:\n",
    "    summary = {\n",
    "        gamma: {\n",
    "            'name': all_results[gamma]['name'],\n",
    "            'history': all_results[gamma]['history'],\n",
    "            'final_acc': all_results[gamma]['history']['acc'][-1],\n",
    "            'final_jfi': all_results[gamma]['history']['jfi'][-1]\n",
    "        }\n",
    "        for gamma in GAMMA_VALUES\n",
    "    }\n",
    "    pickle.dump(summary, f)\n",
    "print(\"‚úÖ Saved: results_cifar10_v2/all_results_summary.pkl\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for gamma in GAMMA_VALUES:\n",
    "    name = all_results[gamma]['name']\n",
    "    acc = all_results[gamma]['history']['acc'][-1]\n",
    "    jfi = all_results[gamma]['history']['jfi'][-1]\n",
    "    gap = all_results[gamma]['history']['accuracy_gap'][-1]\n",
    "    print(f\"   {name}: {acc:.2f}% accuracy, JFI={jfi:.4f}, Gap={gap:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Check WandB dashboard: https://wandb.ai\")\n",
    "print(f\"   TPU Training: {TPU_AVAILABLE}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
