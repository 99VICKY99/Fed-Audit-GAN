{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install packages\n",
    "!pip install -q torch torchvision tqdm matplotlib numpy wandb\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fb275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffcf00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Clone repo (REPLACE YOUR_USERNAME)\n",
    "!git clone https://github.com/99VICKY99/Fed-Audit-GAN.git\n",
    "%cd Fed-Audit-GAN\n",
    "!git checkout strict-4-phase\n",
    "!git branch --show-current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ac9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Complete Training Script with WandB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, num_classes=10, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 128 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128), nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, img_shape[0], 3, 1, 1), nn.Tanh())\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256), nn.ReLU(),\n",
    "            nn.Linear(256, int(np.prod(img_shape))), nn.Tanh())\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input).view(-1, 128, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        return x, torch.clamp(x + delta, -1, 1)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=10, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 16, 3, 2, 1), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(16, 32, 3, 2, 1), nn.BatchNorm2d(32), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1), nn.BatchNorm2d(64), nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.LeakyReLU(0.2))\n",
    "        self.fc = nn.Sequential(nn.Linear(128 * 4, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "def train_gan(G, D, model, loader, epochs=30, device='cuda', l1=1.0, l2=1.0):\n",
    "    G, D, model = G.to(device), D.to(device), model.to(device)\n",
    "    model.eval()\n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCELoss()\n",
    "    for _ in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            bs = imgs.size(0)\n",
    "            real, fake_t = torch.ones(bs, 1, device=device), torch.zeros(bs, 1, device=device)\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            z = torch.randn(bs, G.latent_dim, device=device)\n",
    "            gl = torch.randint(0, G.num_classes, (bs,), device=device)\n",
    "            x, xp = G(z, gl)\n",
    "            with torch.no_grad():\n",
    "                px, pxp = model(x), model(xp)\n",
    "            t1 = -torch.mean((px - pxp) ** 2)\n",
    "            t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "            t3 = l2 * (bce(D(x, gl), real) + bce(D(xp, gl), real)) / 2\n",
    "            opt_G.zero_grad(); (t1 + t2 + t3).backward(); opt_G.step()\n",
    "            x, xp = G(z, gl)\n",
    "            d_loss = (bce(D(imgs, labels), real) + bce(D(x.detach(), gl), fake_t) + bce(D(xp.detach(), gl), fake_t)) / 3\n",
    "            opt_D.zero_grad(); d_loss.backward(); opt_D.step()\n",
    "    return G, D\n",
    "\n",
    "def compute_bias(model, x, xp, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return torch.abs(model(x.to(device)) - model(xp.to(device))).sum(1).mean().item()\n",
    "\n",
    "def partition_data(dataset, n):\n",
    "    idx = np.argsort([dataset[i][1] for i in range(len(dataset))])\n",
    "    shards = np.array_split(idx, n * 2)\n",
    "    np.random.shuffle(shards)\n",
    "    return [np.concatenate([shards[2*i], shards[2*i+1]]) for i in range(n)]\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = sum((model(d.to(device)).argmax(1) == t.to(device)).sum().item() for d, t in loader)\n",
    "    return 100 * correct / sum(len(t) for _, t in loader)\n",
    "\n",
    "# Config\n",
    "N_ROUNDS = 10\n",
    "N_CLIENTS = 5\n",
    "GAMMA = 2.0\n",
    "N_GAN_EPOCHS = 20\n",
    "N_PROBES = 300\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Init WandB\n",
    "wandb.init(project=\"fed-audit-gan\", name=f\"colab_gamma{GAMMA}_clients{N_CLIENTS}\", config={\n",
    "    \"n_rounds\": N_ROUNDS, \"n_clients\": N_CLIENTS, \"gamma\": GAMMA, \"device\": DEVICE})\n",
    "\n",
    "# Data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "client_idx = partition_data(train_data, N_CLIENTS)\n",
    "test_loader = DataLoader(test_data, batch_size=64)\n",
    "val_loader = DataLoader(Subset(train_data, np.random.choice(len(train_data), 1000)), batch_size=32)\n",
    "\n",
    "# Model\n",
    "global_model = CNN().to(DEVICE)\n",
    "history = {'acc': [], 'bias': []}\n",
    "\n",
    "print(f\"Training: {N_ROUNDS} rounds, {N_CLIENTS} clients, gamma={GAMMA}\")\n",
    "for r in range(N_ROUNDS):\n",
    "    print(f\"\\n=== Round {r+1}/{N_ROUNDS} ===\")\n",
    "    \n",
    "    # Phase 1: Client training\n",
    "    updates = []\n",
    "    for c in tqdm(range(N_CLIENTS), desc=\"Phase 1\"):\n",
    "        loader = DataLoader(Subset(train_data, client_idx[c]), batch_size=32, shuffle=True)\n",
    "        local = copy.deepcopy(global_model)\n",
    "        before = copy.deepcopy(global_model.state_dict())\n",
    "        opt = optim.SGD(local.parameters(), lr=0.01)\n",
    "        local.train()\n",
    "        for _ in range(3):\n",
    "            for d, t in loader:\n",
    "                opt.zero_grad()\n",
    "                F.cross_entropy(local(d.to(DEVICE)), t.to(DEVICE)).backward()\n",
    "                opt.step()\n",
    "        updates.append({k: local.state_dict()[k] - before[k] for k in before})\n",
    "    \n",
    "    # Phase 2: GAN\n",
    "    print(\"Phase 2: GAN training\")\n",
    "    G = FairnessGenerator()\n",
    "    D = Discriminator()\n",
    "    G, D = train_gan(G, D, global_model, val_loader, epochs=N_GAN_EPOCHS, device=DEVICE)\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(N_PROBES, G.latent_dim, device=DEVICE)\n",
    "        lbl = torch.randint(0, 10, (N_PROBES,), device=DEVICE)\n",
    "        x, xp = G(z, lbl)\n",
    "    \n",
    "    # Phase 3: Scoring\n",
    "    B_base = compute_bias(global_model, x, xp, DEVICE)\n",
    "    print(f\"Phase 3: B_base={B_base:.4f}\")\n",
    "    S = []\n",
    "    for i, upd in enumerate(updates):\n",
    "        hyp = copy.deepcopy(global_model)\n",
    "        sd = hyp.state_dict()\n",
    "        for k in sd: sd[k] = sd[k] + upd[k]\n",
    "        hyp.load_state_dict(sd)\n",
    "        B_i = compute_bias(hyp, x, xp, DEVICE)\n",
    "        S.append(B_base - B_i)\n",
    "        print(f\"  Client {i}: S={S[-1]:+.4f}\")\n",
    "    \n",
    "    # Phase 4: Aggregation\n",
    "    alpha = F.softmax(torch.tensor(S) * GAMMA, dim=0).tolist()\n",
    "    print(f\"Phase 4: alpha={[f'{a:.3f}' for a in alpha]}\")\n",
    "    sd = global_model.state_dict()\n",
    "    for k in sd:\n",
    "        sd[k] = sd[k] + sum(a * u[k] for a, u in zip(alpha, updates))\n",
    "    global_model.load_state_dict(sd)\n",
    "    \n",
    "    acc = evaluate(global_model, test_loader, DEVICE)\n",
    "    history['acc'].append(acc)\n",
    "    history['bias'].append(B_base)\n",
    "    wandb.log({'round': r+1, 'accuracy': acc, 'bias': B_base, 'avg_S': np.mean(S)})\n",
    "    print(f\"Accuracy: {acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nFinal: {history['acc'][-1]:.2f}%\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33364b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Plot Results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax1.plot(history['acc'], 'b-o')\n",
    "ax1.set_xlabel('Round'); ax1.set_ylabel('Accuracy (%)'); ax1.set_title('Test Accuracy'); ax1.grid(True)\n",
    "ax2.plot(history['bias'], 'r-s')\n",
    "ax2.set_xlabel('Round'); ax2.set_ylabel('Bias'); ax2.set_title('Baseline Bias'); ax2.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Check your WandB dashboard: https://wandb.ai\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
