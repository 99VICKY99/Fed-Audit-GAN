{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed1ac61",
   "metadata": {},
   "source": [
    "# ðŸ”§ Fed-Audit-GAN v3.0 - CIFAR-10 (Stability Fixes) - Kaggle GPU Edition\n",
    "\n",
    "## ðŸŽ¯ Experiments Run (SEPARATE IMPLEMENTATIONS):\n",
    "\n",
    "### ðŸ”µ Baseline: FedAvg (Federated Averaging)\n",
    "- **Completely separate algorithm** (NOT gamma=0!)\n",
    "- Standard data-weighted averaging (McMahan et al., 2017)\n",
    "- NO GAN, NO fairness scoring, NO FedProx\n",
    "\n",
    "### ðŸŸ¢ Our Method: Fed-Audit-GAN (with Stability Fixes!)\n",
    "- **Fed-Audit-GAN Î³ = 0.5** - Balanced fairness weighting (recommended)\n",
    "\n",
    "## ðŸ”§ Fed-Audit-GAN 4-Phase Architecture:\n",
    "1. **Phase 1**: Local Client Training (with FedProx + Gradient Clipping)\n",
    "2. **Phase 2**: GAN Training (with Soft Labels for stability)\n",
    "3. **Phase 3**: Fairness Scoring (with Momentum/EMA)\n",
    "4. **Phase 4**: Fairness-Aware Aggregation (with Reduced Audit Frequency)\n",
    "\n",
    "## â­ 4 STABILITY FIXES TO ELIMINATE ZIG-ZAG OSCILLATION:\n",
    "\n",
    "### Fix 1: Extended Warm-up (10 rounds instead of 5)\n",
    "- Let model learn basics before fairness auditing kicks in\n",
    "- Prevents punishing random model in early rounds\n",
    "\n",
    "### Fix 2: Gradient Clipping (max_norm=1.0)\n",
    "- Caps maximum gradient force to prevent \"shocks\"\n",
    "- Stops GAN from pushing model too far in one step\n",
    "\n",
    "### Fix 3: Reduced Audit Frequency (every 2nd round)\n",
    "- Audit only on even rounds after warm-up\n",
    "- Gives model \"breathing room\" between fairness checks\n",
    "\n",
    "### Fix 4: Soft Labels for GAN (Label Smoothing)\n",
    "- Real: 0.9-1.0 instead of 1.0\n",
    "- Fake: 0.0-0.1 instead of 0.0\n",
    "- Prevents Discriminator from becoming too confident (mode collapse)\n",
    "\n",
    "## â­ PATHOLOGICAL NON-IID (2 Classes + Unequal Sizes):\n",
    "- Each client ONLY has 2 out of 10 classes\n",
    "- Clients have DIFFERENT sample sizes (Dirichlet distribution)\n",
    "- Creates EXTREME fairness challenges\n",
    "\n",
    "## ðŸ“Š Expected Results:\n",
    "- âœ… Smooth accuracy curve (no zig-zag)\n",
    "- âœ… Steady JFI improvement\n",
    "- âœ… Higher final accuracy due to stable convergence\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92534ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies (Kaggle GPU Edition)\n",
    "!pip install -q torch torchvision tqdm matplotlib numpy wandb\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"âœ… WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Imports and GPU Setup (Kaggle T4 x2)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# ðŸš€ GPU SETUP (Kaggle T4 x2)\n",
    "# ============================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    \n",
    "    # Enable cuDNN optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # Enable TF32 for faster matrix operations on Ampere+ GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"âœ… GPU(s) detected: {NUM_GPUS}\")\n",
    "    for i in range(NUM_GPUS):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"   GPU {i}: {props.name}\")\n",
    "        print(f\"      Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "        print(f\"      Compute Capability: {props.major}.{props.minor}\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    NUM_GPUS = 0\n",
    "    print(\"âš ï¸  No GPU detected. Using CPU (training will be slow).\")\n",
    "\n",
    "# Mixed Precision Training\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "if USE_AMP:\n",
    "    print(\"\\nâœ… Mixed Precision Training (AMP) enabled.\")\n",
    "\n",
    "print(f\"\\nðŸ“ Device: {DEVICE}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")\n",
    "print(f\"   GPUs available: {NUM_GPUS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162dab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEFINITIONS (CIFAR-10 - Deeper CNN)\n",
    "# ============================================================\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"Deeper CNN for CIFAR-10 classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32->16\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16->8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8->4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 4->2\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    \"\"\"Generator that produces paired samples (x, x') for fairness testing\"\"\"\n",
    "    def __init__(self, latent_dim=128, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4  # 8\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 512 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input).view(-1, 512, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        return x, torch.clamp(x + delta, -1, 1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Conditional Discriminator - outputs logits for BCEWithLogitsLoss\"\"\"\n",
    "    def __init__(self, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 1)  # No Sigmoid\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "\n",
    "print(\"âœ… Models defined: CNN, FairnessGenerator, Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS (GPU Optimized with AMP)\n",
    "# â­ FIX 4: SOFT LABELS FOR GAN (Label Smoothing)\n",
    "# ============================================================\n",
    "\n",
    "def train_gan(G, D, model, loader, epochs=20, device='cuda', l1=1.0, l2=1.0):\n",
    "    \"\"\"\n",
    "    Train the Fairness GAN with Mixed Precision\n",
    "    \n",
    "    â­ FIX 4: Uses SOFT LABELS instead of hard 0/1 labels\n",
    "    - Real labels: uniform(0.9, 1.0) instead of 1.0\n",
    "    - Fake labels: uniform(0.0, 0.1) instead of 0.0\n",
    "    - Prevents Discriminator from becoming too confident (mode collapse)\n",
    "    \"\"\"\n",
    "    G, D, model = G.to(device), D.to(device), model.to(device)\n",
    "    model.eval()\n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # AMP scalers\n",
    "    if USE_AMP:\n",
    "        scaler_G = torch.amp.GradScaler(device='cuda')\n",
    "        scaler_D = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            bs = imgs.size(0)\n",
    "            \n",
    "            # â­ FIX 4: SOFT LABELS (Label Smoothing)\n",
    "            # Instead of hard 1.0/0.0, use soft ranges to prevent D from being too confident\n",
    "            real_labels = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)  # Real: 0.9-1.0\n",
    "            fake_labels = torch.empty(bs, 1, device=device).uniform_(0.0, 0.1)  # Fake: 0.0-0.1\n",
    "            \n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            z = torch.randn(bs, G.latent_dim, device=device)\n",
    "            gl = torch.randint(0, G.num_classes, (bs,), device=device)\n",
    "            \n",
    "            # Generator\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    with torch.no_grad():\n",
    "                        px, pxp = model(x), model(xp)\n",
    "                    t1 = -torch.mean((px - pxp) ** 2)\n",
    "                    t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                    # Use soft real labels for generator (wants D to think fake is real)\n",
    "                    g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                    t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                    g_loss = t1 + t2 + t3\n",
    "                scaler_G.scale(g_loss).backward()\n",
    "                scaler_G.step(opt_G)\n",
    "                scaler_G.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                with torch.no_grad():\n",
    "                    px, pxp = model(x), model(xp)\n",
    "                t1 = -torch.mean((px - pxp) ** 2)\n",
    "                t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                g_loss = t1 + t2 + t3\n",
    "                g_loss.backward()\n",
    "                opt_G.step()\n",
    "            \n",
    "            # Discriminator\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    # â­ FIX 4: Use soft labels for discriminator training\n",
    "                    d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                              bce(D(x.detach(), gl), fake_labels) + \n",
    "                              bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                scaler_D.scale(d_loss).backward()\n",
    "                scaler_D.step(opt_D)\n",
    "                scaler_D.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                          bce(D(x.detach(), gl), fake_labels) + \n",
    "                          bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                d_loss.backward()\n",
    "                opt_D.step()\n",
    "    \n",
    "    return G, D\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bias(model, x, xp, device):\n",
    "    \"\"\"Compute bias as difference in model predictions between x and x'\"\"\"\n",
    "    model.eval()\n",
    "    x, xp = x.to(device), xp.to(device)\n",
    "    \n",
    "    if USE_AMP:\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    else:\n",
    "        diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    \n",
    "    return diff.item()\n",
    "\n",
    "\n",
    "def partition_data_pathological_non_iid(dataset, n_clients, classes_per_client=2):\n",
    "    \"\"\"\n",
    "    â­ PATHOLOGICAL NON-IID: Each client gets ONLY 'classes_per_client' classes.\n",
    "    â­ UNEQUAL SAMPLE SIZES: Clients have different amounts of data.\n",
    "    \"\"\"\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    n_classes = len(np.unique(labels))  # 10 for CIFAR-10\n",
    "    \n",
    "    # Get indices for each class\n",
    "    class_indices = {c: np.where(labels == c)[0] for c in range(n_classes)}\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "    \n",
    "    # Assign classes to clients (cycling through with only 2 classes each)\n",
    "    client_classes = []\n",
    "    for cid in range(n_clients):\n",
    "        # Each client gets 2 consecutive classes (with wrap-around)\n",
    "        start_class = (cid * classes_per_client) % n_classes\n",
    "        assigned = []\n",
    "        for i in range(classes_per_client):\n",
    "            assigned.append((start_class + i) % n_classes)\n",
    "        client_classes.append(assigned)\n",
    "    \n",
    "    # Count how many clients share each class\n",
    "    class_client_count = {c: sum(1 for cc in client_classes if c in cc) for c in range(n_classes)}\n",
    "    \n",
    "    # â­ UNEQUAL SAMPLE SIZES: Use Dirichlet to create unequal data distribution\n",
    "    # Generate unequal proportions for each client using Dirichlet distribution\n",
    "    alpha = 0.5  # Lower alpha = more unequal distribution\n",
    "    client_proportions = np.random.dirichlet([alpha] * n_clients)\n",
    "    \n",
    "    # Distribute samples to clients with UNEQUAL sizes\n",
    "    client_indices = [[] for _ in range(n_clients)]\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        # Find clients that have this class\n",
    "        clients_with_class = [cid for cid in range(n_clients) if c in client_classes[cid]]\n",
    "        \n",
    "        if len(clients_with_class) > 0:\n",
    "            # Calculate how many samples each client gets for this class\n",
    "            # Based on client proportions (unequal sizes)\n",
    "            class_samples = class_indices[c]\n",
    "            total_for_class = len(class_samples)\n",
    "            \n",
    "            # Normalize proportions for only clients that have this class\n",
    "            relevant_props = np.array([client_proportions[cid] for cid in clients_with_class])\n",
    "            relevant_props = relevant_props / relevant_props.sum()  # Normalize\n",
    "            \n",
    "            # Assign samples based on proportions\n",
    "            start_idx = 0\n",
    "            for i, cid in enumerate(clients_with_class):\n",
    "                if i == len(clients_with_class) - 1:  # Last client gets remaining samples\n",
    "                    end_idx = total_for_class\n",
    "                else:\n",
    "                    n_samples = int(total_for_class * relevant_props[i])\n",
    "                    end_idx = min(start_idx + n_samples, total_for_class)\n",
    "                \n",
    "                if start_idx < end_idx:\n",
    "                    client_indices[cid].extend(class_samples[start_idx:end_idx].tolist())\n",
    "                start_idx = end_idx\n",
    "    \n",
    "    # Shuffle each client's data and ensure minimum samples\n",
    "    result = []\n",
    "    for cid in range(n_clients):\n",
    "        if len(client_indices[cid]) > 0:\n",
    "            indices = np.array(client_indices[cid])\n",
    "            np.random.shuffle(indices)\n",
    "            result.append(indices)\n",
    "        else:\n",
    "            # Fallback: give at least some samples if client has none\n",
    "            # This shouldn't happen with proper Dirichlet, but safety check\n",
    "            fallback_samples = np.random.choice(len(dataset), size=50, replace=False)\n",
    "            result.append(fallback_samples)\n",
    "    \n",
    "    return result, client_classes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model accuracy with AMP\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for d, t in loader:\n",
    "        d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                preds = model(d).argmax(1)\n",
    "        else:\n",
    "            preds = model(d).argmax(1)\n",
    "        \n",
    "        correct += (preds == t).sum().item()\n",
    "        total += len(t)\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_client(model, client_loaders, device):\n",
    "    \"\"\"Evaluate model accuracy on EACH client's data.\"\"\"\n",
    "    model.eval()\n",
    "    client_accuracies = []\n",
    "    \n",
    "    for loader in client_loaders:\n",
    "        correct, total = 0, 0\n",
    "        for d, t in loader:\n",
    "            d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "            \n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    preds = model(d).argmax(1)\n",
    "            else:\n",
    "                preds = model(d).argmax(1)\n",
    "            \n",
    "            correct += (preds == t).sum().item()\n",
    "            total += len(t)\n",
    "        \n",
    "        acc = 100 * correct / total if total > 0 else 0\n",
    "        client_accuracies.append(acc)\n",
    "    \n",
    "    return client_accuracies\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FAIRNESS METRICS (Based on Per-Client Performance!)\n",
    "# ============================================================\n",
    "\n",
    "print(\"   â­ FIX 4: GAN uses SOFT LABELS (0.9-1.0 for real, 0.0-0.1 for fake)\")\n",
    "print(\"âœ… Helper functions defined (2 classes per client, unequal sizes)\")\n",
    "\n",
    "def calculate_jfi(performances):\n",
    "    \"\"\"Jain's Fairness Index: (Î£páµ¢)Â² / (N Ã— Î£páµ¢Â²)\"\"\"\n",
    "    p = np.array(performances)\n",
    "    n = len(p)\n",
    "    if np.sum(p ** 2) == 0:\n",
    "        return 1.0\n",
    "    return (np.sum(p) ** 2) / (n * np.sum(p ** 2))\n",
    "\n",
    "def calculate_accuracy_gap(performances):\n",
    "    \"\"\"Accuracy gap: max(acc) - min(acc)\"\"\"\n",
    "    return np.max(performances) - np.min(performances)\n",
    "\n",
    "def calculate_variance(performances):\n",
    "    \"\"\"Variance of per-client accuracies\"\"\"\n",
    "    return np.var(performances)\n",
    "\n",
    "def calculate_max_min_fairness(performances):\n",
    "    \"\"\"Max-Min Fairness: min(acc) / max(acc)\"\"\"\n",
    "    p = np.array(performances)\n",
    "    if np.max(p) == 0:\n",
    "        return 0.0\n",
    "    return np.min(p) / np.max(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95664af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION (WITH STABILITY FIXES!)\n",
    "# ============================================================\n",
    "\n",
    "# Training Parameters\n",
    "N_ROUNDS = 50           # Total training rounds\n",
    "N_CLIENTS = 20          # Number of federated clients\n",
    "N_GAN_EPOCHS = 15       # GAN training epochs per round (Fed-Audit-GAN only)\n",
    "N_PROBES = 500          # Number of probe samples (Fed-Audit-GAN only)\n",
    "LOCAL_EPOCHS = 3        # Local training epochs per client\n",
    "\n",
    "# â­ FIX 1: EXTENDED WARM-UP (10 rounds instead of 5)\n",
    "# Let model learn basics before fairness auditing kicks in\n",
    "WARMUP_ROUNDS = 10      # â¬†ï¸ Increased from 5 to 10!\n",
    "\n",
    "# â­ FIX 2: GRADIENT CLIPPING\n",
    "GRAD_CLIP_NORM = 1.0    # Maximum gradient norm (prevents \"shock\" updates)\n",
    "\n",
    "# â­ FIX 3: REDUCED AUDIT FREQUENCY\n",
    "AUDIT_FREQUENCY = 2     # Only audit every 2nd round after warm-up\n",
    "\n",
    "# Fed-Audit-GAN Parameters\n",
    "MOMENTUM = 0.8          # EMA momentum for fairness scores\n",
    "MU = 0.01               # FedProx proximal term strength\n",
    "\n",
    "# â­ RECOMMENDED: Î³ = 0.5 (balanced fairness)\n",
    "GAMMA = 0.5             # Single optimal gamma value\n",
    "\n",
    "# DataLoader Parameters (GPU Optimized for T4 x2)\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "PREFETCH_FACTOR = 2\n",
    "\n",
    "# PATHOLOGICAL NON-IID\n",
    "CLASSES_PER_CLIENT = 2  # â­ Each client gets only 2 classes\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ”§ Fed-Audit-GAN v3.0 - CIFAR-10 (STABILITY FIXES)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPUs: {NUM_GPUS}\")\n",
    "print(f\"AMP Enabled: {USE_AMP}\")\n",
    "print(f\"Rounds: {N_ROUNDS}, Clients: {N_CLIENTS}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ EXPERIMENTS TO RUN:\")\n",
    "print(f\"   ðŸ”µ 1. FedAvg (SEPARATE BASELINE)\")\n",
    "print(f\"   ðŸŸ¢ 2. Fed-Audit-GAN Î³={GAMMA} (With Stability Fixes)\")\n",
    "\n",
    "print(f\"\\nâ­ 4 STABILITY FIXES APPLIED:\")\n",
    "print(f\"   FIX 1: Extended Warm-up = {WARMUP_ROUNDS} rounds (was 5)\")\n",
    "print(f\"   FIX 2: Gradient Clipping = max_norm {GRAD_CLIP_NORM}\")\n",
    "print(f\"   FIX 3: Audit Frequency = every {AUDIT_FREQUENCY} rounds\")\n",
    "print(f\"   FIX 4: Soft Labels for GAN (0.9-1.0 real, 0.0-0.1 fake)\")\n",
    "\n",
    "print(f\"\\nâ­ FED-AUDIT-GAN PARAMETERS:\")\n",
    "print(f\"   Gamma (Î³): {GAMMA} (recommended balanced value)\")\n",
    "print(f\"   Momentum (Î²): {MOMENTUM}\")\n",
    "print(f\"   FedProx (Î¼): {MU}\")\n",
    "\n",
    "print(f\"\\nâ­ PATHOLOGICAL NON-IID (UNEQUAL SIZES):\")\n",
    "print(f\"   Each client gets ONLY {CLASSES_PER_CLIENT}/10 classes\")\n",
    "print(f\"   Clients have DIFFERENT sample sizes (Dirichlet Î±=0.5)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85693d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING (CIFAR-10 with GPU Optimizations)\n",
    "# ============================================================\n",
    "\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create PATHOLOGICAL Non-IID partitions (3 classes per client)\n",
    "np.random.seed(42)\n",
    "client_idx, client_classes = partition_data_pathological_non_iid(\n",
    "    train_data, N_CLIENTS, classes_per_client=CLASSES_PER_CLIENT\n",
    ")\n",
    "\n",
    "# Calculate data weights for each client\n",
    "client_data_sizes = [len(idx) for idx in client_idx]\n",
    "total_samples = sum(client_data_sizes)\n",
    "CLIENT_DATA_WEIGHTS = [size / total_samples for size in client_data_sizes]\n",
    "\n",
    "# DataLoader kwargs (GPU optimized)\n",
    "dataloader_kwargs = {\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY,\n",
    "    'persistent_workers': True,\n",
    "    'prefetch_factor': PREFETCH_FACTOR,\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=VAL_BATCH_SIZE, shuffle=False, **dataloader_kwargs)\n",
    "val_loader = DataLoader(\n",
    "    Subset(train_data, np.random.choice(len(train_data), 2000, replace=False)),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **dataloader_kwargs\n",
    ")\n",
    "\n",
    "client_loaders = [\n",
    "    DataLoader(Subset(train_data, client_idx[c]), batch_size=BATCH_SIZE, shuffle=True, **dataloader_kwargs)\n",
    "    for c in range(N_CLIENTS)\n",
    "]\n",
    "\n",
    "print(f\"âœ… Data loaded!\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"â­ PATHOLOGICAL NON-IID + UNEQUAL SIZES ({CLASSES_PER_CLIENT} classes per client)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"   This creates EXTREME fairness challenges:\")\n",
    "print(f\"   - Each client only sees {CLASSES_PER_CLIENT}/10 classes\")\n",
    "print(f\"   - Clients have DIFFERENT amounts of data\")\n",
    "print(f\"   - Forces model to handle both class and size heterogeneity\")\n",
    "for i in range(min(10, N_CLIENTS)):\n",
    "    class_names = [CIFAR10_CLASSES[c] for c in client_classes[i]]\n",
    "    print(f\"   Client {i:2d}: {client_classes[i]} â†’ {class_names} ({client_data_sizes[i]} samples)\")\n",
    "\n",
    "if N_CLIENTS > 10:\n",
    "    print(f\"   ... ({N_CLIENTS - 10} more clients)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7f4458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸš€ SEPARATE IMPLEMENTATIONS: FedAvg vs Fed-Audit-GAN v3.0\n",
    "# ============================================================\n",
    "\n",
    "# ============================================================\n",
    "# PART A: FedAvg (Federated Averaging) - SEPARATE BASELINE\n",
    "# ============================================================\n",
    "\n",
    "def run_fedavg(n_rounds, n_clients, train_data, client_idx, val_loader, test_loader, \n",
    "               client_loaders, local_epochs, device, use_amp, client_data_weights):\n",
    "    \"\"\"\n",
    "    Standard FedAvg (McMahan et al., 2017)\n",
    "    - No GAN, no fairness scoring\n",
    "    - Pure data-weighted averaging of model updates\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸ”µ RUNNING: FedAvg (Federated Averaging) - BASELINE\")\n",
    "    print(\"   This is a SEPARATE algorithm, NOT gamma=0 of Fed-Audit-GAN!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=\"FedAvg\"):\n",
    "        local_weights = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            loss = criterion(output, target)\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            local_weights.append(copy.deepcopy(local_model.state_dict()))\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Data-Weighted Averaging\n",
    "        avg_weights = copy.deepcopy(local_weights[0])\n",
    "        for key in avg_weights.keys():\n",
    "            avg_weights[key] = avg_weights[key] * client_data_weights[0]\n",
    "            for i in range(1, len(local_weights)):\n",
    "                avg_weights[key] += local_weights[i][key] * client_data_weights[i]\n",
    "        \n",
    "        model.load_state_dict(avg_weights)\n",
    "        \n",
    "        # Evaluation\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PART B: Fed-Audit-GAN v3.0 (WITH ALL 4 STABILITY FIXES!)\n",
    "# ============================================================\n",
    "\n",
    "def run_fed_audit_gan_v3(gamma, n_rounds, n_clients, warmup_rounds, momentum, mu,\n",
    "                         grad_clip_norm, audit_frequency,\n",
    "                         train_data, client_idx, val_loader, test_loader, client_loaders,\n",
    "                         n_gan_epochs, n_probes, local_epochs, device, use_amp,\n",
    "                         client_data_weights):\n",
    "    \"\"\"\n",
    "    Fed-Audit-GAN v3.0 with 4-Phase Architecture + STABILITY FIXES\n",
    "    \n",
    "    â­ FIX 1: Extended Warm-up (10 rounds) - Let model learn before auditing\n",
    "    â­ FIX 2: Gradient Clipping (max_norm=1.0) - Prevent gradient shocks\n",
    "    â­ FIX 3: Reduced Audit Frequency (every 2nd round) - Breathing room\n",
    "    â­ FIX 4: Soft Labels for GAN - Prevent mode collapse (in train_gan)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ðŸŸ¢ RUNNING: Fed-Audit-GAN v3.0 (Î³={gamma}) - WITH STABILITY FIXES\")\n",
    "    print(f\"   â­ FIX 1: Warm-up = {warmup_rounds} rounds\")\n",
    "    print(f\"   â­ FIX 2: Gradient Clipping = max_norm {grad_clip_norm}\")\n",
    "    print(f\"   â­ FIX 3: Audit every {audit_frequency} rounds after warm-up\")\n",
    "    print(f\"   â­ FIX 4: Soft Labels for GAN (built into train_gan)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    fairness_history = {i: 0.0 for i in range(n_clients)}\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'bias': [], 'alphas': [],\n",
    "        'raw_scores': [], 'smoothed_scores': [],\n",
    "        'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': [],\n",
    "        'audit_active': []  # Track when auditing is active\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=f\"Fed-Audit-GAN Î³={gamma}\"):\n",
    "        \n",
    "        # â­ FIX 1 & 3: Determine if we should audit this round\n",
    "        # Audit only if: (1) past warm-up AND (2) even round number\n",
    "        should_audit = (rnd >= warmup_rounds) and (rnd % audit_frequency == 0)\n",
    "        \n",
    "        if rnd < warmup_rounds:\n",
    "            phase_msg = f\"ðŸ”¥ Round {rnd+1}: Warm-up Phase (Pure FedAvg)\"\n",
    "        elif should_audit:\n",
    "            phase_msg = f\"âš–ï¸ Round {rnd+1}: Auditing Active (Î³={gamma})\"\n",
    "        else:\n",
    "            phase_msg = f\"ðŸ˜®â€ðŸ’¨ Round {rnd+1}: Breathing Round (Skip Audit)\"\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 1: Local Client Training (with FedProx + GRADIENT CLIPPING)\n",
    "        # ================================================================\n",
    "        updates = []\n",
    "        global_params = [p.clone().detach() for p in model.parameters()]\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            before_state = copy.deepcopy(model.state_dict())\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            ce_loss = F.cross_entropy(output, target)\n",
    "                            prox_loss = sum(((lp - gp) ** 2).sum() \n",
    "                                          for lp, gp in zip(local_model.parameters(), global_params))\n",
    "                            loss = ce_loss + (mu / 2) * prox_loss\n",
    "                        scaler.scale(loss).backward()\n",
    "                        \n",
    "                        # â­ FIX 2: GRADIENT CLIPPING before optimizer step\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=grad_clip_norm)\n",
    "                        \n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        ce_loss = F.cross_entropy(output, target)\n",
    "                        prox_loss = sum(((lp - gp) ** 2).sum() \n",
    "                                      for lp, gp in zip(local_model.parameters(), global_params))\n",
    "                        loss = ce_loss + (mu / 2) * prox_loss\n",
    "                        loss.backward()\n",
    "                        \n",
    "                        # â­ FIX 2: GRADIENT CLIPPING before optimizer step\n",
    "                        torch.nn.utils.clip_grad_norm_(local_model.parameters(), max_norm=grad_clip_norm)\n",
    "                        \n",
    "                        optimizer.step()\n",
    "            \n",
    "            update = {k: local_model.state_dict()[k] - before_state[k] for k in before_state}\n",
    "            updates.append(update)\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 2 & 3: GAN Training + Fairness Scoring (ONLY if should_audit)\n",
    "        # ================================================================\n",
    "        B_base = 0.0\n",
    "        S_fair_raw = [0.0] * n_clients\n",
    "        S_fair_smoothed = [0.0] * n_clients\n",
    "        \n",
    "        if should_audit:\n",
    "            # â­ FIX 4: train_gan now uses SOFT LABELS internally\n",
    "            G = FairnessGenerator(img_shape=(3, 32, 32)).to(device)\n",
    "            D = Discriminator(img_shape=(3, 32, 32)).to(device)\n",
    "            G, D = train_gan(G, D, model, val_loader, epochs=n_gan_epochs, device=device)\n",
    "            \n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(n_probes, G.latent_dim, device=device)\n",
    "                labels = torch.randint(0, 10, (n_probes,), device=device)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        x_probe, xp_probe = G(z, labels)\n",
    "                else:\n",
    "                    x_probe, xp_probe = G(z, labels)\n",
    "            \n",
    "            B_base = compute_bias(model, x_probe, xp_probe, device)\n",
    "            \n",
    "            S_fair_raw = []\n",
    "            S_fair_smoothed = []\n",
    "            \n",
    "            for cid, upd in enumerate(updates):\n",
    "                hyp_model = copy.deepcopy(model)\n",
    "                hyp_state = hyp_model.state_dict()\n",
    "                for k in hyp_state:\n",
    "                    hyp_state[k] = hyp_state[k] + upd[k]\n",
    "                hyp_model.load_state_dict(hyp_state)\n",
    "                \n",
    "                B_client = compute_bias(hyp_model, x_probe, xp_probe, device)\n",
    "                S_current = B_base - B_client\n",
    "                S_fair_raw.append(S_current)\n",
    "                \n",
    "                # Apply momentum (EMA smoothing)\n",
    "                S_prev = fairness_history[cid]\n",
    "                S_smoothed = (momentum * S_prev) + ((1 - momentum) * S_current)\n",
    "                fairness_history[cid] = S_smoothed\n",
    "                S_fair_smoothed.append(S_smoothed)\n",
    "                del hyp_model\n",
    "            \n",
    "            del G, D, x_probe, xp_probe\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Use previous smoothed scores (they're already in fairness_history)\n",
    "            S_fair_smoothed = [fairness_history[i] for i in range(n_clients)]\n",
    "        \n",
    "        history['raw_scores'].append(S_fair_raw.copy())\n",
    "        history['smoothed_scores'].append(S_fair_smoothed.copy())\n",
    "        history['audit_active'].append(should_audit)\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 4: Aggregation\n",
    "        # ================================================================\n",
    "        if rnd < warmup_rounds:\n",
    "            # â­ FIX 1: During extended warm-up, use pure FedAvg\n",
    "            alphas = client_data_weights.copy()\n",
    "        else:\n",
    "            # After warm-up: use fairness-weighted aggregation\n",
    "            alphas = F.softmax(torch.tensor(S_fair_smoothed) * gamma, dim=0).tolist()\n",
    "        \n",
    "        new_state = model.state_dict()\n",
    "        for k in new_state:\n",
    "            new_state[k] = new_state[k] + sum(a * u[k] for a, u in zip(alphas, updates))\n",
    "        model.load_state_dict(new_state)\n",
    "        \n",
    "        # ================================================================\n",
    "        # EVALUATION\n",
    "        # ================================================================\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['bias'].append(B_base)\n",
    "        history['alphas'].append(alphas.copy())\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'bias': B_base,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs),\n",
    "            'audit_active': int(should_audit)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RUN ALL EXPERIMENTS\n",
    "# ============================================================\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 1: FedAvg (SEPARATE BASELINE)\n",
    "# ============================================================\n",
    "wandb.init(\n",
    "    project=\"FED_AUDIT_GAN_TEST_3_CIFAR10\",\n",
    "    name=f\"FedAvg_CIFAR10_clients{N_CLIENTS}_v3\",\n",
    "    config={\n",
    "        \"method\": \"FedAvg\",\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "        \"n_rounds\": N_ROUNDS,\n",
    "        \"n_clients\": N_CLIENTS,\n",
    "        \"non_iid_type\": \"pathological\",\n",
    "        \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "        \"device\": str(DEVICE),\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"amp_enabled\": USE_AMP\n",
    "    }\n",
    ")\n",
    "\n",
    "fedavg_model, fedavg_history = run_fedavg(\n",
    "    n_rounds=N_ROUNDS,\n",
    "    n_clients=N_CLIENTS,\n",
    "    train_data=train_data,\n",
    "    client_idx=client_idx,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    client_loaders=client_loaders,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    device=DEVICE,\n",
    "    use_amp=USE_AMP,\n",
    "    client_data_weights=CLIENT_DATA_WEIGHTS\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "all_results['FedAvg'] = {\n",
    "    'model': fedavg_model,\n",
    "    'history': fedavg_history,\n",
    "    'name': 'FedAvg'\n",
    "}\n",
    "\n",
    "print(f\"âœ… FedAvg Complete!\")\n",
    "print(f\"   Final Accuracy: {fedavg_history['acc'][-1]:.2f}%\")\n",
    "print(f\"   Final JFI: {fedavg_history['jfi'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 2: Fed-Audit-GAN v3.0 with Î³ = 0.5 (RECOMMENDED)\n",
    "# ============================================================\n",
    "method_name = f\"FedAuditGAN_v3_Î³={GAMMA}\"\n",
    "\n",
    "wandb.init(\n",
    "    project=\"FED_AUDIT_GAN_TEST_3_CIFAR10\",\n",
    "    name=f\"{method_name}_CIFAR10_clients{N_CLIENTS}_stability\",\n",
    "    config={\n",
    "        \"method\": method_name,\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "        \"n_rounds\": N_ROUNDS,\n",
    "        \"n_clients\": N_CLIENTS,\n",
    "        \"gamma\": GAMMA,\n",
    "        \"momentum\": MOMENTUM,\n",
    "        \"warmup_rounds\": WARMUP_ROUNDS,\n",
    "        \"mu_fedprox\": MU,\n",
    "        \"grad_clip_norm\": GRAD_CLIP_NORM,\n",
    "        \"audit_frequency\": AUDIT_FREQUENCY,\n",
    "        \"non_iid_type\": \"pathological\",\n",
    "        \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "        \"stability_fixes\": [\"extended_warmup\", \"gradient_clipping\", \"reduced_audit_freq\", \"soft_labels\"],\n",
    "        \"device\": str(DEVICE),\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"amp_enabled\": USE_AMP\n",
    "    }\n",
    ")\n",
    "\n",
    "model, history = run_fed_audit_gan_v3(\n",
    "    gamma=GAMMA,\n",
    "    n_rounds=N_ROUNDS,\n",
    "    n_clients=N_CLIENTS,\n",
    "    warmup_rounds=WARMUP_ROUNDS,\n",
    "    momentum=MOMENTUM,\n",
    "    mu=MU,\n",
    "    grad_clip_norm=GRAD_CLIP_NORM,\n",
    "    audit_frequency=AUDIT_FREQUENCY,\n",
    "    train_data=train_data,\n",
    "    client_idx=client_idx,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    client_loaders=client_loaders,\n",
    "    n_gan_epochs=N_GAN_EPOCHS,\n",
    "    n_probes=N_PROBES,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    device=DEVICE,\n",
    "    use_amp=USE_AMP,\n",
    "    client_data_weights=CLIENT_DATA_WEIGHTS\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "all_results[method_name] = {\n",
    "    'model': model,\n",
    "    'history': history,\n",
    "    'name': method_name\n",
    "}\n",
    "\n",
    "print(f\"âœ… {method_name} Complete!\")\n",
    "print(f\"   Final Accuracy: {history['acc'][-1]:.2f}%\")\n",
    "print(f\"   Final JFI: {history['jfi'][-1]:.4f}\")\n",
    "print(f\"   Accuracy Gap: {history['accuracy_gap'][-1]:.2f}%\")\n",
    "print(f\"   Final Bias: {history['bias'][-1]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ“Š Experiments Run:\")\n",
    "print(\"   1. FedAvg (Separate Baseline)\")\n",
    "print(f\"   2. Fed-Audit-GAN v3.0 Î³={GAMMA} (With 4 Stability Fixes)\")\n",
    "print(\"ðŸ“Š Check your WandB dashboard: https://wandb.ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40880d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š RESULTS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(\"ðŸ“Š CIFAR-10: FedAvg vs Fed-Audit-GAN v3.0 (WITH STABILITY FIXES)\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "print(f\"\\n{'METHOD':<30} {'GLOBAL ACC':<12} {'JFI':<10} {'MAX-MIN':<10} {'GAP':<10} {'MIN ACC':<10} {'MAX ACC':<10}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "best_acc = max(all_results[m]['history']['acc'][-1] for m in method_names)\n",
    "best_jfi = max(all_results[m]['history']['jfi'][-1] for m in method_names)\n",
    "lowest_gap = min(all_results[m]['history']['accuracy_gap'][-1] for m in method_names)\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    max_min = all_results[method]['history']['max_min_fairness'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    min_acc = all_results[method]['history']['min_client_acc'][-1]\n",
    "    max_acc = all_results[method]['history']['max_client_acc'][-1]\n",
    "    \n",
    "    acc_mark = \"ðŸ†\" if acc == best_acc else \"\"\n",
    "    jfi_mark = \"â­\" if jfi == best_jfi else \"\"\n",
    "    gap_mark = \"âœ…\" if gap == lowest_gap else \"\"\n",
    "    \n",
    "    print(f\"{name:<30} {acc:>8.2f}% {acc_mark:<2} {jfi:>8.4f} {jfi_mark:<2} {max_min:>8.4f}   {gap:>6.2f}% {gap_mark:<2} {min_acc:>8.2f}%  {max_acc:>8.2f}%\")\n",
    "\n",
    "print(\"=\" * 110)\n",
    "\n",
    "# Improvement over FedAvg\n",
    "fedavg_acc = all_results['FedAvg']['history']['acc'][-1]\n",
    "fedavg_jfi = all_results['FedAvg']['history']['jfi'][-1]\n",
    "fedavg_gap = all_results['FedAvg']['history']['accuracy_gap'][-1]\n",
    "\n",
    "print(f\"\\nðŸ“ˆ IMPROVEMENT OVER FedAvg:\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    \n",
    "    acc_diff = acc - fedavg_acc\n",
    "    jfi_diff = jfi - fedavg_jfi\n",
    "    gap_reduction = fedavg_gap - gap\n",
    "    \n",
    "    print(f\"   {name}:\")\n",
    "    print(f\"      Accuracy: {'+' if acc_diff >= 0 else ''}{acc_diff:.2f}%\")\n",
    "    print(f\"      JFI: {'+' if jfi_diff >= 0 else ''}{jfi_diff:.4f}\")\n",
    "    print(f\"      Gap Reduction: {gap_reduction:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"â­ STABILITY FIXES APPLIED:\")\n",
    "print(f\"   FIX 1: Extended Warm-up = {WARMUP_ROUNDS} rounds\")\n",
    "print(f\"   FIX 2: Gradient Clipping = max_norm {GRAD_CLIP_NORM}\")\n",
    "print(f\"   FIX 3: Audit Frequency = every {AUDIT_FREQUENCY} rounds\")\n",
    "print(\"   FIX 4: Soft Labels for GAN (0.9-1.0 real, 0.0-0.1 fake)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80350a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š COMPREHENSIVE VISUALIZATION (WITH STABILITY REGIONS)\n",
    "# ============================================================\n",
    "\n",
    "colors = {\n",
    "    'FedAvg': '#e74c3c',                      # Red\n",
    "    f'FedAuditGAN_v3_Î³={GAMMA}': '#2ecc71',   # Green\n",
    "}\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "rounds = range(1, N_ROUNDS + 1)\n",
    "\n",
    "# Plot 1: Global Accuracy\n",
    "ax = axes[0, 0]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    linewidth = 2.5 if method == 'FedAvg' else 2\n",
    "    ax.plot(rounds, acc, color=colors[method], linestyle=linestyle, \n",
    "            linewidth=linewidth, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange', label=f'Warm-up ({WARMUP_ROUNDS} rounds)')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('CIFAR-10: Global Test Accuracy (Stable Convergence)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: JFI\n",
    "ax = axes[0, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    jfi = all_results[method]['history']['jfi']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, jfi, color=colors[method], linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('JFI', fontsize=12)\n",
    "ax.set_title(\"Jain's Fairness Index (Steady Improvement)\", fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy Gap\n",
    "ax = axes[0, 2]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    gap = all_results[method]['history']['accuracy_gap']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, gap, color=colors[method], linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy Gap (%)', fontsize=12)\n",
    "ax.set_title('Best-Worst Client Gap (Smooth Reduction)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Variance\n",
    "ax = axes[1, 0]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    var = all_results[method]['history']['variance']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, var, color=colors[method], linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Variance', fontsize=12)\n",
    "ax.set_title('Per-Client Accuracy Variance (No Zig-Zag!)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Min-Max Range\n",
    "ax = axes[1, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    min_acc = all_results[method]['history']['min_client_acc']\n",
    "    max_acc = all_results[method]['history']['max_client_acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.fill_between(rounds, min_acc, max_acc, color=colors[method], alpha=0.2)\n",
    "    ax.plot(rounds, min_acc, color=colors[method], linestyle=linestyle, linewidth=1.5)\n",
    "    ax.plot(rounds, max_acc, color=colors[method], linestyle=linestyle, linewidth=1.5, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange')\n",
    "ax.set_xlabel('Round', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Min-Max Client Accuracy Range', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final Per-Client Accuracy\n",
    "ax = axes[1, 2]\n",
    "x = np.arange(N_CLIENTS)\n",
    "width = 0.35\n",
    "for i, method in enumerate(method_names):\n",
    "    name = all_results[method]['name']\n",
    "    client_accs = all_results[method]['history']['client_accuracies'][-1]\n",
    "    ax.bar(x + i*width, client_accs, width, label=name, color=colors[method], alpha=0.8)\n",
    "ax.set_xlabel('Client ID', fontsize=12)\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Per-Client Accuracy (Final Round)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cifar10_v3_stability_fixes_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“ Results saved to: cifar10_v3_stability_fixes_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ce3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE ALL MODELS AND RESULTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs('results_cifar10_v3_stable', exist_ok=True)\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    filename = f\"results_cifar10_v3_stable/{name.replace('=', '').replace('.', '_').replace('Î³', 'gamma')}_CIFAR10.pth\"\n",
    "    \n",
    "    save_dict = {\n",
    "        'model_state_dict': all_results[method]['model'].state_dict(),\n",
    "        'history': all_results[method]['history'],\n",
    "        'config': {\n",
    "            'n_rounds': N_ROUNDS,\n",
    "            'n_clients': N_CLIENTS,\n",
    "            'classes_per_client': CLASSES_PER_CLIENT,\n",
    "            'device': str(DEVICE),\n",
    "            'num_gpus': NUM_GPUS\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if method != 'FedAvg':\n",
    "        save_dict['config']['momentum'] = MOMENTUM\n",
    "        save_dict['config']['warmup_rounds'] = WARMUP_ROUNDS\n",
    "        save_dict['config']['mu'] = MU\n",
    "        save_dict['config']['gamma'] = GAMMA\n",
    "        save_dict['config']['grad_clip_norm'] = GRAD_CLIP_NORM\n",
    "        save_dict['config']['audit_frequency'] = AUDIT_FREQUENCY\n",
    "        save_dict['config']['stability_fixes'] = [\n",
    "            \"extended_warmup_10_rounds\",\n",
    "            \"gradient_clipping_1.0\",\n",
    "            \"reduced_audit_freq_every_2\",\n",
    "            \"soft_labels_gan\"\n",
    "        ]\n",
    "    \n",
    "    torch.save(save_dict, filename)\n",
    "    print(f\"âœ… Saved: {filename}\")\n",
    "\n",
    "with open('results_cifar10_v3_stable/all_results_summary.pkl', 'wb') as f:\n",
    "    summary = {\n",
    "        method: {\n",
    "            'name': all_results[method]['name'],\n",
    "            'history': all_results[method]['history'],\n",
    "            'final_acc': all_results[method]['history']['acc'][-1],\n",
    "            'final_jfi': all_results[method]['history']['jfi'][-1]\n",
    "        }\n",
    "        for method in method_names\n",
    "    }\n",
    "    pickle.dump(summary, f)\n",
    "print(\"âœ… Saved: results_cifar10_v3_stable/all_results_summary.pkl\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“Š FINAL RESULTS SUMMARY (Fed-Audit-GAN v3.0 with Stability Fixes)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ”µ BASELINE:\")\n",
    "print(f\"   FedAvg: {all_results['FedAvg']['history']['acc'][-1]:.2f}% accuracy, JFI={all_results['FedAvg']['history']['jfi'][-1]:.4f}\")\n",
    "print(\"\\nðŸŸ¢ FED-AUDIT-GAN v3.0 (WITH STABILITY FIXES):\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    print(f\"   {name}: {acc:.2f}% accuracy, JFI={jfi:.4f}, Gap={gap:.2f}%\")\n",
    "\n",
    "print(\"\\nâ­ STABILITY FIXES SUMMARY:\")\n",
    "print(f\"   1. Extended Warm-up: {WARMUP_ROUNDS} rounds (model learns basics first)\")\n",
    "print(f\"   2. Gradient Clipping: max_norm={GRAD_CLIP_NORM} (prevents shock updates)\")\n",
    "print(f\"   3. Audit Frequency: every {AUDIT_FREQUENCY} rounds (breathing room)\")\n",
    "print(\"   4. Soft Labels: 0.9-1.0 real, 0.0-0.1 fake (prevents mode collapse)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nðŸ“Š Check WandB dashboard: https://wandb.ai\")\n",
    "print(f\"   Trained on: {NUM_GPUS} GPU(s) with AMP={USE_AMP}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
