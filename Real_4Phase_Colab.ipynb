{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fed-AuditGAN: Real 4-Phase Implementation\n",
    "\n",
    "Key Formula: `alpha_k = (1 - gamma) * S_acc + gamma * S_fair`\n",
    "\n",
    "- gamma=0.3: More accuracy\n",
    "- gamma=0.5: Balanced\n",
    "- gamma=0.7: More fairness"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install wandb torch torchvision tqdm matplotlib -q\n",
    "print(\"Done!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import wandb\n",
    "wandb.login()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Fed-AuditGAN: Real 4-Phase Implementation\n",
    "Strictly follows the mathematical specification from the project slides.\n",
    "\n",
    "Phase 1: Standard FL (Client Side)\n",
    "    W_k = argmin L_task(W; D_k)\n",
    "    \u00ce\u201dW_k = W_k - \u00ce\u02dc_t\n",
    "\n",
    "Phase 2: Generative Fairness Auditing (Server Side)\n",
    "    L_G = \u00ce\u00b1 \u00c2\u00b7 L_Realism(x, x') - \u00ce\u00b2 \u00c2\u00b7 |\u00ce\u02dc_t(x) - \u00ce\u02dc_t(x')|\n",
    "    Output: Probes P = {(x_1, x'_1), ..., (x_n, x'_n)}\n",
    "\n",
    "Phase 3: Fairness Contribution Scoring (The Audit)\n",
    "    B_base = (1/|P|) \u00ce\u00a3 |\u00ce\u02dc_t(x) - \u00ce\u02dc_t(x')|\n",
    "    \u00ce\u02dc_test = \u00ce\u02dc_t + \u00ce\u201dW_k\n",
    "    B_k = (1/|P|) \u00ce\u00a3 |\u00ce\u02dc_test(x) - \u00ce\u02dc_test(x')|\n",
    "    S_k^fair = B_base - B_k\n",
    "\n",
    "Phase 4: Multi-Objective Aggregation (The Reward)\n",
    "    S_k^acc = L_val(\u00ce\u02dc_t) - L_val(\u00ce\u02dc_t + \u00ce\u201dW_k)\n",
    "    \u00ce\u00b1_k = (1 - \u00ce\u00b3) \u00c2\u00b7 S_k^acc + \u00ce\u00b3 \u00c2\u00b7 S_k^fair\n",
    "    \u00ce\u02dc_{t+1} = \u00ce\u02dc_t + \u00ce\u00b7_global \u00c2\u00b7 \u00ce\u00a3 (\u00ce\u00b1_k / \u00ce\u00a3\u00ce\u00b1_j) \u00c2\u00b7 \u00ce\u201dW_k\n",
    "\n",
    "Key Formula:\n",
    "    \u00ce\u00b1_k = (1 - \u00ce\u00b3) \u00c2\u00b7 S_k^acc + \u00ce\u00b3 \u00c2\u00b7 S_k^fair\n",
    "    \n",
    "    \u00ce\u00b3 = 0: Pure FedAvg (Ignore Fairness)\n",
    "    \u00ce\u00b3 = 1: Pure Fairness (Ignore Accuracy)\n",
    "    \u00ce\u00b3 = 0.5: Balanced Approach\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MODEL DEFINITIONS\n",
    "# ============================================================\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"Simple CNN for MNIST classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator that produces (x, x') pairs where:\n",
    "    - x and x' are similar (minimizes L_Realism)\n",
    "    - But model predictions differ (maximizes |\u00ce\u02dc(x) - \u00ce\u02dc(x')|)\n",
    "    \n",
    "    Loss: L_G = \u00ce\u00b1 \u00c2\u00b7 L_Realism(x, x') - \u00ce\u00b2 \u00c2\u00b7 |\u00ce\u02dc_t(x) - \u00ce\u02dc_t(x')|\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=100, num_classes=10, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        \n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4\n",
    "        \n",
    "        self.l1 = nn.Linear(latent_dim * 2, 128 * self.init_size ** 2)\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, 1, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Delta network for generating x' = x + delta\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(-1, 128, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        \n",
    "        # Generate delta and create x'\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        x_prime = torch.clamp(x + delta, -1, 1)\n",
    "        \n",
    "        return x, x_prime\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Conditional discriminator for GAN training\"\"\"\n",
    "    def __init__(self, num_classes=10, img_shape=(1, 28, 28)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        \n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 16, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(16, 32, 3, 2, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 64, 3, 2, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 2: GAN TRAINING\n",
    "# L_G = \u00ce\u00b1 \u00c2\u00b7 L_Realism(x, x') - \u00ce\u00b2 \u00c2\u00b7 |\u00ce\u02dc_t(x) - \u00ce\u02dc_t(x')|\n",
    "# ============================================================\n",
    "\n",
    "def train_fairness_gan(G, D, model, loader, epochs=30, device='cuda', alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Train the Fairness Generator following the formula:\n",
    "    L_G = \u00ce\u00b1 \u00c2\u00b7 L_Realism(x, x') - \u00ce\u00b2 \u00c2\u00b7 |\u00ce\u02dc_t(x) - \u00ce\u02dc_t(x')|\n",
    "    \n",
    "    Args:\n",
    "        G: Generator\n",
    "        D: Discriminator\n",
    "        model: Current global model \u00ce\u02dc_t (frozen)\n",
    "        loader: Data loader\n",
    "        epochs: Training epochs\n",
    "        alpha: Weight for realism loss\n",
    "        beta: Weight for prediction difference (adversarial bias term)\n",
    "    \"\"\"\n",
    "    G, D, model = G.to(device), D.to(device), model.to(device)\n",
    "    model.eval()  # Freeze \u00ce\u02dc_t\n",
    "    \n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            batch_size = imgs.size(0)\n",
    "            real_target = torch.ones(batch_size, 1, device=device)\n",
    "            fake_target = torch.zeros(batch_size, 1, device=device)\n",
    "            \n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            z = torch.randn(batch_size, G.latent_dim, device=device)\n",
    "            gen_labels = torch.randint(0, G.num_classes, (batch_size,), device=device)\n",
    "            \n",
    "            # Generate (x, x') pairs\n",
    "            x, x_prime = G(z, gen_labels)\n",
    "            \n",
    "            # Get predictions from frozen model \u00ce\u02dc_t\n",
    "            with torch.no_grad():\n",
    "                pred_x = model(x)\n",
    "                pred_x_prime = model(x_prime)\n",
    "            \n",
    "            # ================================================================\n",
    "            # Generator Loss: L_G = \u00ce\u00b1 \u00c2\u00b7 L_Realism - \u00ce\u00b2 \u00c2\u00b7 |\u00ce\u02dc(x) - \u00ce\u02dc(x')|\n",
    "            # ================================================================\n",
    "            \n",
    "            # Term 1: Prediction difference (we want to MAXIMIZE this, so negate)\n",
    "            # -\u00ce\u00b2 \u00c2\u00b7 |\u00ce\u02dc_t(x) - \u00ce\u02dc_t(x')|\n",
    "            pred_diff = -beta * torch.mean((pred_x - pred_x_prime) ** 2)\n",
    "            \n",
    "            # Term 2: Realism loss (keep x and x' similar)\n",
    "            # \u00ce\u00b1 \u00c2\u00b7 L_Realism(x, x') = \u00ce\u00b1 \u00c2\u00b7 ||x - x'||\u00c2\u00b2\n",
    "            realism_loss = alpha * torch.mean((x - x_prime) ** 2)\n",
    "            \n",
    "            # Term 3: GAN realism (fool the discriminator)\n",
    "            gan_loss = (bce(D(x, gen_labels), real_target) + \n",
    "                       bce(D(x_prime, gen_labels), real_target)) / 2\n",
    "            \n",
    "            # Total generator loss\n",
    "            g_loss = pred_diff + realism_loss + gan_loss\n",
    "            \n",
    "            opt_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "            \n",
    "            # ================================================================\n",
    "            # Discriminator Loss\n",
    "            # ================================================================\n",
    "            x, x_prime = G(z, gen_labels)\n",
    "            \n",
    "            d_real = bce(D(imgs, labels), real_target)\n",
    "            d_fake_x = bce(D(x.detach(), gen_labels), fake_target)\n",
    "            d_fake_xp = bce(D(x_prime.detach(), gen_labels), fake_target)\n",
    "            \n",
    "            d_loss = (d_real + d_fake_x + d_fake_xp) / 3\n",
    "            \n",
    "            opt_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            opt_D.step()\n",
    "    \n",
    "    return G, D\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# PHASE 3: FAIRNESS SCORING\n",
    "# S_k^fair = B_base - B_k\n",
    "# ============================================================\n",
    "\n",
    "def compute_bias(model, x, x_prime, device):\n",
    "    \"\"\"\n",
    "    Compute bias: B = (1/|P|) \u00ce\u00a3 |\u00ce\u02dc(x) - \u00ce\u02dc(x')|\n",
    "    \n",
    "    This is the Statistical Parity Difference (SPD) approximated\n",
    "    by the prediction gap on the probes.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred_x = model(x.to(device))\n",
    "        pred_x_prime = model(x_prime.to(device))\n",
    "        # Sum over classes, mean over samples\n",
    "        bias = torch.abs(pred_x - pred_x_prime).sum(dim=1).mean().item()\n",
    "    return bias\n",
    "\n",
    "\n",
    "def compute_accuracy_score(model, update, val_loader, device):\n",
    "    \"\"\"\n",
    "    Compute accuracy score: S_k^acc = L_val(\u00ce\u02dc_t) - L_val(\u00ce\u02dc_t + \u00ce\u201dW_k)\n",
    "    \n",
    "    Measures how much the client reduces the validation loss.\n",
    "    Positive = client improves accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute L_val(\u00ce\u02dc_t) - loss with current model\n",
    "    loss_before = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss_before += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            count += len(target)\n",
    "    loss_before /= count\n",
    "    \n",
    "    # Create hypothetical model: \u00ce\u02dc_test = \u00ce\u02dc_t + \u00ce\u201dW_k\n",
    "    hyp_model = copy.deepcopy(model)\n",
    "    hyp_sd = hyp_model.state_dict()\n",
    "    for k in hyp_sd:\n",
    "        hyp_sd[k] = hyp_sd[k] + update[k]\n",
    "    hyp_model.load_state_dict(hyp_sd)\n",
    "    \n",
    "    # Compute L_val(\u00ce\u02dc_t + \u00ce\u201dW_k)\n",
    "    loss_after = 0\n",
    "    count = 0\n",
    "    hyp_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = hyp_model(data)\n",
    "            loss_after += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            count += len(target)\n",
    "    loss_after /= count\n",
    "    \n",
    "    # S_k^acc = L_val(\u00ce\u02dc_t) - L_val(\u00ce\u02dc_t + \u00ce\u201dW_k)\n",
    "    # Positive means client reduces loss (good for accuracy)\n",
    "    return loss_before - loss_after\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def partition_data_non_iid(dataset, n_clients):\n",
    "    \"\"\"Create non-IID data partition (each client gets 2 digit classes)\"\"\"\n",
    "    idx = np.argsort([dataset[i][1] for i in range(len(dataset))])\n",
    "    shards = np.array_split(idx, n_clients * 2)\n",
    "    np.random.shuffle(shards)\n",
    "    return [np.concatenate([shards[2*i], shards[2*i+1]]) for i in range(n_clients)]\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, device):\n",
    "    \"\"\"Evaluate model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            correct += (output.argmax(1) == target).sum().item()\n",
    "            total += len(target)\n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "def normalize_scores(scores):\n",
    "    \"\"\"Normalize scores to [0, 1] range\"\"\"\n",
    "    scores = np.array(scores)\n",
    "    min_s = scores.min()\n",
    "    max_s = scores.max()\n",
    "    if max_s - min_s < 1e-8:\n",
    "        return np.ones_like(scores) / len(scores)\n",
    "    return (scores - min_s) / (max_s - min_s)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================\n",
    "\n",
    "def run_fed_audit_gan(config):\n",
    "    \"\"\"\n",
    "    Run Fed-AuditGAN with the real 4-phase algorithm.\n",
    "    \n",
    "    Key Formula (Phase 4):\n",
    "        \u00ce\u00b1_k = (1 - \u00ce\u00b3) \u00c2\u00b7 S_k^acc + \u00ce\u00b3 \u00c2\u00b7 S_k^fair\n",
    "        \n",
    "    Where:\n",
    "        \u00ce\u00b3 = 0: Pure FedAvg (Ignore Fairness)\n",
    "        \u00ce\u00b3 = 1: Pure Fairness (Ignore Accuracy)\n",
    "        \u00ce\u00b3 \u00e2\u02c6\u02c6 (0, 1): Balanced Approach\n",
    "    \"\"\"\n",
    "    device = config['device']\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb.init(\n",
    "        project=\"fed-audit-gan-real\",\n",
    "        name=f\"gamma{config['gamma']}_clients{config['n_clients']}_rounds{config['n_rounds']}\",\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    # Create client data partitions\n",
    "    client_indices = partition_data_non_iid(train_data, config['n_clients'])\n",
    "    \n",
    "    # Create data loaders\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "    val_indices = np.random.choice(len(train_data), 1000, replace=False)\n",
    "    val_loader = DataLoader(Subset(train_data, val_indices), batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Initialize global model\n",
    "    global_model = CNN().to(device)\n",
    "    \n",
    "    # History\n",
    "    history = {\n",
    "        'accuracy': [],\n",
    "        'bias': [],\n",
    "        'avg_S_fair': [],\n",
    "        'avg_S_acc': [],\n",
    "        'avg_alpha': []\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Fed-AuditGAN: Real 4-Phase Implementation\")\n",
    "    print(f\"Formula: \u00ce\u00b1_k = (1 - \u00ce\u00b3) \u00c2\u00b7 S_k^acc + \u00ce\u00b3 \u00c2\u00b7 S_k^fair\")\n",
    "    print(f\"\u00ce\u00b3 = {config['gamma']}\")\n",
    "    print(f\"  \u00ce\u00b3=0: Pure FedAvg, \u00ce\u00b3=1: Pure Fairness, \u00ce\u00b3=0.5: Balanced\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for round_num in range(config['n_rounds']):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Round {round_num + 1}/{config['n_rounds']}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # PHASE 1: Standard Federated Learning (Client Side)\n",
    "        # W_k = argmin L_task(W; D_k)\n",
    "        # \u00ce\u201dW_k = W_k - \u00ce\u02dc_t\n",
    "        # ============================================================\n",
    "        print(\"\\n\u00f0\u0178\u201c\u0152 Phase 1: Client Local Training\")\n",
    "        \n",
    "        client_updates = []\n",
    "        for client_id in tqdm(range(config['n_clients']), desc=\"  Training clients\"):\n",
    "            # Create client data loader\n",
    "            client_loader = DataLoader(\n",
    "                Subset(train_data, client_indices[client_id]),\n",
    "                batch_size=32, shuffle=True\n",
    "            )\n",
    "            \n",
    "            # Copy global model for local training\n",
    "            local_model = copy.deepcopy(global_model)\n",
    "            before_weights = copy.deepcopy(global_model.state_dict())\n",
    "            \n",
    "            # Local SGD training\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=config['lr'])\n",
    "            local_model.train()\n",
    "            \n",
    "            for epoch in range(config['local_epochs']):\n",
    "                for data, target in client_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    output = local_model(data)\n",
    "                    loss = F.cross_entropy(output, target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Compute update: \u00ce\u201dW_k = W_k - \u00ce\u02dc_t\n",
    "            update = {k: local_model.state_dict()[k] - before_weights[k] \n",
    "                     for k in before_weights}\n",
    "            client_updates.append(update)\n",
    "        \n",
    "        # ============================================================\n",
    "        # PHASE 2: Generative Fairness Auditing (Server Side)\n",
    "        # L_G = \u00ce\u00b1 \u00c2\u00b7 L_Realism(x, x') - \u00ce\u00b2 \u00c2\u00b7 |\u00ce\u02dc_t(x) - \u00ce\u02dc_t(x')|\n",
    "        # ============================================================\n",
    "        print(\"\\n\u00f0\u0178\u201c\u0152 Phase 2: GAN Fairness Auditing\")\n",
    "        \n",
    "        G = FairnessGenerator().to(device)\n",
    "        D = Discriminator().to(device)\n",
    "        \n",
    "        # Train GAN to find bias probes\n",
    "        G, D = train_fairness_gan(\n",
    "            G, D, global_model, val_loader,\n",
    "            epochs=config['gan_epochs'],\n",
    "            device=device,\n",
    "            alpha=config['gan_alpha'],\n",
    "            beta=config['gan_beta']\n",
    "        )\n",
    "        \n",
    "        # Generate fairness probes: P = {(x_1, x'_1), ..., (x_n, x'_n)}\n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(config['n_probes'], G.latent_dim, device=device)\n",
    "            labels = torch.randint(0, 10, (config['n_probes'],), device=device)\n",
    "            x_probes, x_prime_probes = G(z, labels)\n",
    "        \n",
    "        # ============================================================\n",
    "        # PHASE 3: Fairness Contribution Scoring (The Audit)\n",
    "        # B_base = (1/|P|) \u00ce\u00a3 |\u00ce\u02dc_t(x) - \u00ce\u02dc_t(x')|\n",
    "        # S_k^fair = B_base - B_k\n",
    "        # ============================================================\n",
    "        print(\"\\n\u00f0\u0178\u201c\u0152 Phase 3: Fairness Scoring\")\n",
    "        \n",
    "        # Compute baseline bias: B_base on current global model \u00ce\u02dc_t\n",
    "        B_base = compute_bias(global_model, x_probes, x_prime_probes, device)\n",
    "        print(f\"  B_base (global model bias): {B_base:.6f}\")\n",
    "        \n",
    "        S_fair = []  # Fairness scores\n",
    "        S_acc = []   # Accuracy scores\n",
    "        \n",
    "        for client_id, update in enumerate(client_updates):\n",
    "            # Create hypothetical model: \u00ce\u02dc_test = \u00ce\u02dc_t + \u00ce\u201dW_k\n",
    "            hyp_model = copy.deepcopy(global_model)\n",
    "            hyp_sd = hyp_model.state_dict()\n",
    "            for k in hyp_sd:\n",
    "                hyp_sd[k] = hyp_sd[k] + update[k]\n",
    "            hyp_model.load_state_dict(hyp_sd)\n",
    "            \n",
    "            # Compute B_k\n",
    "            B_k = compute_bias(hyp_model, x_probes, x_prime_probes, device)\n",
    "            \n",
    "            # S_k^fair = B_base - B_k\n",
    "            # Positive = client reduces bias (good for fairness)\n",
    "            s_fair = B_base - B_k\n",
    "            S_fair.append(s_fair)\n",
    "            \n",
    "            # S_k^acc = L_val(\u00ce\u02dc_t) - L_val(\u00ce\u02dc_t + \u00ce\u201dW_k)\n",
    "            s_acc = compute_accuracy_score(global_model, update, val_loader, device)\n",
    "            S_acc.append(s_acc)\n",
    "            \n",
    "            print(f\"  Client {client_id}: S_fair={s_fair:+.6f}, S_acc={s_acc:+.6f}, B_k={B_k:.6f}\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # PHASE 4: Multi-Objective Aggregation (The Reward)\n",
    "        # \u00ce\u00b1_k = (1 - \u00ce\u00b3) \u00c2\u00b7 S_k^acc + \u00ce\u00b3 \u00c2\u00b7 S_k^fair\n",
    "        # \u00ce\u02dc_{t+1} = \u00ce\u02dc_t + \u00ce\u00b7_global \u00c2\u00b7 \u00ce\u00a3 (\u00ce\u00b1_k / \u00ce\u00a3\u00ce\u00b1_j) \u00c2\u00b7 \u00ce\u201dW_k\n",
    "        # ============================================================\n",
    "        print(\"\\n\u00f0\u0178\u201c\u0152 Phase 4: Multi-Objective Aggregation\")\n",
    "        \n",
    "        gamma = config['gamma']\n",
    "        \n",
    "        # Normalize scores to [0, 1] range\n",
    "        S_fair_norm = normalize_scores(S_fair)\n",
    "        S_acc_norm = normalize_scores(S_acc)\n",
    "        \n",
    "        # Compute \u00ce\u00b1_k = (1 - \u00ce\u00b3) \u00c2\u00b7 S_k^acc + \u00ce\u00b3 \u00c2\u00b7 S_k^fair\n",
    "        alphas = []\n",
    "        for i in range(config['n_clients']):\n",
    "            alpha_k = (1 - gamma) * S_acc_norm[i] + gamma * S_fair_norm[i]\n",
    "            alphas.append(alpha_k)\n",
    "        \n",
    "        # Normalize alphas: \u00ce\u00b1_k / \u00ce\u00a3\u00ce\u00b1_j\n",
    "        alpha_sum = sum(alphas)\n",
    "        if alpha_sum > 0:\n",
    "            alphas_normalized = [a / alpha_sum for a in alphas]\n",
    "        else:\n",
    "            alphas_normalized = [1.0 / config['n_clients']] * config['n_clients']\n",
    "        \n",
    "        print(f\"  \u00ce\u00b3 = {gamma}\")\n",
    "        print(f\"  S_fair (normalized): {[f'{s:.4f}' for s in S_fair_norm]}\")\n",
    "        print(f\"  S_acc (normalized):  {[f'{s:.4f}' for s in S_acc_norm]}\")\n",
    "        print(f\"  \u00ce\u00b1 (raw):            {[f'{a:.4f}' for a in alphas]}\")\n",
    "        print(f\"  \u00ce\u00b1 (normalized):     {[f'{a:.4f}' for a in alphas_normalized]}\")\n",
    "        \n",
    "        # Apply weighted aggregation: \u00ce\u02dc_{t+1} = \u00ce\u02dc_t + \u00ce\u00b7_global \u00c2\u00b7 \u00ce\u00a3 \u00ce\u00b1_k \u00c2\u00b7 \u00ce\u201dW_k\n",
    "        global_sd = global_model.state_dict()\n",
    "        for k in global_sd:\n",
    "            weighted_update = sum(\n",
    "                alphas_normalized[i] * client_updates[i][k] \n",
    "                for i in range(config['n_clients'])\n",
    "            )\n",
    "            global_sd[k] = global_sd[k] + config['eta_global'] * weighted_update\n",
    "        global_model.load_state_dict(global_sd)\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy = evaluate_model(global_model, test_loader, device)\n",
    "        \n",
    "        # Log metrics\n",
    "        history['accuracy'].append(accuracy)\n",
    "        history['bias'].append(B_base)\n",
    "        history['avg_S_fair'].append(np.mean(S_fair))\n",
    "        history['avg_S_acc'].append(np.mean(S_acc))\n",
    "        history['avg_alpha'].append(np.mean(alphas_normalized))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': round_num + 1,\n",
    "            'accuracy': accuracy,\n",
    "            'bias': B_base,\n",
    "            'avg_S_fair': np.mean(S_fair),\n",
    "            'avg_S_acc': np.mean(S_acc),\n",
    "            'alpha_max': max(alphas_normalized),\n",
    "            'alpha_min': min(alphas_normalized),\n",
    "            'alpha_std': np.std(alphas_normalized)\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n  \u00e2\u0153\u2026 Round {round_num + 1} Complete\")\n",
    "        print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "        print(f\"  Bias: {B_base:.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"Training Complete!\")\n",
    "    print(f\"Final Accuracy: {history['accuracy'][-1]:.2f}%\")\n",
    "    print(f\"Final Bias: {history['bias'][-1]:.6f}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    return history, global_model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ENTRY POINT\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Fed-AuditGAN Real 4-Phase')\n",
    "    parser.add_argument('--gamma', type=float, default=0.5, help='Trade-off parameter (0=FedAvg, 1=Fairness)')\n",
    "    parser.add_argument('--n_rounds', type=int, default=10, help='Number of FL rounds')\n",
    "    parser.add_argument('--n_clients', type=int, default=5, help='Number of clients')\n",
    "    parser.add_argument('--local_epochs', type=int, default=3, help='Local training epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, help='Client learning rate')\n",
    "    parser.add_argument('--eta_global', type=float, default=1.0, help='Global learning rate')\n",
    "    parser.add_argument('--gan_epochs', type=int, default=20, help='GAN training epochs')\n",
    "    parser.add_argument('--n_probes', type=int, default=300, help='Number of fairness probes')\n",
    "    parser.add_argument('--gan_alpha', type=float, default=1.0, help='GAN realism weight')\n",
    "    parser.add_argument('--gan_beta', type=float, default=1.0, help='GAN bias weight')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    config = {\n",
    "        'gamma': args.gamma,\n",
    "        'n_rounds': args.n_rounds,\n",
    "        'n_clients': args.n_clients,\n",
    "        'local_epochs': args.local_epochs,\n",
    "        'lr': args.lr,\n",
    "        'eta_global': args.eta_global,\n",
    "        'gan_epochs': args.gan_epochs,\n",
    "        'n_probes': args.n_probes,\n",
    "        'gan_alpha': args.gan_alpha,\n",
    "        'gan_beta': args.gan_beta,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    }\n",
    "    \n",
    "    run_fed_audit_gan(config)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run with gamma = 0.3, 0.5, 0.7\n",
    "import torch\n",
    "results = {}\n",
    "for gamma in [0.3, 0.5, 0.7]:\n",
    "    config = {\n",
    "        \"gamma\": gamma,\n",
    "        \"n_rounds\": 10,\n",
    "        \"n_clients\": 5,\n",
    "        \"local_epochs\": 3,\n",
    "        \"lr\": 0.01,\n",
    "        \"eta_global\": 1.0,\n",
    "        \"gan_epochs\": 20,\n",
    "        \"n_probes\": 300,\n",
    "        \"gan_alpha\": 1.0,\n",
    "        \"gan_beta\": 1.0,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    }\n",
    "    history, model = run_fed_audit_gan(config)\n",
    "    results[gamma] = history\n",
    "print(\"All experiments complete!\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "colors = {0.3: \"blue\", 0.5: \"green\", 0.7: \"red\"}\n",
    "rounds = list(range(1, 11))\n",
    "\n",
    "for g in [0.3, 0.5, 0.7]:\n",
    "    axes[0].plot(rounds, results[g][\"accuracy\"], \"o-\", label=f\"gamma={g}\", color=colors[g])\n",
    "    axes[1].plot(rounds, results[g][\"bias\"], \"o-\", label=f\"gamma={g}\", color=colors[g])\n",
    "\n",
    "axes[0].set_xlabel(\"Round\"); axes[0].set_ylabel(\"Accuracy (%)\")\n",
    "axes[0].set_title(\"Accuracy\"); axes[0].legend(); axes[0].grid(True)\n",
    "axes[1].set_xlabel(\"Round\"); axes[1].set_ylabel(\"Bias\")\n",
    "axes[1].set_title(\"Bias\"); axes[1].legend(); axes[1].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary:\")\n",
    "for g in [0.3, 0.5, 0.7]:\n",
    "    print(f\"gamma={g}: Acc={results[g]['accuracy'][-1]:.2f}%, Bias={results[g]['bias'][-1]:.4f}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}