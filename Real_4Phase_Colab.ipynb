{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Fed-AuditGAN: Real 4-Phase Implementation\n\nKey Formula: `alpha_k = (1 - gamma) * S_acc + gamma * S_fair`\n\n- gamma=0.3: More accuracy\n- gamma=0.5: Balanced\n- gamma=0.7: More fairness\n\n**Config: 30 rounds, 10 clients**"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!pip install wandb torch torchvision tqdm matplotlib -q\nprint(\"Done!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import wandb\nwandb.login()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\nimport numpy as np\nimport copy\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {DEVICE}\")\n\nclass CNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.dropout2(x)\n        return self.fc2(x)\n\nclass FairnessGenerator(nn.Module):\n    def __init__(self, latent_dim=100, num_classes=10, img_shape=(1, 28, 28)):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.num_classes = num_classes\n        self.img_shape = img_shape\n        self.label_emb = nn.Embedding(num_classes, latent_dim)\n        self.init_size = img_shape[1] // 4\n        self.l1 = nn.Linear(latent_dim * 2, 128 * self.init_size ** 2)\n        self.conv_blocks = nn.Sequential(\n            nn.BatchNorm2d(128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, 3, 1, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 64, 3, 1, 1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, img_shape[0], 3, 1, 1),\n            nn.Tanh()\n        )\n        self.delta_net = nn.Sequential(\n            nn.Linear(latent_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, int(np.prod(img_shape))),\n            nn.Tanh()\n        )\n        self.delta_scale = 0.1\n\n    def forward(self, z, labels):\n        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n        out = self.l1(gen_input)\n        out = out.view(-1, 128, self.init_size, self.init_size)\n        x = self.conv_blocks(out)\n        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n        x_prime = torch.clamp(x + delta, -1, 1)\n        return x, x_prime\n\nclass Discriminator(nn.Module):\n    def __init__(self, num_classes=10, img_shape=(1, 28, 28)):\n        super().__init__()\n        self.num_classes = num_classes\n        self.img_shape = img_shape\n        self.label_emb = nn.Embedding(num_classes, num_classes)\n        self.conv = nn.Sequential(\n            nn.Conv2d(img_shape[0] + num_classes, 16, 3, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(16, 32, 3, 2, 1),\n            nn.BatchNorm2d(32),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(32, 64, 3, 2, 1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, 3, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2)\n        )\n        self.fc = nn.Sequential(nn.Linear(128 * 4, 1), nn.Sigmoid())\n\n    def forward(self, img, labels):\n        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n        out = self.conv(torch.cat([img, label_map], dim=1))\n        return self.fc(out.view(out.size(0), -1))\n\ndef train_fairness_gan(G, D, model, loader, epochs=20, alpha=1.0, beta=1.0):\n    G, D, model = G.to(DEVICE), D.to(DEVICE), model.to(DEVICE)\n    model.eval()\n    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    bce = nn.BCELoss()\n    for epoch in range(epochs):\n        for imgs, labels in loader:\n            batch_size = imgs.size(0)\n            real_t = torch.ones(batch_size, 1, device=DEVICE)\n            fake_t = torch.zeros(batch_size, 1, device=DEVICE)\n            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n            z = torch.randn(batch_size, G.latent_dim, device=DEVICE)\n            gen_labels = torch.randint(0, G.num_classes, (batch_size,), device=DEVICE)\n            x, x_prime = G(z, gen_labels)\n            with torch.no_grad():\n                pred_x, pred_xp = model(x), model(x_prime)\n            pred_diff = -beta * torch.mean((pred_x - pred_xp) ** 2)\n            realism = alpha * torch.mean((x - x_prime) ** 2)\n            gan_loss = (bce(D(x, gen_labels), real_t) + bce(D(x_prime, gen_labels), real_t)) / 2\n            g_loss = pred_diff + realism + gan_loss\n            opt_G.zero_grad()\n            g_loss.backward()\n            opt_G.step()\n            x, x_prime = G(z, gen_labels)\n            d_real = bce(D(imgs, labels), real_t)\n            d_fake = (bce(D(x.detach(), gen_labels), fake_t) + bce(D(x_prime.detach(), gen_labels), fake_t)) / 2\n            d_loss = (d_real + d_fake) / 2\n            opt_D.zero_grad()\n            d_loss.backward()\n            opt_D.step()\n    return G, D\n\ndef compute_bias(model, x, x_prime):\n    model.eval()\n    with torch.no_grad():\n        pred_x = model(x.to(DEVICE))\n        pred_xp = model(x_prime.to(DEVICE))\n        return torch.abs(pred_x - pred_xp).sum(dim=1).mean().item()\n\ndef compute_acc_score(model, update, val_loader):\n    model.eval()\n    loss_before = 0\n    count = 0\n    with torch.no_grad():\n        for d, t in val_loader:\n            d, t = d.to(DEVICE), t.to(DEVICE)\n            loss_before += F.cross_entropy(model(d), t, reduction='sum').item()\n            count += len(t)\n    loss_before /= count\n    hyp = copy.deepcopy(model)\n    sd = hyp.state_dict()\n    for k in sd:\n        sd[k] = sd[k] + update[k]\n    hyp.load_state_dict(sd)\n    hyp.eval()\n    loss_after = 0\n    count = 0\n    with torch.no_grad():\n        for d, t in val_loader:\n            d, t = d.to(DEVICE), t.to(DEVICE)\n            loss_after += F.cross_entropy(hyp(d), t, reduction='sum').item()\n            count += len(t)\n    loss_after /= count\n    return loss_before - loss_after\n\ndef partition_non_iid(dataset, n):\n    idx = np.argsort([dataset[i][1] for i in range(len(dataset))])\n    shards = np.array_split(idx, n * 2)\n    np.random.shuffle(shards)\n    return [np.concatenate([shards[2*i], shards[2*i+1]]) for i in range(n)]\n\ndef evaluate(model, loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for d, t in loader:\n            d, t = d.to(DEVICE), t.to(DEVICE)\n            correct += (model(d).argmax(1) == t).sum().item()\n            total += len(t)\n    return 100 * correct / total\n\ndef normalize(scores):\n    s = np.array(scores)\n    if s.max() - s.min() < 1e-8:\n        return np.ones_like(s) / len(s)\n    return (s - s.min()) / (s.max() - s.min())\n\nprint(\"Implementation loaded!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def run_fed_audit_gan(gamma, n_rounds=30, n_clients=10, local_epochs=3, lr=0.01, gan_epochs=20, n_probes=300):\n    wandb.init(project=\"fed-audit-gan-real\", name=f\"gamma{gamma}_r{n_rounds}_c{n_clients}\", \n               config={'gamma': gamma, 'n_rounds': n_rounds, 'n_clients': n_clients})\n    \n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n    train_data = datasets.MNIST('./data', train=True, download=True, transform=transform)\n    test_data = datasets.MNIST('./data', train=False, download=True, transform=transform)\n    \n    client_idx = partition_non_iid(train_data, n_clients)\n    print(f\"Data partitioned: {n_clients} clients with non-IID distribution\")\n    \n    test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n    val_idx = np.random.choice(len(train_data), 1000, replace=False)\n    val_loader = DataLoader(Subset(train_data, val_idx), batch_size=32, shuffle=False)\n    \n    global_model = CNN().to(DEVICE)\n    history = {'accuracy': [], 'bias': [], 'alphas': []}\n    \n    print(f\"Fed-AuditGAN | gamma={gamma} | {n_rounds} rounds | {n_clients} clients\")\n    print(f\"Formula: alpha_k = (1-gamma)*S_acc + gamma*S_fair\")\n    \n    for rnd in tqdm(range(n_rounds), desc=f\"gamma={gamma}\"):\n        updates = []\n        for cid in range(n_clients):\n            loader = DataLoader(Subset(train_data, client_idx[cid]), batch_size=32, shuffle=True)\n            local = copy.deepcopy(global_model)\n            before = copy.deepcopy(global_model.state_dict())\n            opt = optim.SGD(local.parameters(), lr=lr)\n            local.train()\n            for _ in range(local_epochs):\n                for d, t in loader:\n                    d, t = d.to(DEVICE), t.to(DEVICE)\n                    opt.zero_grad()\n                    F.cross_entropy(local(d), t).backward()\n                    opt.step()\n            updates.append({k: local.state_dict()[k] - before[k] for k in before})\n        \n        G, D = FairnessGenerator().to(DEVICE), Discriminator().to(DEVICE)\n        G, D = train_fairness_gan(G, D, global_model, val_loader, epochs=gan_epochs)\n        G.eval()\n        with torch.no_grad():\n            z = torch.randn(n_probes, G.latent_dim, device=DEVICE)\n            lbls = torch.randint(0, 10, (n_probes,), device=DEVICE)\n            x_p, xp_p = G(z, lbls)\n        \n        B_base = compute_bias(global_model, x_p, xp_p)\n        S_fair, S_acc = [], []\n        for upd in updates:\n            hyp = copy.deepcopy(global_model)\n            sd = hyp.state_dict()\n            for k in sd:\n                sd[k] = sd[k] + upd[k]\n            hyp.load_state_dict(sd)\n            S_fair.append(B_base - compute_bias(hyp, x_p, xp_p))\n            S_acc.append(compute_acc_score(global_model, upd, val_loader))\n        \n        S_fair_n, S_acc_n = normalize(S_fair), normalize(S_acc)\n        alphas = [(1-gamma)*S_acc_n[i] + gamma*S_fair_n[i] for i in range(n_clients)]\n        a_sum = sum(alphas)\n        alphas = [a/a_sum if a_sum > 0 else 1/n_clients for a in alphas]\n        \n        sd = global_model.state_dict()\n        for k in sd:\n            sd[k] = sd[k] + sum(alphas[i] * updates[i][k] for i in range(n_clients))\n        global_model.load_state_dict(sd)\n        \n        acc = evaluate(global_model, test_loader)\n        history['accuracy'].append(acc)\n        history['bias'].append(B_base)\n        history['alphas'].append(alphas)\n        \n        wandb.log({'round': rnd+1, 'accuracy': acc, 'bias': B_base, \n                   'alpha_max': max(alphas), 'alpha_min': min(alphas), 'alpha_std': np.std(alphas)})\n    \n    print(f\"gamma={gamma} Done | Accuracy: {history['accuracy'][-1]:.2f}%\")\n    wandb.finish()\n    return history\n\nprint(\"Training function ready!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Run experiments with gamma = 0.3, 0.5, 0.7\nresults = {}\nfor gamma in [0.3, 0.5, 0.7]:\n    print(f\"Running gamma = {gamma}\")\n    results[gamma] = run_fed_audit_gan(gamma=gamma, n_rounds=30, n_clients=10)\n\nprint(\"All experiments complete!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = {0.3: 'blue', 0.5: 'green', 0.7: 'red'}\nrounds = list(range(1, 31))\n\nfor g in [0.3, 0.5, 0.7]:\n    axes[0].plot(rounds, results[g]['accuracy'], 'o-', label=f'gamma={g}', color=colors[g], markersize=3)\n    axes[1].plot(rounds, results[g]['bias'], 'o-', label=f'gamma={g}', color=colors[g], markersize=3)\n\naxes[0].set_xlabel('Round')\naxes[0].set_ylabel('Accuracy (%)')\naxes[0].set_title('Accuracy vs Round')\naxes[0].legend()\naxes[0].grid(True)\n\naxes[1].set_xlabel('Round')\naxes[1].set_ylabel('Bias')\naxes[1].set_title('Bias vs Round')\naxes[1].legend()\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.savefig('results.png', dpi=150)\nplt.show()\n\nprint(\"Summary:\")\nfor g in [0.3, 0.5, 0.7]:\n    print(f\"gamma={g}: Acc={results[g]['accuracy'][-1]:.2f}%, Bias={results[g]['bias'][-1]:.4f}\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}