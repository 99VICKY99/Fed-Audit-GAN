{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c091279f",
   "metadata": {},
   "source": [
    "# ðŸ”§ Fed-Audit-GAN v7.0 - CIFAR-10 (Simplified: No Gradient Clipping, No Warmup)\n",
    "\n",
    "## ðŸŽ¯ Experiments Run:\n",
    "\n",
    "### ðŸ”µ Baseline: FedAvg (Federated Averaging)\n",
    "- Standard data-weighted averaging (McMahan et al., 2017)\n",
    "- NO GAN, NO fairness scoring\n",
    "\n",
    "### ðŸŸ¢ Our Method: Fed-Audit-GAN \n",
    "- **Fed-Audit-GAN Î³ = 0.3** - Accuracy-weighted\n",
    "- **Fed-Audit-GAN Î³ = 0.7** - Fairness-weighted\n",
    "\n",
    "## ðŸ”§ Fed-Audit-GAN 4-Phase Architecture:\n",
    "1. **Phase 1**: Local Client Training (SGD - NO gradient clipping)\n",
    "2. **Phase 2**: GAN Training (EVERY round from round 1!)\n",
    "3. **Phase 3**: Fairness Scoring\n",
    "4. **Phase 4**: V2 Linear Aggregation (from round 1 - NO warmup!)\n",
    "\n",
    "## â­ KEY CHANGES FROM V6:\n",
    "- âŒ **REMOVED**: Gradient clipping\n",
    "- âŒ **REMOVED**: Warmup rounds - V2 Linear from round 1!\n",
    "- âŒ **REMOVED**: Soft labels - using hard labels (1.0, 0.0)\n",
    "- âœ… **KEPT**: GAN trains EVERY round\n",
    "- âœ… **KEPT**: V2 Linear Formula for aggregation\n",
    "\n",
    "## â­ V2 Linear Aggregation Formula:\n",
    "```\n",
    "Weight = (1 - Î³) Ã— Accuracy_norm + Î³ Ã— Fairness_norm\n",
    "```\n",
    "\n",
    "## â­ NON-IID:\n",
    "- Each client gets only 2-3 classes (extreme heterogeneity)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce8860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies\n",
    "!pip install wandb torch torchvision tqdm matplotlib numpy -q\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c125316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"âœ… WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd7d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Imports and GPU Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GPU Configuration with optimizations\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "# Enable cuDNN optimizations\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Using device: {DEVICE} | GPUs available: {NUM_GPUS}\")\n",
    "\n",
    "# ========== GPU UTILIZATION ==========\n",
    "NUM_WORKERS = 8\n",
    "PIN_MEMORY = True\n",
    "BATCH_SIZE = 1024 if NUM_GPUS > 1 else 512\n",
    "VAL_BATCH_SIZE = 2048 if NUM_GPUS > 1 else 1024\n",
    "PREFETCH_FACTOR = 4\n",
    "N_PROBES = 2000\n",
    "\n",
    "# Experiment settings\n",
    "N_ROUNDS = 50\n",
    "N_CLIENTS = 20\n",
    "GAN_EPOCHS = 20\n",
    "LOCAL_EPOCHS = 3\n",
    "\n",
    "# CIFAR-10 specific\n",
    "IMG_SHAPE = (3, 32, 32)\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Pre-load CIFAR-10 dataset\n",
    "print(\"Pre-loading CIFAR-10 dataset...\")\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "TRAIN_DATA = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "TEST_DATA = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Non-IID partition: each client gets only 2-3 classes\n",
    "def partition_non_iid_extreme(dataset, n_clients, classes_per_client=2, seed=42):\n",
    "    \"\"\"Extreme non-IID: each client gets only a few classes\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    class_indices = {c: np.where(labels == c)[0] for c in range(NUM_CLASSES)}\n",
    "    \n",
    "    client_data = []\n",
    "    classes_assigned = []\n",
    "    \n",
    "    for cid in range(n_clients):\n",
    "        n_classes = np.random.randint(classes_per_client, classes_per_client + 2)\n",
    "        client_classes = np.random.choice(NUM_CLASSES, n_classes, replace=False)\n",
    "        classes_assigned.append(client_classes.tolist())\n",
    "        \n",
    "        client_idx = []\n",
    "        for c in client_classes:\n",
    "            n_samples = len(class_indices[c]) // (n_clients // 2)\n",
    "            start = np.random.randint(0, max(1, len(class_indices[c]) - n_samples))\n",
    "            client_idx.extend(class_indices[c][start:start + n_samples])\n",
    "        \n",
    "        np.random.shuffle(client_idx)\n",
    "        client_data.append(np.array(client_idx))\n",
    "    \n",
    "    return client_data, classes_assigned\n",
    "\n",
    "CLIENT_IDX, CLIENT_CLASSES = partition_non_iid_extreme(TRAIN_DATA, N_CLIENTS)\n",
    "print(f\"Client data sizes: {[len(idx) for idx in CLIENT_IDX]}\")\n",
    "print(f\"Client classes: {CLIENT_CLASSES}\")\n",
    "\n",
    "# Pre-create DataLoaders\n",
    "TEST_LOADER = DataLoader(TEST_DATA, batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                         num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                         persistent_workers=True, prefetch_factor=PREFETCH_FACTOR)\n",
    "\n",
    "VAL_IDX = np.random.choice(len(TRAIN_DATA), 2000, replace=False)\n",
    "VAL_LOADER = DataLoader(Subset(TRAIN_DATA, VAL_IDX), batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                        persistent_workers=True, prefetch_factor=PREFETCH_FACTOR)\n",
    "\n",
    "# Client evaluation loaders\n",
    "CLIENT_EVAL_LOADERS = []\n",
    "for cid in range(N_CLIENTS):\n",
    "    loader = DataLoader(Subset(TRAIN_DATA, CLIENT_IDX[cid]), batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                       num_workers=4, pin_memory=PIN_MEMORY, persistent_workers=True)\n",
    "    CLIENT_EVAL_LOADERS.append(loader)\n",
    "\n",
    "# Client training loaders\n",
    "CLIENT_LOADERS = []\n",
    "for cid in range(N_CLIENTS):\n",
    "    loader = DataLoader(Subset(TRAIN_DATA, CLIENT_IDX[cid]), batch_size=BATCH_SIZE, shuffle=True,\n",
    "                       num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
    "                       persistent_workers=True, prefetch_factor=PREFETCH_FACTOR)\n",
    "    CLIENT_LOADERS.append(loader)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CIFAR-10 Fed-AuditGAN v7.0 - SIMPLIFIED\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"âŒ NO Gradient Clipping\")\n",
    "print(f\"âŒ NO Warmup Rounds\")\n",
    "print(f\"âŒ NO Soft Labels (using hard labels)\")\n",
    "print(f\"âœ… GAN every round from round 1\")\n",
    "print(f\"âœ… V2 Linear aggregation from round 1\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1305a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MODEL DEFINITIONS ==========\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"CNN for CIFAR-10 classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.conv3 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        self.conv4 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32->16\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16->8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8->4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 4->2\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim=128, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4  # 8\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 512 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 512, 3, 1, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input).view(-1, 512, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        return x, torch.clamp(x + delta, -1, 1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 128, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(512, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 1)\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "\n",
    "# ========== Helper Functions ==========\n",
    "def make_parallel(model):\n",
    "    if NUM_GPUS > 1:\n",
    "        return nn.DataParallel(model)\n",
    "    return model\n",
    "\n",
    "def get_base_model(model):\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        return model.module\n",
    "    return model\n",
    "\n",
    "print(\"âœ… Models defined: CNN, FairnessGenerator, Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad10b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== FAIRNESS METRICS ==========\n",
    "\n",
    "def compute_jfi(performances):\n",
    "    p = np.array(performances)\n",
    "    n = len(p)\n",
    "    if np.sum(p**2) == 0:\n",
    "        return 1.0\n",
    "    return (np.sum(p)**2) / (n * np.sum(p**2))\n",
    "\n",
    "def compute_max_min_fairness(performances):\n",
    "    p = np.array(performances)\n",
    "    if np.max(p) == 0:\n",
    "        return 0.0\n",
    "    return np.min(p) / np.max(p)\n",
    "\n",
    "def compute_fairness_variance(performances):\n",
    "    return np.var(np.array(performances))\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_client(model, client_loaders):\n",
    "    model.eval()\n",
    "    client_accuracies = []\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        for loader in client_loaders:\n",
    "            correct, total = 0, 0\n",
    "            for d, t in loader:\n",
    "                d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "                correct += (model(d).argmax(1) == t).sum().item()\n",
    "                total += len(t)\n",
    "            client_accuracies.append(100 * correct / total if total > 0 else 0)\n",
    "    return client_accuracies\n",
    "\n",
    "def compute_all_fairness_metrics(client_accuracies):\n",
    "    return {\n",
    "        'jfi': compute_jfi(client_accuracies),\n",
    "        'max_min_fairness': compute_max_min_fairness(client_accuracies),\n",
    "        'fairness_variance': compute_fairness_variance(client_accuracies),\n",
    "        'min_accuracy': np.min(client_accuracies),\n",
    "        'max_accuracy': np.max(client_accuracies),\n",
    "        'mean_accuracy': np.mean(client_accuracies)\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        for d, t in loader:\n",
    "            d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "            correct += (model(d).argmax(1) == t).sum().item()\n",
    "            total += len(t)\n",
    "    return 100 * correct / total\n",
    "\n",
    "def normalize(scores):\n",
    "    s = np.array(scores)\n",
    "    if s.max() - s.min() < 1e-8:\n",
    "        return np.ones_like(s) / len(s)\n",
    "    return (s - s.min()) / (s.max() - s.min())\n",
    "\n",
    "print(\"âœ… Fairness metrics defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c27667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== GAN TRAINING () ==========\n",
    "\n",
    "def train_fairness_gan(G, D, model, loader, epochs=15, alpha=1.0, beta=1.0):\n",
    "    \"\"\"\n",
    "    Train GAN with HARD LABELS (1.0 and 0.0) - NO soft labels!\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    G = make_parallel(G)\n",
    "    D = make_parallel(D)\n",
    "    \n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    scaler = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    latent_dim = get_base_model(G).latent_dim\n",
    "    num_classes = get_base_model(G).num_classes\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            batch_size = imgs.size(0)\n",
    "            \n",
    "            # âŒ NO SOFT LABELS - Using HARD labels!\n",
    "            real_t = torch.ones(batch_size, 1, device=DEVICE)   # Hard 1.0\n",
    "            fake_t = torch.zeros(batch_size, 1, device=DEVICE)  # Hard 0.0\n",
    "            \n",
    "            imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "            labels = labels.to(DEVICE, non_blocking=True)\n",
    "            z = torch.randn(batch_size, latent_dim, device=DEVICE)\n",
    "            gen_labels = torch.randint(0, num_classes, (batch_size,), device=DEVICE)\n",
    "            \n",
    "            # Generator\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                x, x_prime = G(z, gen_labels)\n",
    "                with torch.no_grad():\n",
    "                    pred_x, pred_xp = model(x), model(x_prime)\n",
    "                pred_diff = -beta * torch.mean((pred_x - pred_xp) ** 2)\n",
    "                realism = alpha * torch.mean((x - x_prime) ** 2)\n",
    "                gan_loss = (bce(D(x, gen_labels), real_t) + bce(D(x_prime, gen_labels), real_t)) / 2\n",
    "                g_loss = pred_diff + realism + gan_loss\n",
    "            scaler.scale(g_loss).backward()\n",
    "            scaler.step(opt_G)\n",
    "            \n",
    "            # Discriminator\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                x, x_prime = G(z, gen_labels)\n",
    "                d_real = bce(D(imgs, labels), real_t)\n",
    "                d_fake = (bce(D(x.detach(), gen_labels), fake_t) + bce(D(x_prime.detach(), gen_labels), fake_t)) / 2\n",
    "                d_loss = (d_real + d_fake) / 2\n",
    "            scaler.scale(d_loss).backward()\n",
    "            scaler.step(opt_D)\n",
    "            scaler.update()\n",
    "    \n",
    "    return get_base_model(G), get_base_model(D)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bias(model, x, x_prime):\n",
    "    model.eval()\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        pred_x = model(x)\n",
    "        pred_xp = model(x_prime)\n",
    "    return torch.abs(pred_x - pred_xp).sum(dim=1).mean().item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_acc_score(model, update, val_loader):\n",
    "    model.eval()\n",
    "    loss_before, count = 0.0, 0\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        for d, t in val_loader:\n",
    "            d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "            loss_before += F.cross_entropy(model(d), t, reduction='sum').item()\n",
    "            count += len(t)\n",
    "    loss_before /= count\n",
    "    \n",
    "    base_model = get_base_model(model)\n",
    "    hyp = copy.deepcopy(base_model)\n",
    "    sd = hyp.state_dict()\n",
    "    \n",
    "    for k in sd:\n",
    "        update_key = k\n",
    "        if k not in update:\n",
    "            update_key = k.replace('module.', '') if k.startswith('module.') else 'module.' + k\n",
    "        if update_key in update:\n",
    "            sd[k] = sd[k] + update[update_key]\n",
    "        elif k in update:\n",
    "            sd[k] = sd[k] + update[k]\n",
    "    \n",
    "    hyp.load_state_dict(sd)\n",
    "    hyp = make_parallel(hyp.to(DEVICE))\n",
    "    hyp.eval()\n",
    "    \n",
    "    loss_after = 0.0\n",
    "    with torch.amp.autocast(device_type='cuda'):\n",
    "        for d, t in val_loader:\n",
    "            d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "            loss_after += F.cross_entropy(hyp(d), t, reduction='sum').item()\n",
    "    return loss_before - loss_after / count\n",
    "\n",
    "\n",
    "print(\"âœ… GAN training functions defined (NO soft labels!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a557c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TRAINING FUNCTIONS ==========\n",
    "\n",
    "def run_fedavg(n_rounds=N_ROUNDS, n_clients=N_CLIENTS, local_epochs=LOCAL_EPOCHS, lr=0.01):\n",
    "    \"\"\"Standard FedAvg baseline\"\"\"\n",
    "    wandb.init(project=\"FED_AUDIT_GAN_TEST_7_CIFAR10\", name=\"FedAvg_v7\", \n",
    "               config={'method': 'FedAvg', 'n_rounds': n_rounds, 'n_clients': n_clients})\n",
    "    \n",
    "    global_model = make_parallel(CNN().to(DEVICE))\n",
    "    scaler = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    history = {'accuracy': [], 'jfi': [], 'max_min_fairness': [], 'fairness_variance': [],\n",
    "               'min_accuracy': [], 'max_accuracy': [], 'client_accuracies': [], 'weights': []}\n",
    "    \n",
    "    print(f\"FedAvg | Rounds: {n_rounds} | Clients: {n_clients} | GPUs: {NUM_GPUS}\")\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=\"FedAvg\"):\n",
    "        updates = []\n",
    "        client_sizes = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local = make_parallel(copy.deepcopy(get_base_model(global_model)).to(DEVICE))\n",
    "            before = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "            opt = optim.SGD(local.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "            local.train()\n",
    "            \n",
    "            # âŒ NO GRADIENT CLIPPING\n",
    "            for _ in range(local_epochs):\n",
    "                for d, t in CLIENT_LOADERS[cid]:\n",
    "                    d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        loss = F.cross_entropy(local(d), t)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "            \n",
    "            after = local.state_dict()\n",
    "            updates.append({k: after[k] - before[k] for k in before})\n",
    "            client_sizes.append(len(CLIENT_IDX[cid]))\n",
    "            del local\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # FedAvg aggregation\n",
    "        total_size = sum(client_sizes)\n",
    "        alphas = [size / total_size for size in client_sizes]\n",
    "        \n",
    "        sd = global_model.state_dict()\n",
    "        for k in sd:\n",
    "            sd[k] = sd[k] + sum(alphas[i] * updates[i][k] for i in range(n_clients))\n",
    "        global_model.load_state_dict(sd)\n",
    "        \n",
    "        # Evaluate\n",
    "        acc = evaluate(global_model, TEST_LOADER)\n",
    "        client_accs = evaluate_per_client(global_model, CLIENT_EVAL_LOADERS)\n",
    "        fairness = compute_all_fairness_metrics(client_accs)\n",
    "        \n",
    "        history['accuracy'].append(acc)\n",
    "        history['jfi'].append(fairness['jfi'])\n",
    "        history['max_min_fairness'].append(fairness['max_min_fairness'])\n",
    "        history['fairness_variance'].append(fairness['fairness_variance'])\n",
    "        history['min_accuracy'].append(fairness['min_accuracy'])\n",
    "        history['max_accuracy'].append(fairness['max_accuracy'])\n",
    "        history['client_accuracies'].append(client_accs)\n",
    "        history['weights'].append(alphas)\n",
    "        \n",
    "        wandb.log({'round': rnd+1, 'accuracy': acc, 'jfi': fairness['jfi'],\n",
    "                   'max_min_fairness': fairness['max_min_fairness'],\n",
    "                   'fairness_variance': fairness['fairness_variance'],\n",
    "                   'min_client_acc': fairness['min_accuracy'],\n",
    "                   'max_client_acc': fairness['max_accuracy']})\n",
    "    \n",
    "    print(f\"FedAvg Done | Accuracy: {history['accuracy'][-1]:.2f}% | JFI: {history['jfi'][-1]:.4f}\")\n",
    "    wandb.finish()\n",
    "    return history\n",
    "\n",
    "\n",
    "def run_fed_audit_gan(gamma, n_rounds=N_ROUNDS, n_clients=N_CLIENTS, local_epochs=LOCAL_EPOCHS, \n",
    "                      lr=0.01, gan_epochs=GAN_EPOCHS, n_probes=N_PROBES):\n",
    "    \"\"\"\n",
    "    Fed-AuditGAN v7.0 - Simplified\n",
    "    âŒ NO gradient clipping\n",
    "    âŒ NO warmup rounds\n",
    "    âŒ NO soft labels\n",
    "    âœ… V2 Linear from round 1\n",
    "    \"\"\"\n",
    "    wandb.init(project=\"FED_AUDIT_GAN_TEST_7_CIFAR10\", name=f\"FedAuditGAN_v7_gamma{gamma}\",\n",
    "               config={'method': 'FedAuditGAN_v7', 'gamma': gamma, 'n_rounds': n_rounds, \n",
    "                       'n_clients': n_clients, 'gradient_clipping': False, 'warmup': False, 'soft_labels': False})\n",
    "    \n",
    "    global_model = make_parallel(CNN().to(DEVICE))\n",
    "    scaler = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    history = {'accuracy': [], 'bias': [], 'jfi': [], 'max_min_fairness': [], 'fairness_variance': [],\n",
    "               'min_accuracy': [], 'max_accuracy': [], 'client_accuracies': [], 'weights': []}\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Fed-AuditGAN v7.0 | gamma={gamma}\")\n",
    "    print(f\"âŒ NO Gradient Clipping | âŒ NO Warmup | âŒ NO Soft Labels\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=f\"gamma={gamma}\"):\n",
    "        # ================================================================\n",
    "        # PHASE 1: Client training (NO gradient clipping!)\n",
    "        # ================================================================\n",
    "        updates = []\n",
    "        for cid in range(n_clients):\n",
    "            local = make_parallel(copy.deepcopy(get_base_model(global_model)).to(DEVICE))\n",
    "            before = {k: v.clone() for k, v in global_model.state_dict().items()}\n",
    "            opt = optim.SGD(local.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "            local.train()\n",
    "            \n",
    "            for _ in range(local_epochs):\n",
    "                for d, t in CLIENT_LOADERS[cid]:\n",
    "                    d, t = d.to(DEVICE, non_blocking=True), t.to(DEVICE, non_blocking=True)\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        loss = F.cross_entropy(local(d), t)\n",
    "                    scaler.scale(loss).backward()\n",
    "                    # âŒ NO GRADIENT CLIPPING!\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "            \n",
    "            after = local.state_dict()\n",
    "            updates.append({k: after[k] - before[k] for k in before})\n",
    "            del local\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 2: GAN auditing (NO warmup - train from round 1!)\n",
    "        # ================================================================\n",
    "        G = FairnessGenerator(img_shape=IMG_SHAPE).to(DEVICE)\n",
    "        D = Discriminator(img_shape=IMG_SHAPE).to(DEVICE)\n",
    "        G, D = train_fairness_gan(G, D, global_model, VAL_LOADER, epochs=gan_epochs)\n",
    "        G.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_probes, G.latent_dim, device=DEVICE)\n",
    "            lbls = torch.randint(0, NUM_CLASSES, (n_probes,), device=DEVICE)\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                G_parallel = make_parallel(G)\n",
    "                x_p, xp_p = G_parallel(z, lbls)\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 3: Scoring\n",
    "        # ================================================================\n",
    "        B_base = compute_bias(global_model, x_p, xp_p)\n",
    "        S_fair, S_acc = [], []\n",
    "        for upd in updates:\n",
    "            hyp = make_parallel(copy.deepcopy(get_base_model(global_model)).to(DEVICE))\n",
    "            sd = hyp.state_dict()\n",
    "            for k in sd:\n",
    "                if k in upd:\n",
    "                    sd[k] = sd[k] + upd[k]\n",
    "            hyp.load_state_dict(sd)\n",
    "            S_fair.append(B_base - compute_bias(hyp, x_p, xp_p))\n",
    "            S_acc.append(compute_acc_score(global_model, upd, VAL_LOADER))\n",
    "            del hyp\n",
    "        \n",
    "        del G, D, x_p, xp_p\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 4: V2 Linear Aggregation (NO warmup - from round 1!)\n",
    "        # ================================================================\n",
    "        S_fair_n, S_acc_n = normalize(S_fair), normalize(S_acc)\n",
    "        alphas = [(1-gamma)*S_acc_n[i] + gamma*S_fair_n[i] for i in range(n_clients)]\n",
    "        a_sum = sum(alphas)\n",
    "        alphas = [a/a_sum if a_sum > 0 else 1/n_clients for a in alphas]\n",
    "        \n",
    "        sd = global_model.state_dict()\n",
    "        for k in sd:\n",
    "            sd[k] = sd[k] + sum(alphas[i] * updates[i][k] for i in range(n_clients))\n",
    "        global_model.load_state_dict(sd)\n",
    "        \n",
    "        # Evaluate\n",
    "        acc = evaluate(global_model, TEST_LOADER)\n",
    "        client_accs = evaluate_per_client(global_model, CLIENT_EVAL_LOADERS)\n",
    "        fairness = compute_all_fairness_metrics(client_accs)\n",
    "        \n",
    "        history['accuracy'].append(acc)\n",
    "        history['bias'].append(B_base)\n",
    "        history['jfi'].append(fairness['jfi'])\n",
    "        history['max_min_fairness'].append(fairness['max_min_fairness'])\n",
    "        history['fairness_variance'].append(fairness['fairness_variance'])\n",
    "        history['min_accuracy'].append(fairness['min_accuracy'])\n",
    "        history['max_accuracy'].append(fairness['max_accuracy'])\n",
    "        history['client_accuracies'].append(client_accs)\n",
    "        history['weights'].append(alphas)\n",
    "        \n",
    "        wandb.log({'round': rnd+1, 'accuracy': acc, 'bias': B_base, 'jfi': fairness['jfi'],\n",
    "                   'max_min_fairness': fairness['max_min_fairness'],\n",
    "                   'fairness_variance': fairness['fairness_variance'],\n",
    "                   'min_client_acc': fairness['min_accuracy'],\n",
    "                   'max_client_acc': fairness['max_accuracy']})\n",
    "    \n",
    "    print(f\"gamma={gamma} Done | Accuracy: {history['accuracy'][-1]:.2f}% | JFI: {history['jfi'][-1]:.4f}\")\n",
    "    wandb.finish()\n",
    "    return history\n",
    "\n",
    "print(\"âœ… Training functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== RUN ALL EXPERIMENTS ==========\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1. Run FedAvg baseline\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Running FedAvg Baseline\")\n",
    "print(\"=\"*60)\n",
    "results['FedAvg'] = run_fedavg(n_rounds=N_ROUNDS, n_clients=N_CLIENTS)\n",
    "\n",
    "# 2. Run Fed-AuditGAN with gamma = 0.3 and 0.7\n",
    "for gamma in [0.3, 0.7]:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Running Fed-AuditGAN v7.0 gamma = {gamma}\")\n",
    "    print(\"=\"*60)\n",
    "    results[f'gamma{gamma}'] = run_fed_audit_gan(gamma=gamma, n_rounds=N_ROUNDS, n_clients=N_CLIENTS)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… All experiments complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c97866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== VISUALIZATION ==========\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "colors = {'FedAvg': 'black', 'gamma0.3': 'blue', 'gamma0.7': 'red'}\n",
    "linestyles = {'FedAvg': '--', 'gamma0.3': '-', 'gamma0.7': '-'}\n",
    "rounds = list(range(1, N_ROUNDS + 1))\n",
    "\n",
    "# Plot 1: Global Accuracy\n",
    "for method in results:\n",
    "    axes[0, 0].plot(rounds, results[method]['accuracy'], linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[0, 0].set_xlabel('Round'); axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "axes[0, 0].set_title('CIFAR-10 v7: Global Test Accuracy'); axes[0, 0].legend(); axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: JFI\n",
    "for method in results:\n",
    "    axes[0, 1].plot(rounds, results[method]['jfi'], linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[0, 1].set_xlabel('Round'); axes[0, 1].set_ylabel('JFI')\n",
    "axes[0, 1].set_title(\"Jain's Fairness Index (higher=fairer)\"); axes[0, 1].legend(); axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Max-Min Fairness\n",
    "for method in results:\n",
    "    axes[0, 2].plot(rounds, results[method]['max_min_fairness'], linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[0, 2].set_xlabel('Round'); axes[0, 2].set_ylabel('Min/Max Ratio')\n",
    "axes[0, 2].set_title('Max-Min Fairness (higher=fairer)'); axes[0, 2].legend(); axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Variance\n",
    "for method in results:\n",
    "    axes[1, 0].plot(rounds, results[method]['fairness_variance'], linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[1, 0].set_xlabel('Round'); axes[1, 0].set_ylabel('Variance')\n",
    "axes[1, 0].set_title('Per-Client Accuracy Variance (lower=fairer)'); axes[1, 0].legend(); axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Accuracy Gap\n",
    "for method in results:\n",
    "    gap = [results[method]['max_accuracy'][i] - results[method]['min_accuracy'][i] for i in range(len(rounds))]\n",
    "    axes[1, 1].plot(rounds, gap, linestyle=linestyles[method],\n",
    "                    label=method, color=colors[method], linewidth=2)\n",
    "axes[1, 1].set_xlabel('Round'); axes[1, 1].set_ylabel('Gap (%)')\n",
    "axes[1, 1].set_title('Best-Worst Client Gap (lower=fairer)'); axes[1, 1].legend(); axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final Per-Client Accuracy\n",
    "final_client_accs = {m: results[m]['client_accuracies'][-1] for m in results}\n",
    "x = np.arange(N_CLIENTS)\n",
    "width = 0.25\n",
    "for i, method in enumerate(results):\n",
    "    axes[1, 2].bar(x + i*width, final_client_accs[method], width, label=method, color=colors[method], alpha=0.8)\n",
    "axes[1, 2].set_xlabel('Client ID'); axes[1, 2].set_ylabel('Accuracy (%)')\n",
    "axes[1, 2].set_title(f'Per-Client Accuracy (Round {N_ROUNDS})'); axes[1, 2].legend(); axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cifar10_v7_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Summary Table\n",
    "print(\"\\n\" + \"=\"*110)\n",
    "print(\"CIFAR-10 v7.0 FINAL RESULTS SUMMARY (Round {})\".format(N_ROUNDS))\n",
    "print(\"=\"*110)\n",
    "print(f\"{'Method':<15} {'Global Acc':>10} {'JFI':>8} {'Max-Min':>10} {'Variance':>12} {'Min Acc':>10} {'Max Acc':>10} {'Gap':>8}\")\n",
    "print(\"-\"*110)\n",
    "for method in results:\n",
    "    acc = results[method]['accuracy'][-1]\n",
    "    jfi = results[method]['jfi'][-1]\n",
    "    mmf = results[method]['max_min_fairness'][-1]\n",
    "    var = results[method]['fairness_variance'][-1]\n",
    "    min_acc = results[method]['min_accuracy'][-1]\n",
    "    max_acc = results[method]['max_accuracy'][-1]\n",
    "    gap = max_acc - min_acc\n",
    "    print(f\"{method:<15} {acc:>10.2f}% {jfi:>8.4f} {mmf:>10.4f} {var:>12.2f} {min_acc:>10.2f}% {max_acc:>10.2f}% {gap:>8.2f}%\")\n",
    "print(\"=\"*110)\n",
    "\n",
    "# V7 Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V7.0 CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"âŒ Gradient Clipping: REMOVED\")\n",
    "print(\"âŒ Warmup Rounds: REMOVED (V2 Linear from round 1)\")\n",
    "print(\"âŒ Soft Labels: REMOVED (using hard labels 1.0/0.0)\")\n",
    "print(\"âœ… GAN: Trains every round\")\n",
    "print(\"âœ… Aggregation: V2 Linear Formula\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c7590",
   "metadata": {},
   "source": [
    "# ðŸ”§ Fed-Audit-GAN v7.0 - CIFAR-10 (No Gradient Clipping, No Warmup)\n",
    "\n",
    "## ðŸŽ¯ Experiments Run:\n",
    "\n",
    "### ðŸ”µ Baseline: FedAvg (Federated Averaging)\n",
    "- Standard data-weighted averaging (McMahan et al., 2017)\n",
    "- NO GAN, NO fairness scoring\n",
    "\n",
    "### ðŸŸ¢ Our Method: Fed-Audit-GAN (V2 Linear Formula)\n",
    "- **Fed-Audit-GAN Î³ = 0.3** - Accuracy-weighted\n",
    "- **Fed-Audit-GAN Î³ = 0.7** - Fairness-weighted\n",
    "\n",
    "## ðŸ”§ Fed-Audit-GAN 4-Phase Architecture:\n",
    "1. **Phase 1**: Local Client Training (SGD - NO gradient clipping)\n",
    "2. **Phase 2**: GAN Training (EVERY round from round 1!)\n",
    "3. **Phase 3**: Fairness Scoring (with EMA)\n",
    "4. **Phase 4**: V2 Linear Aggregation (from round 1 - NO warmup!)\n",
    "\n",
    "## â­ KEY CHANGES FROM V6:\n",
    "- âŒ **REMOVED**: Gradient clipping\n",
    "- âŒ **REMOVED**: Warmup rounds - V2 Linear from round 1!\n",
    "- âœ… **KEPT**: GAN trains EVERY round\n",
    "- âœ… **KEPT**: V2 Linear Formula for aggregation\n",
    "- âœ… **KEPT**: Soft labels for GAN stability\n",
    "- âœ… **KEPT**: EMA momentum for fairness scores\n",
    "\n",
    "## â­ V2 Linear Aggregation Formula:\n",
    "```\n",
    "Weight = (1 - Î³) Ã— Accuracy_norm + Î³ Ã— Fairness_norm\n",
    "```\n",
    "\n",
    "## â­ PATHOLOGICAL NON-IID:\n",
    "- Each client ONLY has 2 out of 10 classes\n",
    "- Clients have DIFFERENT sample sizes (Dirichlet distribution)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a8864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies\n",
    "!pip install -q torch torchvision tqdm matplotlib numpy wandb\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21afe1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"âœ… WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3246b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Imports and GPU Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# ðŸš€ GPU SETUP\n",
    "# ============================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    \n",
    "    # Enable cuDNN optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # Enable TF32 for faster matrix operations\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"âœ… GPU(s) detected: {NUM_GPUS}\")\n",
    "    for i in range(NUM_GPUS):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"   GPU {i}: {props.name}\")\n",
    "        print(f\"      Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    NUM_GPUS = 0\n",
    "    print(\"âš ï¸  No GPU detected. Using CPU.\")\n",
    "\n",
    "# Mixed Precision Training\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "if USE_AMP:\n",
    "    print(\"\\nâœ… Mixed Precision Training (AMP) enabled.\")\n",
    "\n",
    "print(f\"\\nðŸ“ Device: {DEVICE}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a63af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEFINITIONS (CIFAR-10)\n",
    "# ============================================================\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"CNN for CIFAR-10 classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32->16\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16->8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8->4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 4->2\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    \"\"\"Generator that produces paired samples (x, x') for fairness testing\"\"\"\n",
    "    def __init__(self, latent_dim=128, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4  # 8\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 512 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input).view(-1, 512, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        return x, torch.clamp(x + delta, -1, 1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Conditional Discriminator\"\"\"\n",
    "    def __init__(self, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 1)\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "\n",
    "print(\"âœ… Models defined: CNN, FairnessGenerator, Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8cad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def train_gan(G, D, model, loader, epochs=15, device='cuda', l1=1.0, l2=1.0):\n",
    "    \"\"\"\n",
    "    Train the Fairness GAN with Soft Labels for stability\n",
    "    \"\"\"\n",
    "    G, D, model = G.to(device), D.to(device), model.to(device)\n",
    "    model.eval()\n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    if USE_AMP:\n",
    "        scaler_G = torch.amp.GradScaler(device='cuda')\n",
    "        scaler_D = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            bs = imgs.size(0)\n",
    "            \n",
    "            # Soft Labels for stability\n",
    "            real_labels = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "            fake_labels = torch.empty(bs, 1, device=device).uniform_(0.0, 0.1)\n",
    "            \n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            z = torch.randn(bs, G.latent_dim, device=device)\n",
    "            gl = torch.randint(0, G.num_classes, (bs,), device=device)\n",
    "            \n",
    "            # Generator\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    with torch.no_grad():\n",
    "                        px, pxp = model(x), model(xp)\n",
    "                    t1 = -torch.mean((px - pxp) ** 2)\n",
    "                    t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                    g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                    t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                    g_loss = t1 + t2 + t3\n",
    "                scaler_G.scale(g_loss).backward()\n",
    "                scaler_G.step(opt_G)\n",
    "                scaler_G.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                with torch.no_grad():\n",
    "                    px, pxp = model(x), model(xp)\n",
    "                t1 = -torch.mean((px - pxp) ** 2)\n",
    "                t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                g_loss = t1 + t2 + t3\n",
    "                g_loss.backward()\n",
    "                opt_G.step()\n",
    "            \n",
    "            # Discriminator\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                              bce(D(x.detach(), gl), fake_labels) + \n",
    "                              bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                scaler_D.scale(d_loss).backward()\n",
    "                scaler_D.step(opt_D)\n",
    "                scaler_D.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                          bce(D(x.detach(), gl), fake_labels) + \n",
    "                          bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                d_loss.backward()\n",
    "                opt_D.step()\n",
    "    \n",
    "    return G, D\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bias(model, x, xp, device):\n",
    "    \"\"\"Compute bias as difference in model predictions between x and x'\"\"\"\n",
    "    model.eval()\n",
    "    x, xp = x.to(device), xp.to(device)\n",
    "    \n",
    "    if USE_AMP:\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    else:\n",
    "        diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    \n",
    "    return diff.item()\n",
    "\n",
    "\n",
    "def partition_data_pathological_non_iid(dataset, n_clients, classes_per_client=2):\n",
    "    \"\"\"Pathological Non-IID with unequal sample sizes\"\"\"\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    class_indices = {c: np.where(labels == c)[0] for c in range(n_classes)}\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "    \n",
    "    client_classes = []\n",
    "    for cid in range(n_clients):\n",
    "        start_class = (cid * classes_per_client) % n_classes\n",
    "        assigned = [(start_class + i) % n_classes for i in range(classes_per_client)]\n",
    "        client_classes.append(assigned)\n",
    "    \n",
    "    # Unequal sample sizes using Dirichlet\n",
    "    alpha = 0.5\n",
    "    client_proportions = np.random.dirichlet([alpha] * n_clients)\n",
    "    \n",
    "    client_indices = [[] for _ in range(n_clients)]\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        clients_with_class = [cid for cid in range(n_clients) if c in client_classes[cid]]\n",
    "        \n",
    "        if len(clients_with_class) > 0:\n",
    "            class_samples = class_indices[c]\n",
    "            total_for_class = len(class_samples)\n",
    "            \n",
    "            relevant_props = np.array([client_proportions[cid] for cid in clients_with_class])\n",
    "            relevant_props = relevant_props / relevant_props.sum()\n",
    "            \n",
    "            start_idx = 0\n",
    "            for i, cid in enumerate(clients_with_class):\n",
    "                if i == len(clients_with_class) - 1:\n",
    "                    end_idx = total_for_class\n",
    "                else:\n",
    "                    n_samples = int(total_for_class * relevant_props[i])\n",
    "                    end_idx = min(start_idx + n_samples, total_for_class)\n",
    "                \n",
    "                if start_idx < end_idx:\n",
    "                    client_indices[cid].extend(class_samples[start_idx:end_idx].tolist())\n",
    "                start_idx = end_idx\n",
    "    \n",
    "    result = []\n",
    "    for cid in range(n_clients):\n",
    "        if len(client_indices[cid]) > 0:\n",
    "            indices = np.array(client_indices[cid])\n",
    "            np.random.shuffle(indices)\n",
    "            result.append(indices)\n",
    "        else:\n",
    "            fallback_samples = np.random.choice(len(dataset), size=50, replace=False)\n",
    "            result.append(fallback_samples)\n",
    "    \n",
    "    return result, client_classes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for d, t in loader:\n",
    "        d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                preds = model(d).argmax(1)\n",
    "        else:\n",
    "            preds = model(d).argmax(1)\n",
    "        \n",
    "        correct += (preds == t).sum().item()\n",
    "        total += len(t)\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_client(model, client_loaders, device):\n",
    "    \"\"\"Evaluate model accuracy on each client's data\"\"\"\n",
    "    model.eval()\n",
    "    client_accuracies = []\n",
    "    \n",
    "    for loader in client_loaders:\n",
    "        correct, total = 0, 0\n",
    "        for d, t in loader:\n",
    "            d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "            \n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    preds = model(d).argmax(1)\n",
    "            else:\n",
    "                preds = model(d).argmax(1)\n",
    "            \n",
    "            correct += (preds == t).sum().item()\n",
    "            total += len(t)\n",
    "        \n",
    "        acc = 100 * correct / total if total > 0 else 0\n",
    "        client_accuracies.append(acc)\n",
    "    \n",
    "    return client_accuracies\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FAIRNESS METRICS\n",
    "# ============================================================\n",
    "\n",
    "def calculate_jfi(performances):\n",
    "    \"\"\"Jain's Fairness Index\"\"\"\n",
    "    p = np.array(performances)\n",
    "    n = len(p)\n",
    "    if np.sum(p ** 2) == 0:\n",
    "        return 1.0\n",
    "    return (np.sum(p) ** 2) / (n * np.sum(p ** 2))\n",
    "\n",
    "def calculate_accuracy_gap(performances):\n",
    "    return np.max(performances) - np.min(performances)\n",
    "\n",
    "def calculate_variance(performances):\n",
    "    return np.var(performances)\n",
    "\n",
    "def calculate_max_min_fairness(performances):\n",
    "    p = np.array(performances)\n",
    "    if np.max(p) == 0:\n",
    "        return 0.0\n",
    "    return np.min(p) / np.max(p)\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e70560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION (V7 - No Gradient Clipping, No Warmup)\n",
    "# ============================================================\n",
    "\n",
    "# Training Parameters\n",
    "N_ROUNDS = 50\n",
    "N_CLIENTS = 20\n",
    "N_GAN_EPOCHS = 15\n",
    "N_PROBES = 500\n",
    "LOCAL_EPOCHS = 3\n",
    "\n",
    "# âŒ V7 CHANGE: NO WARMUP ROUNDS - V2 Linear from round 1!\n",
    "# WARMUP_ROUNDS = 0  # Removed!\n",
    "\n",
    "# Fed-Audit-GAN Parameters\n",
    "MOMENTUM = 0.8  # EMA momentum for fairness scores\n",
    "\n",
    "# âŒ V7 CHANGE: NO GRADIENT CLIPPING\n",
    "# GRAD_CLIP_NORM = None  # Removed!\n",
    "\n",
    "# Test multiple gamma values\n",
    "GAMMA_VALUES = [0.3, 0.7]\n",
    "\n",
    "# DataLoader Parameters\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "PREFETCH_FACTOR = 2\n",
    "\n",
    "# PATHOLOGICAL NON-IID\n",
    "CLASSES_PER_CLIENT = 2\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ”§ Fed-Audit-GAN v7.0 - CIFAR-10 (No Gradient Clipping, No Warmup)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPUs: {NUM_GPUS}\")\n",
    "print(f\"AMP Enabled: {USE_AMP}\")\n",
    "print(f\"Rounds: {N_ROUNDS}, Clients: {N_CLIENTS}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ EXPERIMENTS TO RUN:\")\n",
    "print(f\"   ðŸ”µ 1. FedAvg (BASELINE)\")\n",
    "for i, g in enumerate(GAMMA_VALUES, 2):\n",
    "    print(f\"   ðŸŸ¢ {i}. Fed-Audit-GAN Î³={g}\")\n",
    "\n",
    "print(f\"\\nâ­ V7 CHANGES FROM V6:\")\n",
    "print(f\"   âŒ REMOVED: Gradient clipping\")\n",
    "print(f\"   âŒ REMOVED: Warmup rounds - V2 Linear from round 1!\")\n",
    "print(f\"   âœ… KEPT: GAN trains EVERY round\")\n",
    "print(f\"   âœ… KEPT: V2 Linear aggregation formula\")\n",
    "print(f\"   âœ… KEPT: Soft labels for GAN\")\n",
    "print(f\"   âœ… KEPT: EMA momentum ({MOMENTUM})\")\n",
    "\n",
    "print(f\"\\nâ­ PATHOLOGICAL NON-IID:\")\n",
    "print(f\"   Each client gets ONLY {CLASSES_PER_CLIENT}/10 classes\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ceabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create PATHOLOGICAL Non-IID partitions\n",
    "np.random.seed(42)\n",
    "client_idx, client_classes = partition_data_pathological_non_iid(\n",
    "    train_data, N_CLIENTS, classes_per_client=CLASSES_PER_CLIENT\n",
    ")\n",
    "\n",
    "# Calculate data weights\n",
    "client_data_sizes = [len(idx) for idx in client_idx]\n",
    "total_samples = sum(client_data_sizes)\n",
    "CLIENT_DATA_WEIGHTS = [size / total_samples for size in client_data_sizes]\n",
    "\n",
    "# DataLoader kwargs\n",
    "dataloader_kwargs = {\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY,\n",
    "    'persistent_workers': True,\n",
    "    'prefetch_factor': PREFETCH_FACTOR,\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=VAL_BATCH_SIZE, shuffle=False, **dataloader_kwargs)\n",
    "val_loader = DataLoader(\n",
    "    Subset(train_data, np.random.choice(len(train_data), 2000, replace=False)),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **dataloader_kwargs\n",
    ")\n",
    "\n",
    "client_loaders = [\n",
    "    DataLoader(Subset(train_data, client_idx[c]), batch_size=BATCH_SIZE, shuffle=True, **dataloader_kwargs)\n",
    "    for c in range(N_CLIENTS)\n",
    "]\n",
    "\n",
    "print(f\"âœ… Data loaded!\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"â­ PATHOLOGICAL NON-IID ({CLASSES_PER_CLIENT} classes per client)\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(min(10, N_CLIENTS)):\n",
    "    class_names = [CIFAR10_CLASSES[c] for c in client_classes[i]]\n",
    "    print(f\"   Client {i:2d}: {client_classes[i]} â†’ {class_names} ({client_data_sizes[i]} samples)\")\n",
    "\n",
    "if N_CLIENTS > 10:\n",
    "    print(f\"   ... ({N_CLIENTS - 10} more clients)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTIONS (V7 - No Gradient Clipping, No Warmup)\n",
    "# ============================================================\n",
    "\n",
    "def run_fedavg(n_rounds, n_clients, train_data, client_idx, val_loader, test_loader, \n",
    "               client_loaders, local_epochs, device, use_amp, client_data_weights):\n",
    "    \"\"\"\n",
    "    Standard FedAvg (McMahan et al., 2017)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸ”µ RUNNING: FedAvg (Federated Averaging) - BASELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=\"FedAvg\"):\n",
    "        local_weights = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            loss = criterion(output, target)\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            local_weights.append(copy.deepcopy(local_model.state_dict()))\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Data-Weighted Averaging\n",
    "        avg_weights = copy.deepcopy(local_weights[0])\n",
    "        for key in avg_weights.keys():\n",
    "            avg_weights[key] = avg_weights[key] * client_data_weights[0]\n",
    "            for i in range(1, len(local_weights)):\n",
    "                avg_weights[key] += local_weights[i][key] * client_data_weights[i]\n",
    "        \n",
    "        model.load_state_dict(avg_weights)\n",
    "        \n",
    "        # Evaluation\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def run_fed_audit_gan_v7(gamma, n_rounds, n_clients, momentum,\n",
    "                         train_data, client_idx, val_loader, \n",
    "                         test_loader, client_loaders, n_gan_epochs, n_probes, \n",
    "                         local_epochs, device, use_amp, client_data_weights):\n",
    "    \"\"\"\n",
    "    Fed-Audit-GAN v7.0 - No Gradient Clipping, No Warmup\n",
    "    \n",
    "    âŒ REMOVED: Gradient clipping\n",
    "    âŒ REMOVED: Warmup rounds - V2 Linear from round 1!\n",
    "    âœ… KEPT: GAN trains EVERY round\n",
    "    âœ… KEPT: V2 Linear aggregation formula\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ðŸŸ¢ RUNNING: Fed-Audit-GAN v7.0 (Î³={gamma})\")\n",
    "    print(f\"   âŒ NO Gradient Clipping\")\n",
    "    print(f\"   âŒ NO Warmup - V2 Linear from round 1!\")\n",
    "    print(f\"   âœ… V2 Linear Formula: Weight = (1-Î³)Ã—Acc + Î³Ã—Fair\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    fairness_history = {i: 0.0 for i in range(n_clients)}\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'bias': [], 'alphas': [],\n",
    "        'raw_scores': [], 'smoothed_scores': [],\n",
    "        'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=f\"Fed-Audit-GAN Î³={gamma}\"):\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 1: Local Client Training (NO Gradient Clipping!)\n",
    "        # ================================================================\n",
    "        updates = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            before_state = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # SGD without Gradient Clipping\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            loss = F.cross_entropy(output, target)\n",
    "                        scaler.scale(loss).backward()\n",
    "                        # âŒ NO GRADIENT CLIPPING\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        loss = F.cross_entropy(output, target)\n",
    "                        loss.backward()\n",
    "                        # âŒ NO GRADIENT CLIPPING\n",
    "                        optimizer.step()\n",
    "            \n",
    "            update = {k: local_model.state_dict()[k] - before_state[k] for k in before_state}\n",
    "            updates.append(update)\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 2 & 3: GAN Training + Fairness Scoring\n",
    "        # GAN trains EVERY round from round 1!\n",
    "        # ================================================================\n",
    "        G = FairnessGenerator(img_shape=(3, 32, 32)).to(device)\n",
    "        D = Discriminator(img_shape=(3, 32, 32)).to(device)\n",
    "        G, D = train_gan(G, D, model, val_loader, epochs=n_gan_epochs, device=device)\n",
    "        \n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_probes, G.latent_dim, device=device)\n",
    "            labels = torch.randint(0, 10, (n_probes,), device=device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x_probe, xp_probe = G(z, labels)\n",
    "            else:\n",
    "                x_probe, xp_probe = G(z, labels)\n",
    "        \n",
    "        B_base = compute_bias(model, x_probe, xp_probe, device)\n",
    "        \n",
    "        S_fair_raw = []\n",
    "        S_fair_smoothed = []\n",
    "        \n",
    "        for cid, upd in enumerate(updates):\n",
    "            hyp_model = copy.deepcopy(model)\n",
    "            hyp_state = hyp_model.state_dict()\n",
    "            for k in hyp_state:\n",
    "                hyp_state[k] = hyp_state[k] + upd[k]\n",
    "            hyp_model.load_state_dict(hyp_state)\n",
    "            \n",
    "            B_client = compute_bias(hyp_model, x_probe, xp_probe, device)\n",
    "            S_current = B_base - B_client\n",
    "            S_fair_raw.append(S_current)\n",
    "            \n",
    "            # EMA smoothing\n",
    "            S_prev = fairness_history[cid]\n",
    "            S_smoothed = (momentum * S_prev) + ((1 - momentum) * S_current)\n",
    "            fairness_history[cid] = S_smoothed\n",
    "            S_fair_smoothed.append(S_smoothed)\n",
    "            del hyp_model\n",
    "        \n",
    "        del G, D, x_probe, xp_probe\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        history['raw_scores'].append(S_fair_raw.copy())\n",
    "        history['smoothed_scores'].append(S_fair_smoothed.copy())\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 4: V2 LINEAR AGGREGATION (FROM ROUND 1 - NO WARMUP!)\n",
    "        # ================================================================\n",
    "        \n",
    "        # Normalize Fairness Score\n",
    "        f_tensor = torch.tensor(S_fair_smoothed, device=device, dtype=torch.float32)\n",
    "        f_min, f_max = f_tensor.min(), f_tensor.max()\n",
    "        if f_max != f_min:\n",
    "            f_norm = (f_tensor - f_min) / (f_max - f_min)\n",
    "        else:\n",
    "            f_norm = torch.ones_like(f_tensor) * 0.5\n",
    "        \n",
    "        # Normalize Accuracy Score (using data weights as proxy)\n",
    "        a_tensor = torch.tensor(client_data_weights, device=device, dtype=torch.float32)\n",
    "        a_min, a_max = a_tensor.min(), a_tensor.max()\n",
    "        if a_max != a_min:\n",
    "            a_norm = (a_tensor - a_min) / (a_max - a_min)\n",
    "        else:\n",
    "            a_norm = torch.ones_like(a_tensor) * 0.5\n",
    "        \n",
    "        # V2 Linear Formula\n",
    "        raw_weights = ((1 - gamma) * a_norm) + (gamma * f_norm) + 1e-8\n",
    "        \n",
    "        # Final Normalization\n",
    "        alphas = (raw_weights / raw_weights.sum()).tolist()\n",
    "        \n",
    "        # Debug output every 10 rounds\n",
    "        if rnd % 10 == 0:\n",
    "            print(f\"\\n   ðŸ“Š Round {rnd+1} Debug (Î³={gamma}):\")\n",
    "            print(f\"      Fairness: min={min(S_fair_smoothed):.4f}, max={max(S_fair_smoothed):.4f}\")\n",
    "            print(f\"      Alphas: min={min(alphas):.4f}, max={max(alphas):.4f}\")\n",
    "        \n",
    "        # Apply weighted aggregation\n",
    "        new_state = model.state_dict()\n",
    "        for k in new_state:\n",
    "            new_state[k] = new_state[k] + sum(a * u[k] for a, u in zip(alphas, updates))\n",
    "        model.load_state_dict(new_state)\n",
    "        \n",
    "        # ================================================================\n",
    "        # EVALUATION\n",
    "        # ================================================================\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['bias'].append(B_base)\n",
    "        history['alphas'].append(alphas.copy())\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'bias': B_base,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs),\n",
    "            'alpha_min': min(alphas),\n",
    "            'alpha_max': max(alphas)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "print(\"âœ… Training functions defined (FedAvg + Fed-Audit-GAN v7.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a636e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN ALL EXPERIMENTS\n",
    "# ============================================================\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 1: FedAvg (BASELINE)\n",
    "# ============================================================\n",
    "wandb.init(\n",
    "    project=\"FED_AUDIT_GAN_TEST_7_CIFAR10\",\n",
    "    name=f\"FedAvg_CIFAR10_clients{N_CLIENTS}_v7\",\n",
    "    config={\n",
    "        \"method\": \"FedAvg\",\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "        \"n_rounds\": N_ROUNDS,\n",
    "        \"n_clients\": N_CLIENTS,\n",
    "        \"non_iid_type\": \"pathological\",\n",
    "        \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "        \"device\": str(DEVICE),\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"amp_enabled\": USE_AMP\n",
    "    }\n",
    ")\n",
    "\n",
    "fedavg_model, fedavg_history = run_fedavg(\n",
    "    n_rounds=N_ROUNDS,\n",
    "    n_clients=N_CLIENTS,\n",
    "    train_data=train_data,\n",
    "    client_idx=client_idx,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    client_loaders=client_loaders,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    device=DEVICE,\n",
    "    use_amp=USE_AMP,\n",
    "    client_data_weights=CLIENT_DATA_WEIGHTS\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "all_results['FedAvg'] = {\n",
    "    'model': fedavg_model,\n",
    "    'history': fedavg_history,\n",
    "    'name': 'FedAvg'\n",
    "}\n",
    "\n",
    "print(f\"âœ… FedAvg Complete!\")\n",
    "print(f\"   Final Accuracy: {fedavg_history['acc'][-1]:.2f}%\")\n",
    "print(f\"   Final JFI: {fedavg_history['jfi'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENTS 2-3: Fed-Audit-GAN v7.0 with Î³ = 0.3 and 0.7\n",
    "# ============================================================\n",
    "for gamma in GAMMA_VALUES:\n",
    "    method_name = f\"FedAuditGAN_v7_Î³={gamma}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"FED_AUDIT_GAN_TEST_7_CIFAR10\",\n",
    "        name=f\"{method_name}_CIFAR10_clients{N_CLIENTS}_no_warmup\",\n",
    "        config={\n",
    "            \"method\": method_name,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"n_rounds\": N_ROUNDS,\n",
    "            \"n_clients\": N_CLIENTS,\n",
    "            \"gamma\": gamma,\n",
    "            \"momentum\": MOMENTUM,\n",
    "            \"warmup_rounds\": 0,  # NO WARMUP!\n",
    "            \"gradient_clipping\": False,  # NO GRADIENT CLIPPING!\n",
    "            \"gan_every_round\": True,\n",
    "            \"aggregation_method\": \"V2_LINEAR\",\n",
    "            \"non_iid_type\": \"pathological\",\n",
    "            \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "            \"device\": str(DEVICE),\n",
    "            \"num_gpus\": NUM_GPUS,\n",
    "            \"amp_enabled\": USE_AMP\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model, history = run_fed_audit_gan_v7(\n",
    "        gamma=gamma,\n",
    "        n_rounds=N_ROUNDS,\n",
    "        n_clients=N_CLIENTS,\n",
    "        momentum=MOMENTUM,\n",
    "        train_data=train_data,\n",
    "        client_idx=client_idx,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        client_loaders=client_loaders,\n",
    "        n_gan_epochs=N_GAN_EPOCHS,\n",
    "        n_probes=N_PROBES,\n",
    "        local_epochs=LOCAL_EPOCHS,\n",
    "        device=DEVICE,\n",
    "        use_amp=USE_AMP,\n",
    "        client_data_weights=CLIENT_DATA_WEIGHTS\n",
    "    )\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    all_results[method_name] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'name': method_name\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… {method_name} Complete!\")\n",
    "    print(f\"   Final Accuracy: {history['acc'][-1]:.2f}%\")\n",
    "    print(f\"   Final JFI: {history['jfi'][-1]:.4f}\")\n",
    "    print(f\"   Accuracy Gap: {history['accuracy_gap'][-1]:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65262033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "# Color scheme\n",
    "colors = {\n",
    "    'FedAvg': '#1f77b4',\n",
    "    'FedAuditGAN_v7_Î³=0.3': '#2ca02c',\n",
    "    'FedAuditGAN_v7_Î³=0.7': '#d62728'\n",
    "}\n",
    "\n",
    "linestyles = {\n",
    "    'FedAvg': '--',\n",
    "    'FedAuditGAN_v7_Î³=0.3': '-',\n",
    "    'FedAuditGAN_v7_Î³=0.7': '-'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "rounds = list(range(1, N_ROUNDS + 1))\n",
    "\n",
    "# Plot 1: Global Accuracy\n",
    "ax = axes[0, 0]\n",
    "for name, result in all_results.items():\n",
    "    ax.plot(rounds, result['history']['acc'], \n",
    "            label=name, color=colors.get(name, 'gray'), \n",
    "            linestyle=linestyles.get(name, '-'), linewidth=2)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('CIFAR-10: Global Test Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: JFI\n",
    "ax = axes[0, 1]\n",
    "for name, result in all_results.items():\n",
    "    ax.plot(rounds, result['history']['jfi'], \n",
    "            label=name, color=colors.get(name, 'gray'),\n",
    "            linestyle=linestyles.get(name, '-'), linewidth=2)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('JFI')\n",
    "ax.set_title(\"Jain's Fairness Index (higher=fairer)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Max-Min Fairness\n",
    "ax = axes[0, 2]\n",
    "for name, result in all_results.items():\n",
    "    ax.plot(rounds, result['history']['max_min_fairness'], \n",
    "            label=name, color=colors.get(name, 'gray'),\n",
    "            linestyle=linestyles.get(name, '-'), linewidth=2)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Min/Max Ratio')\n",
    "ax.set_title('Max-Min Fairness (higher=fairer)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Variance\n",
    "ax = axes[1, 0]\n",
    "for name, result in all_results.items():\n",
    "    ax.plot(rounds, result['history']['variance'], \n",
    "            label=name, color=colors.get(name, 'gray'),\n",
    "            linestyle=linestyles.get(name, '-'), linewidth=2)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Variance')\n",
    "ax.set_title('Per-Client Accuracy Variance (lower=fairer)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Accuracy Gap\n",
    "ax = axes[1, 1]\n",
    "for name, result in all_results.items():\n",
    "    ax.plot(rounds, result['history']['accuracy_gap'], \n",
    "            label=name, color=colors.get(name, 'gray'),\n",
    "            linestyle=linestyles.get(name, '-'), linewidth=2)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Gap (%)')\n",
    "ax.set_title('Best-Worst Client Gap (lower=fairer)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final Per-Client Accuracy\n",
    "ax = axes[1, 2]\n",
    "x = np.arange(N_CLIENTS)\n",
    "width = 0.25\n",
    "for i, (name, result) in enumerate(all_results.items()):\n",
    "    final_accs = result['history']['client_accuracies'][-1]\n",
    "    ax.bar(x + i * width, final_accs, width, \n",
    "           label=name, color=colors.get(name, 'gray'), alpha=0.8)\n",
    "ax.set_xlabel('Client ID')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title(f'Per-Client Accuracy (Round {N_ROUNDS})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cifar10_v7_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Plots saved to cifar10_v7_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26090e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 110)\n",
    "print(f\"CIFAR-10 V7 FINAL RESULTS SUMMARY (Round {N_ROUNDS})\")\n",
    "print(\"=\" * 110)\n",
    "print(f\"{'Method':<30} {'Accuracy':>10} {'JFI':>8} {'Max-Min':>10} {'Variance':>12} {'Min Acc':>10} {'Max Acc':>10} {'Gap':>8}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for name, result in all_results.items():\n",
    "    h = result['history']\n",
    "    acc = h['acc'][-1]\n",
    "    jfi = h['jfi'][-1]\n",
    "    mmf = h['max_min_fairness'][-1]\n",
    "    var = h['variance'][-1]\n",
    "    min_acc = h['min_client_acc'][-1]\n",
    "    max_acc = h['max_client_acc'][-1]\n",
    "    gap = h['accuracy_gap'][-1]\n",
    "    \n",
    "    print(f\"{name:<30} {acc:>10.2f}% {jfi:>8.4f} {mmf:>10.4f} {var:>12.2f} {min_acc:>10.2f}% {max_acc:>10.2f}% {gap:>8.2f}%\")\n",
    "\n",
    "print(\"=\" * 110)\n",
    "\n",
    "# Winner analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FAIRNESS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best JFI\n",
    "best_jfi_name = max(all_results.keys(), key=lambda x: all_results[x]['history']['jfi'][-1])\n",
    "best_jfi_val = all_results[best_jfi_name]['history']['jfi'][-1]\n",
    "print(f\"Best JFI:        {best_jfi_name} ({best_jfi_val:.4f})\")\n",
    "\n",
    "# Best Max-Min Fairness\n",
    "best_mmf_name = max(all_results.keys(), key=lambda x: all_results[x]['history']['max_min_fairness'][-1])\n",
    "best_mmf_val = all_results[best_mmf_name]['history']['max_min_fairness'][-1]\n",
    "print(f\"Best Max-Min:    {best_mmf_name} ({best_mmf_val:.4f})\")\n",
    "\n",
    "# Lowest Variance\n",
    "best_var_name = min(all_results.keys(), key=lambda x: all_results[x]['history']['variance'][-1])\n",
    "best_var_val = all_results[best_var_name]['history']['variance'][-1]\n",
    "print(f\"Lowest Variance: {best_var_name} ({best_var_val:.2f})\")\n",
    "\n",
    "# Smallest Gap\n",
    "best_gap_name = min(all_results.keys(), key=lambda x: all_results[x]['history']['accuracy_gap'][-1])\n",
    "best_gap_val = all_results[best_gap_name]['history']['accuracy_gap'][-1]\n",
    "print(f\"Smallest Gap:    {best_gap_name} ({best_gap_val:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… V7 Experiment Complete!\")\n",
    "print(\"   âŒ NO Gradient Clipping\")\n",
    "print(\"   âŒ NO Warmup Rounds\")\n",
    "print(\"   âœ… V2 Linear Aggregation from Round 1\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42123a3a",
   "metadata": {},
   "source": [
    "# ðŸ”§ Fed-Audit-GAN v7.0 - CIFAR-10 (FedProx + GAN Every Round)\n",
    "\n",
    "## ðŸŽ¯ Experiments Run:\n",
    "\n",
    "### ðŸ”µ Baseline: FedAvg (Federated Averaging)\n",
    "- Standard data-weighted averaging (McMahan et al., 2017)\n",
    "- NO GAN, NO fairness scoring\n",
    "\n",
    "### ðŸŸ¢ Our Method: Fed-Audit-GAN (V2 Linear Formula)\n",
    "- **Fed-Audit-GAN Î³ = 0.3** - Accuracy-weighted\n",
    "- **Fed-Audit-GAN Î³ = 0.7** - Fairness-weighted\n",
    "\n",
    "## ðŸ”§ Fed-Audit-GAN 4-Phase Architecture:\n",
    "1. **Phase 1**: Local Client Training (SGD + FedProx)\n",
    "2. **Phase 2**: GAN Training (EVERY round - including warm-up!)\n",
    "3. **Phase 3**: Fairness Scoring (with EMA)\n",
    "4. **Phase 4**: V2 Linear Aggregation (Min-Max Normalized)\n",
    "\n",
    "## â­ KEY CHANGES FROM V6:\n",
    "- âœ… **ADDED BACK**: FedProx proximal term (Î¼=0.01)\n",
    "- âŒ **REMOVED**: Gradient clipping\n",
    "- âœ… **KEPT**: GAN trains EVERY round (including warm-up rounds 1-10)\n",
    "- âœ… **KEPT**: Audit every round after round 10\n",
    "- âœ… **KEPT**: V2 Linear Formula for aggregation\n",
    "- âœ… **KEPT**: Soft labels for GAN stability\n",
    "- âœ… **KEPT**: Warm-up period (but GAN still trains during warm-up)\n",
    "\n",
    "## â­ V2 Linear Aggregation Formula:\n",
    "```\n",
    "Weight = (1 - Î³) Ã— Accuracy_norm + Î³ Ã— Fairness_norm\n",
    "```\n",
    "\n",
    "## â­ PATHOLOGICAL NON-IID:\n",
    "- Each client ONLY has 2 out of 10 classes\n",
    "- Clients have DIFFERENT sample sizes (Dirichlet distribution)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20eca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Dependencies\n",
    "!pip install -q torch torchvision tqdm matplotlib numpy wandb\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c9be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Login to WandB\n",
    "import wandb\n",
    "wandb.login()\n",
    "print(\"âœ… WandB logged in!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Imports and GPU Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# ðŸš€ GPU SETUP\n",
    "# ============================================================\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    NUM_GPUS = torch.cuda.device_count()\n",
    "    \n",
    "    # Enable cuDNN optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    \n",
    "    # Enable TF32 for faster matrix operations\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"âœ… GPU(s) detected: {NUM_GPUS}\")\n",
    "    for i in range(NUM_GPUS):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"   GPU {i}: {props.name}\")\n",
    "        print(f\"      Memory: {props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    NUM_GPUS = 0\n",
    "    print(\"âš ï¸  No GPU detected. Using CPU.\")\n",
    "\n",
    "# Mixed Precision Training\n",
    "USE_AMP = torch.cuda.is_available()\n",
    "if USE_AMP:\n",
    "    print(\"\\nâœ… Mixed Precision Training (AMP) enabled.\")\n",
    "\n",
    "print(f\"\\nðŸ“ Device: {DEVICE}\")\n",
    "print(f\"   PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEFINITIONS (CIFAR-10)\n",
    "# ============================================================\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"CNN for CIFAR-10 classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512 * 2 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32->16\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16->8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8->4\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))  # 4->2\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class FairnessGenerator(nn.Module):\n",
    "    \"\"\"Generator that produces paired samples (x, x') for fairness testing\"\"\"\n",
    "    def __init__(self, latent_dim=128, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, latent_dim)\n",
    "        self.init_size = img_shape[1] // 4  # 8\n",
    "        self.l1 = nn.Linear(latent_dim * 2, 512 * self.init_size ** 2)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(512, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, 3, 1, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, img_shape[0], 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.delta_scale = 0.1\n",
    "\n",
    "    def forward(self, z, labels):\n",
    "        gen_input = torch.cat([z, self.label_emb(labels)], dim=1)\n",
    "        out = self.l1(gen_input).view(-1, 512, self.init_size, self.init_size)\n",
    "        x = self.conv_blocks(out)\n",
    "        delta = self.delta_net(z).view(-1, *self.img_shape) * self.delta_scale\n",
    "        return x, torch.clamp(x + delta, -1, 1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Conditional Discriminator\"\"\"\n",
    "    def __init__(self, num_classes=10, img_shape=(3, 32, 32)):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.img_shape = img_shape\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(img_shape[0] + num_classes, 64, 3, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(256, 512, 3, 2, 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.fc = nn.Linear(512 * 2 * 2, 1)\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        label_map = self.label_emb(labels).view(-1, self.num_classes, 1, 1)\n",
    "        label_map = label_map.expand(-1, -1, self.img_shape[1], self.img_shape[2])\n",
    "        out = self.conv(torch.cat([img, label_map], dim=1))\n",
    "        return self.fc(out.view(out.size(0), -1))\n",
    "\n",
    "\n",
    "print(\"âœ… Models defined: CNN, FairnessGenerator, Discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb90dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def train_gan(G, D, model, loader, epochs=15, device='cuda', l1=1.0, l2=1.0):\n",
    "    \"\"\"\n",
    "    Train the Fairness GAN with Soft Labels for stability\n",
    "    \"\"\"\n",
    "    G, D, model = G.to(device), D.to(device), model.to(device)\n",
    "    model.eval()\n",
    "    opt_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    opt_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    if USE_AMP:\n",
    "        scaler_G = torch.amp.GradScaler(device='cuda')\n",
    "        scaler_D = torch.amp.GradScaler(device='cuda')\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        for imgs, labels in loader:\n",
    "            bs = imgs.size(0)\n",
    "            \n",
    "            # Soft Labels for stability\n",
    "            real_labels = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "            fake_labels = torch.empty(bs, 1, device=device).uniform_(0.0, 0.1)\n",
    "            \n",
    "            imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            z = torch.randn(bs, G.latent_dim, device=device)\n",
    "            gl = torch.randint(0, G.num_classes, (bs,), device=device)\n",
    "            \n",
    "            # Generator\n",
    "            opt_G.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    with torch.no_grad():\n",
    "                        px, pxp = model(x), model(xp)\n",
    "                    t1 = -torch.mean((px - pxp) ** 2)\n",
    "                    t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                    g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                    t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                    g_loss = t1 + t2 + t3\n",
    "                scaler_G.scale(g_loss).backward()\n",
    "                scaler_G.step(opt_G)\n",
    "                scaler_G.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                with torch.no_grad():\n",
    "                    px, pxp = model(x), model(xp)\n",
    "                t1 = -torch.mean((px - pxp) ** 2)\n",
    "                t2 = l1 * torch.mean((x - xp) ** 2)\n",
    "                g_real = torch.empty(bs, 1, device=device).uniform_(0.9, 1.0)\n",
    "                t3 = l2 * (bce(D(x, gl), g_real) + bce(D(xp, gl), g_real)) / 2\n",
    "                g_loss = t1 + t2 + t3\n",
    "                g_loss.backward()\n",
    "                opt_G.step()\n",
    "            \n",
    "            # Discriminator\n",
    "            opt_D.zero_grad(set_to_none=True)\n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x, xp = G(z, gl)\n",
    "                    d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                              bce(D(x.detach(), gl), fake_labels) + \n",
    "                              bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                scaler_D.scale(d_loss).backward()\n",
    "                scaler_D.step(opt_D)\n",
    "                scaler_D.update()\n",
    "            else:\n",
    "                x, xp = G(z, gl)\n",
    "                d_loss = (bce(D(imgs, labels), real_labels) + \n",
    "                          bce(D(x.detach(), gl), fake_labels) + \n",
    "                          bce(D(xp.detach(), gl), fake_labels)) / 3\n",
    "                d_loss.backward()\n",
    "                opt_D.step()\n",
    "    \n",
    "    return G, D\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bias(model, x, xp, device):\n",
    "    \"\"\"Compute bias as difference in model predictions between x and x'\"\"\"\n",
    "    model.eval()\n",
    "    x, xp = x.to(device), xp.to(device)\n",
    "    \n",
    "    if USE_AMP:\n",
    "        with torch.amp.autocast(device_type='cuda'):\n",
    "            diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    else:\n",
    "        diff = torch.abs(model(x) - model(xp)).sum(1).mean()\n",
    "    \n",
    "    return diff.item()\n",
    "\n",
    "\n",
    "def partition_data_pathological_non_iid(dataset, n_clients, classes_per_client=2):\n",
    "    \"\"\"Pathological Non-IID with unequal sample sizes\"\"\"\n",
    "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
    "    n_classes = len(np.unique(labels))\n",
    "    \n",
    "    class_indices = {c: np.where(labels == c)[0] for c in range(n_classes)}\n",
    "    for c in class_indices:\n",
    "        np.random.shuffle(class_indices[c])\n",
    "    \n",
    "    client_classes = []\n",
    "    for cid in range(n_clients):\n",
    "        start_class = (cid * classes_per_client) % n_classes\n",
    "        assigned = [(start_class + i) % n_classes for i in range(classes_per_client)]\n",
    "        client_classes.append(assigned)\n",
    "    \n",
    "    # Unequal sample sizes using Dirichlet\n",
    "    alpha = 0.5\n",
    "    client_proportions = np.random.dirichlet([alpha] * n_clients)\n",
    "    \n",
    "    client_indices = [[] for _ in range(n_clients)]\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        clients_with_class = [cid for cid in range(n_clients) if c in client_classes[cid]]\n",
    "        \n",
    "        if len(clients_with_class) > 0:\n",
    "            class_samples = class_indices[c]\n",
    "            total_for_class = len(class_samples)\n",
    "            \n",
    "            relevant_props = np.array([client_proportions[cid] for cid in clients_with_class])\n",
    "            relevant_props = relevant_props / relevant_props.sum()\n",
    "            \n",
    "            start_idx = 0\n",
    "            for i, cid in enumerate(clients_with_class):\n",
    "                if i == len(clients_with_class) - 1:\n",
    "                    end_idx = total_for_class\n",
    "                else:\n",
    "                    n_samples = int(total_for_class * relevant_props[i])\n",
    "                    end_idx = min(start_idx + n_samples, total_for_class)\n",
    "                \n",
    "                if start_idx < end_idx:\n",
    "                    client_indices[cid].extend(class_samples[start_idx:end_idx].tolist())\n",
    "                start_idx = end_idx\n",
    "    \n",
    "    result = []\n",
    "    for cid in range(n_clients):\n",
    "        if len(client_indices[cid]) > 0:\n",
    "            indices = np.array(client_indices[cid])\n",
    "            np.random.shuffle(indices)\n",
    "            result.append(indices)\n",
    "        else:\n",
    "            fallback_samples = np.random.choice(len(dataset), size=50, replace=False)\n",
    "            result.append(fallback_samples)\n",
    "    \n",
    "    return result, client_classes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate model accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    for d, t in loader:\n",
    "        d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "        \n",
    "        if USE_AMP:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                preds = model(d).argmax(1)\n",
    "        else:\n",
    "            preds = model(d).argmax(1)\n",
    "        \n",
    "        correct += (preds == t).sum().item()\n",
    "        total += len(t)\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_per_client(model, client_loaders, device):\n",
    "    \"\"\"Evaluate model accuracy on each client's data\"\"\"\n",
    "    model.eval()\n",
    "    client_accuracies = []\n",
    "    \n",
    "    for loader in client_loaders:\n",
    "        correct, total = 0, 0\n",
    "        for d, t in loader:\n",
    "            d, t = d.to(device, non_blocking=True), t.to(device, non_blocking=True)\n",
    "            \n",
    "            if USE_AMP:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    preds = model(d).argmax(1)\n",
    "            else:\n",
    "                preds = model(d).argmax(1)\n",
    "            \n",
    "            correct += (preds == t).sum().item()\n",
    "            total += len(t)\n",
    "        \n",
    "        acc = 100 * correct / total if total > 0 else 0\n",
    "        client_accuracies.append(acc)\n",
    "    \n",
    "    return client_accuracies\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FAIRNESS METRICS\n",
    "# ============================================================\n",
    "\n",
    "def calculate_jfi(performances):\n",
    "    \"\"\"Jain's Fairness Index\"\"\"\n",
    "    p = np.array(performances)\n",
    "    n = len(p)\n",
    "    if np.sum(p ** 2) == 0:\n",
    "        return 1.0\n",
    "    return (np.sum(p) ** 2) / (n * np.sum(p ** 2))\n",
    "\n",
    "def calculate_accuracy_gap(performances):\n",
    "    return np.max(performances) - np.min(performances)\n",
    "\n",
    "def calculate_variance(performances):\n",
    "    return np.var(performances)\n",
    "\n",
    "def calculate_max_min_fairness(performances):\n",
    "    p = np.array(performances)\n",
    "    if np.max(p) == 0:\n",
    "        return 0.0\n",
    "    return np.min(p) / np.max(p)\n",
    "\n",
    "\n",
    "print(\"âœ… Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56785aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION (V7 - FedProx + GAN Every Round)\n",
    "# ============================================================\n",
    "\n",
    "# Training Parameters\n",
    "N_ROUNDS = 50\n",
    "N_CLIENTS = 20\n",
    "N_GAN_EPOCHS = 15\n",
    "N_PROBES = 500\n",
    "LOCAL_EPOCHS = 3\n",
    "\n",
    "# Warm-up (aggregation only - GAN still trains!)\n",
    "WARMUP_ROUNDS = 10\n",
    "\n",
    "# GAN trains EVERY round (no audit frequency limit)\n",
    "# After warm-up, audit every round\n",
    "\n",
    "# Fed-Audit-GAN Parameters\n",
    "MOMENTUM = 0.8  # EMA momentum for fairness scores\n",
    "\n",
    "# â­ V7 CHANGE: FedProx ADDED BACK\n",
    "MU = 0.01  # FedProx proximal term coefficient\n",
    "\n",
    "# âŒ V7: Gradient Clipping REMOVED\n",
    "# GRAD_CLIP_NORM = 1.0  # Removed!\n",
    "\n",
    "# Test multiple gamma values\n",
    "GAMMA_VALUES = [0.3, 0.7]\n",
    "\n",
    "# DataLoader Parameters\n",
    "BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 256\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "PREFETCH_FACTOR = 2\n",
    "\n",
    "# PATHOLOGICAL NON-IID\n",
    "CLASSES_PER_CLIENT = 2\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ”§ Fed-Audit-GAN v7.0 - CIFAR-10 (FedProx + GAN Every Round)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"GPUs: {NUM_GPUS}\")\n",
    "print(f\"AMP Enabled: {USE_AMP}\")\n",
    "print(f\"Rounds: {N_ROUNDS}, Clients: {N_CLIENTS}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ EXPERIMENTS TO RUN:\")\n",
    "print(f\"   ðŸ”µ 1. FedAvg (BASELINE)\")\n",
    "for i, g in enumerate(GAMMA_VALUES, 2):\n",
    "    print(f\"   ðŸŸ¢ {i}. Fed-Audit-GAN Î³={g}\")\n",
    "\n",
    "print(f\"\\nâ­ V7 CHANGES FROM V6:\")\n",
    "print(f\"   âœ… ADDED BACK: FedProx proximal term (Î¼={MU})\")\n",
    "print(f\"   âŒ REMOVED: Gradient clipping\")\n",
    "print(f\"   âœ… KEPT: GAN trains EVERY round (including warm-up!)\")\n",
    "print(f\"   âœ… KEPT: Audit every round after round {WARMUP_ROUNDS}\")\n",
    "print(f\"   âœ… KEPT: V2 Linear aggregation formula\")\n",
    "print(f\"   âœ… KEPT: Soft labels for GAN\")\n",
    "print(f\"   âœ… KEPT: EMA momentum ({MOMENTUM})\")\n",
    "\n",
    "print(f\"\\nâ­ PATHOLOGICAL NON-IID:\")\n",
    "print(f\"   Each client gets ONLY {CLASSES_PER_CLIENT}/10 classes\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76753d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# DATA LOADING\n",
    "# ============================================================\n",
    "\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "])\n",
    "\n",
    "train_data = datasets.CIFAR10('./data', train=True, download=True, transform=transform_train)\n",
    "test_data = datasets.CIFAR10('./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "# Create PATHOLOGICAL Non-IID partitions\n",
    "np.random.seed(42)\n",
    "client_idx, client_classes = partition_data_pathological_non_iid(\n",
    "    train_data, N_CLIENTS, classes_per_client=CLASSES_PER_CLIENT\n",
    ")\n",
    "\n",
    "# Calculate data weights\n",
    "client_data_sizes = [len(idx) for idx in client_idx]\n",
    "total_samples = sum(client_data_sizes)\n",
    "CLIENT_DATA_WEIGHTS = [size / total_samples for size in client_data_sizes]\n",
    "\n",
    "# DataLoader kwargs\n",
    "dataloader_kwargs = {\n",
    "    'num_workers': NUM_WORKERS,\n",
    "    'pin_memory': PIN_MEMORY,\n",
    "    'persistent_workers': True,\n",
    "    'prefetch_factor': PREFETCH_FACTOR,\n",
    "}\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=VAL_BATCH_SIZE, shuffle=False, **dataloader_kwargs)\n",
    "val_loader = DataLoader(\n",
    "    Subset(train_data, np.random.choice(len(train_data), 2000, replace=False)),\n",
    "    batch_size=BATCH_SIZE, shuffle=False, **dataloader_kwargs\n",
    ")\n",
    "\n",
    "client_loaders = [\n",
    "    DataLoader(Subset(train_data, client_idx[c]), batch_size=BATCH_SIZE, shuffle=True, **dataloader_kwargs)\n",
    "    for c in range(N_CLIENTS)\n",
    "]\n",
    "\n",
    "print(f\"âœ… Data loaded!\")\n",
    "print(f\"   Training samples: {len(train_data)}\")\n",
    "print(f\"   Test samples: {len(test_data)}\")\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"â­ PATHOLOGICAL NON-IID ({CLASSES_PER_CLIENT} classes per client)\")\n",
    "print(\"=\" * 70)\n",
    "for i in range(min(10, N_CLIENTS)):\n",
    "    class_names = [CIFAR10_CLASSES[c] for c in client_classes[i]]\n",
    "    print(f\"   Client {i:2d}: {client_classes[i]} â†’ {class_names} ({client_data_sizes[i]} samples)\")\n",
    "\n",
    "if N_CLIENTS > 10:\n",
    "    print(f\"   ... ({N_CLIENTS - 10} more clients)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf5f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# TRAINING FUNCTIONS (V7 - FedProx + GAN Every Round)\n",
    "# ============================================================\n",
    "\n",
    "def run_fedavg(n_rounds, n_clients, train_data, client_idx, val_loader, test_loader, \n",
    "               client_loaders, local_epochs, device, use_amp, client_data_weights):\n",
    "    \"\"\"\n",
    "    Standard FedAvg (McMahan et al., 2017)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ðŸ”µ RUNNING: FedAvg (Federated Averaging) - BASELINE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=\"FedAvg\"):\n",
    "        local_weights = []\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            loss = criterion(output, target)\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        loss = criterion(output, target)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            local_weights.append(copy.deepcopy(local_model.state_dict()))\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Data-Weighted Averaging\n",
    "        avg_weights = copy.deepcopy(local_weights[0])\n",
    "        for key in avg_weights.keys():\n",
    "            avg_weights[key] = avg_weights[key] * client_data_weights[0]\n",
    "            for i in range(1, len(local_weights)):\n",
    "                avg_weights[key] += local_weights[i][key] * client_data_weights[i]\n",
    "        \n",
    "        model.load_state_dict(avg_weights)\n",
    "        \n",
    "        # Evaluation\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "def run_fed_audit_gan_v7(gamma, n_rounds, n_clients, warmup_rounds, momentum,\n",
    "                         mu, train_data, client_idx, val_loader, \n",
    "                         test_loader, client_loaders, n_gan_epochs, n_probes, \n",
    "                         local_epochs, device, use_amp, client_data_weights):\n",
    "    \"\"\"\n",
    "    Fed-Audit-GAN v7.0 - FedProx + GAN Every Round\n",
    "    \n",
    "    âœ… ADDED BACK: FedProx proximal term\n",
    "    âŒ REMOVED: Gradient clipping\n",
    "    âœ… KEPT: GAN trains EVERY round (including warm-up!)\n",
    "    âœ… KEPT: Audit every round after warm-up\n",
    "    âœ… KEPT: V2 Linear aggregation formula\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\"ðŸŸ¢ RUNNING: Fed-Audit-GAN v7.0 (Î³={gamma})\")\n",
    "    print(f\"   âœ… FedProx (Î¼={mu})\")\n",
    "    print(f\"   âœ… GAN trains EVERY round (including warm-up!)\")\n",
    "    print(f\"   âœ… V2 Linear Formula: Weight = (1-Î³)Ã—Acc + Î³Ã—Fair\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = CNN().to(device)\n",
    "    scaler = torch.amp.GradScaler(device='cuda') if use_amp else None\n",
    "    \n",
    "    fairness_history = {i: 0.0 for i in range(n_clients)}\n",
    "    \n",
    "    history = {\n",
    "        'acc': [], 'bias': [], 'alphas': [],\n",
    "        'raw_scores': [], 'smoothed_scores': [],\n",
    "        'client_accuracies': [],\n",
    "        'jfi': [], 'max_min_fairness': [], 'variance': [], 'accuracy_gap': [],\n",
    "        'min_client_acc': [], 'max_client_acc': [],\n",
    "        'gan_trained': []\n",
    "    }\n",
    "    \n",
    "    for rnd in tqdm(range(n_rounds), desc=f\"Fed-Audit-GAN Î³={gamma}\"):\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 1: Local Client Training (with FedProx!)\n",
    "        # ================================================================\n",
    "        updates = []\n",
    "        global_state = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        for cid in range(n_clients):\n",
    "            local_model = copy.deepcopy(model)\n",
    "            local_model.train()\n",
    "            before_state = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # SGD with FedProx (NO Gradient Clipping)\n",
    "            optimizer = optim.SGD(local_model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)\n",
    "            \n",
    "            for epoch in range(local_epochs):\n",
    "                for data, target in client_loaders[cid]:\n",
    "                    data = data.to(device, non_blocking=True)\n",
    "                    target = target.to(device, non_blocking=True)\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    if use_amp:\n",
    "                        with torch.amp.autocast(device_type='cuda'):\n",
    "                            output = local_model(data)\n",
    "                            loss = F.cross_entropy(output, target)\n",
    "                            \n",
    "                            # âœ… FedProx Proximal Term\n",
    "                            prox_term = 0.0\n",
    "                            for name, param in local_model.named_parameters():\n",
    "                                if name in global_state:\n",
    "                                    prox_term += ((param - global_state[name].to(device)) ** 2).sum()\n",
    "                            loss = loss + (mu / 2) * prox_term\n",
    "                        \n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        output = local_model(data)\n",
    "                        loss = F.cross_entropy(output, target)\n",
    "                        \n",
    "                        # âœ… FedProx Proximal Term\n",
    "                        prox_term = 0.0\n",
    "                        for name, param in local_model.named_parameters():\n",
    "                            if name in global_state:\n",
    "                                prox_term += ((param - global_state[name].to(device)) ** 2).sum()\n",
    "                        loss = loss + (mu / 2) * prox_term\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            \n",
    "            update = {k: local_model.state_dict()[k] - before_state[k] for k in before_state}\n",
    "            updates.append(update)\n",
    "            del local_model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 2 & 3: GAN Training + Fairness Scoring\n",
    "        # GAN trains EVERY round (including warm-up!)\n",
    "        # ================================================================\n",
    "        B_base = 0.0\n",
    "        S_fair_raw = [0.0] * n_clients\n",
    "        S_fair_smoothed = [0.0] * n_clients\n",
    "        \n",
    "        # Train GAN EVERY round (including warm-up)\n",
    "        G = FairnessGenerator(img_shape=(3, 32, 32)).to(device)\n",
    "        D = Discriminator(img_shape=(3, 32, 32)).to(device)\n",
    "        G, D = train_gan(G, D, model, val_loader, epochs=n_gan_epochs, device=device)\n",
    "        \n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_probes, G.latent_dim, device=device)\n",
    "            labels = torch.randint(0, 10, (n_probes,), device=device)\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.amp.autocast(device_type='cuda'):\n",
    "                    x_probe, xp_probe = G(z, labels)\n",
    "            else:\n",
    "                x_probe, xp_probe = G(z, labels)\n",
    "        \n",
    "        B_base = compute_bias(model, x_probe, xp_probe, device)\n",
    "        \n",
    "        S_fair_raw = []\n",
    "        S_fair_smoothed = []\n",
    "        \n",
    "        for cid, upd in enumerate(updates):\n",
    "            hyp_model = copy.deepcopy(model)\n",
    "            hyp_state = hyp_model.state_dict()\n",
    "            for k in hyp_state:\n",
    "                hyp_state[k] = hyp_state[k] + upd[k]\n",
    "            hyp_model.load_state_dict(hyp_state)\n",
    "            \n",
    "            B_client = compute_bias(hyp_model, x_probe, xp_probe, device)\n",
    "            S_current = B_base - B_client\n",
    "            S_fair_raw.append(S_current)\n",
    "            \n",
    "            # EMA smoothing\n",
    "            S_prev = fairness_history[cid]\n",
    "            S_smoothed = (momentum * S_prev) + ((1 - momentum) * S_current)\n",
    "            fairness_history[cid] = S_smoothed\n",
    "            S_fair_smoothed.append(S_smoothed)\n",
    "            del hyp_model\n",
    "        \n",
    "        del G, D, x_probe, xp_probe\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        history['raw_scores'].append(S_fair_raw.copy())\n",
    "        history['smoothed_scores'].append(S_fair_smoothed.copy())\n",
    "        history['gan_trained'].append(True)  # GAN trains every round\n",
    "        \n",
    "        # ================================================================\n",
    "        # PHASE 4: V2 LINEAR AGGREGATION\n",
    "        # Warm-up: Use FedAvg (but GAN still trained for score accumulation)\n",
    "        # After warm-up: Use V2 Linear with gamma\n",
    "        # ================================================================\n",
    "        if rnd < warmup_rounds:\n",
    "            # During warm-up: Use FedAvg weights (but GAN still trained!)\n",
    "            alphas = client_data_weights.copy()\n",
    "        else:\n",
    "            # After warm-up: Use V2 Linear Formula\n",
    "            \n",
    "            # Normalize Fairness Score\n",
    "            f_tensor = torch.tensor(S_fair_smoothed, device=device, dtype=torch.float32)\n",
    "            f_min, f_max = f_tensor.min(), f_tensor.max()\n",
    "            if f_max != f_min:\n",
    "                f_norm = (f_tensor - f_min) / (f_max - f_min)\n",
    "            else:\n",
    "                f_norm = torch.ones_like(f_tensor) * 0.5\n",
    "            \n",
    "            # Normalize Accuracy Score (using data weights as proxy)\n",
    "            a_tensor = torch.tensor(client_data_weights, device=device, dtype=torch.float32)\n",
    "            a_min, a_max = a_tensor.min(), a_tensor.max()\n",
    "            if a_max != a_min:\n",
    "                a_norm = (a_tensor - a_min) / (a_max - a_min)\n",
    "            else:\n",
    "                a_norm = torch.ones_like(a_tensor) * 0.5\n",
    "            \n",
    "            # V2 Linear Formula\n",
    "            raw_weights = ((1 - gamma) * a_norm) + (gamma * f_norm) + 1e-8\n",
    "            \n",
    "            # Final Normalization\n",
    "            alphas = (raw_weights / raw_weights.sum()).tolist()\n",
    "            \n",
    "            # Debug output every 10 rounds\n",
    "            if rnd % 10 == 0:\n",
    "                print(f\"\\n   ðŸ“Š Round {rnd+1} Debug (Î³={gamma}):\")\n",
    "                print(f\"      Fairness: min={min(S_fair_smoothed):.4f}, max={max(S_fair_smoothed):.4f}\")\n",
    "                print(f\"      Alphas: min={min(alphas):.4f}, max={max(alphas):.4f}\")\n",
    "        \n",
    "        # Apply weighted aggregation\n",
    "        new_state = model.state_dict()\n",
    "        for k in new_state:\n",
    "            new_state[k] = new_state[k] + sum(a * u[k] for a, u in zip(alphas, updates))\n",
    "        model.load_state_dict(new_state)\n",
    "        \n",
    "        # ================================================================\n",
    "        # EVALUATION\n",
    "        # ================================================================\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        client_accs = evaluate_per_client(model, client_loaders, device)\n",
    "        \n",
    "        jfi = calculate_jfi(client_accs)\n",
    "        max_min = calculate_max_min_fairness(client_accs)\n",
    "        var = calculate_variance(client_accs)\n",
    "        gap = calculate_accuracy_gap(client_accs)\n",
    "        \n",
    "        history['acc'].append(acc)\n",
    "        history['bias'].append(B_base)\n",
    "        history['alphas'].append(alphas.copy())\n",
    "        history['client_accuracies'].append(client_accs.copy())\n",
    "        history['jfi'].append(jfi)\n",
    "        history['max_min_fairness'].append(max_min)\n",
    "        history['variance'].append(var)\n",
    "        history['accuracy_gap'].append(gap)\n",
    "        history['min_client_acc'].append(min(client_accs))\n",
    "        history['max_client_acc'].append(max(client_accs))\n",
    "        \n",
    "        wandb.log({\n",
    "            'round': rnd + 1,\n",
    "            'accuracy': acc,\n",
    "            'bias': B_base,\n",
    "            'jfi': jfi,\n",
    "            'max_min_fairness': max_min,\n",
    "            'fairness_variance': var,\n",
    "            'accuracy_gap': gap,\n",
    "            'min_client_acc': min(client_accs),\n",
    "            'max_client_acc': max(client_accs),\n",
    "            'gan_trained': 1,\n",
    "            'alpha_min': min(alphas),\n",
    "            'alpha_max': max(alphas)\n",
    "        })\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "print(\"âœ… Training functions defined (FedAvg + Fed-Audit-GAN v7.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c580f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN ALL EXPERIMENTS\n",
    "# ============================================================\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENT 1: FedAvg (BASELINE)\n",
    "# ============================================================\n",
    "wandb.init(\n",
    "    project=\"FED_AUDIT_GAN_TEST_7_CIFAR10\",\n",
    "    name=f\"FedAvg_CIFAR10_clients{N_CLIENTS}_v7\",\n",
    "    config={\n",
    "        \"method\": \"FedAvg\",\n",
    "        \"dataset\": \"CIFAR-10\",\n",
    "        \"n_rounds\": N_ROUNDS,\n",
    "        \"n_clients\": N_CLIENTS,\n",
    "        \"non_iid_type\": \"pathological\",\n",
    "        \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "        \"device\": str(DEVICE),\n",
    "        \"num_gpus\": NUM_GPUS,\n",
    "        \"amp_enabled\": USE_AMP\n",
    "    }\n",
    ")\n",
    "\n",
    "fedavg_model, fedavg_history = run_fedavg(\n",
    "    n_rounds=N_ROUNDS,\n",
    "    n_clients=N_CLIENTS,\n",
    "    train_data=train_data,\n",
    "    client_idx=client_idx,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    client_loaders=client_loaders,\n",
    "    local_epochs=LOCAL_EPOCHS,\n",
    "    device=DEVICE,\n",
    "    use_amp=USE_AMP,\n",
    "    client_data_weights=CLIENT_DATA_WEIGHTS\n",
    ")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "all_results['FedAvg'] = {\n",
    "    'model': fedavg_model,\n",
    "    'history': fedavg_history,\n",
    "    'name': 'FedAvg'\n",
    "}\n",
    "\n",
    "print(f\"âœ… FedAvg Complete!\")\n",
    "print(f\"   Final Accuracy: {fedavg_history['acc'][-1]:.2f}%\")\n",
    "print(f\"   Final JFI: {fedavg_history['jfi'][-1]:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXPERIMENTS 2-3: Fed-Audit-GAN v7.0 with Î³ = 0.3 and 0.7\n",
    "# ============================================================\n",
    "for gamma in GAMMA_VALUES:\n",
    "    method_name = f\"FedAuditGAN_v7_Î³={gamma}\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"FED_AUDIT_GAN_TEST_7_CIFAR10\",\n",
    "        name=f\"{method_name}_CIFAR10_clients{N_CLIENTS}_FedProx\",\n",
    "        config={\n",
    "            \"method\": method_name,\n",
    "            \"dataset\": \"CIFAR-10\",\n",
    "            \"n_rounds\": N_ROUNDS,\n",
    "            \"n_clients\": N_CLIENTS,\n",
    "            \"gamma\": gamma,\n",
    "            \"momentum\": MOMENTUM,\n",
    "            \"warmup_rounds\": WARMUP_ROUNDS,\n",
    "            \"fedprox\": True,\n",
    "            \"mu\": MU,\n",
    "            \"gradient_clipping\": False,\n",
    "            \"gan_every_round\": True,\n",
    "            \"gan_during_warmup\": True,\n",
    "            \"aggregation_method\": \"V2_LINEAR\",\n",
    "            \"non_iid_type\": \"pathological\",\n",
    "            \"classes_per_client\": CLASSES_PER_CLIENT,\n",
    "            \"device\": str(DEVICE),\n",
    "            \"num_gpus\": NUM_GPUS,\n",
    "            \"amp_enabled\": USE_AMP\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    model, history = run_fed_audit_gan_v7(\n",
    "        gamma=gamma,\n",
    "        n_rounds=N_ROUNDS,\n",
    "        n_clients=N_CLIENTS,\n",
    "        warmup_rounds=WARMUP_ROUNDS,\n",
    "        momentum=MOMENTUM,\n",
    "        mu=MU,\n",
    "        train_data=train_data,\n",
    "        client_idx=client_idx,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        client_loaders=client_loaders,\n",
    "        n_gan_epochs=N_GAN_EPOCHS,\n",
    "        n_probes=N_PROBES,\n",
    "        local_epochs=LOCAL_EPOCHS,\n",
    "        device=DEVICE,\n",
    "        use_amp=USE_AMP,\n",
    "        client_data_weights=CLIENT_DATA_WEIGHTS\n",
    "    )\n",
    "    \n",
    "    wandb.finish()\n",
    "    \n",
    "    all_results[method_name] = {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'name': method_name\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… {method_name} Complete!\")\n",
    "    print(f\"   Final Accuracy: {history['acc'][-1]:.2f}%\")\n",
    "    print(f\"   Final JFI: {history['jfi'][-1]:.4f}\")\n",
    "    print(f\"   Accuracy Gap: {history['accuracy_gap'][-1]:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3171c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š RESULTS SUMMARY TABLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"ðŸ“Š CIFAR-10: FedAvg vs Fed-Audit-GAN v7.0 (FedProx + GAN Every Round)\")\n",
    "print(\"=\" * 120)\n",
    "\n",
    "print(f\"\\n{'METHOD':<35} {'GLOBAL ACC':<12} {'JFI':<10} {'MAX-MIN':<10} {'GAP':<10} {'MIN ACC':<10} {'MAX ACC':<10}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "best_acc = max(all_results[m]['history']['acc'][-1] for m in method_names)\n",
    "best_jfi = max(all_results[m]['history']['jfi'][-1] for m in method_names)\n",
    "lowest_gap = min(all_results[m]['history']['accuracy_gap'][-1] for m in method_names)\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    max_min = all_results[method]['history']['max_min_fairness'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    min_acc = all_results[method]['history']['min_client_acc'][-1]\n",
    "    max_acc = all_results[method]['history']['max_client_acc'][-1]\n",
    "    \n",
    "    acc_mark = \"ðŸ†\" if acc == best_acc else \"\"\n",
    "    jfi_mark = \"â­\" if jfi == best_jfi else \"\"\n",
    "    gap_mark = \"âœ…\" if gap == lowest_gap else \"\"\n",
    "    \n",
    "    print(f\"{name:<35} {acc:>8.2f}% {acc_mark:<2} {jfi:>8.4f} {jfi_mark:<2} {max_min:>8.4f}   {gap:>6.2f}% {gap_mark:<2} {min_acc:>8.2f}%  {max_acc:>8.2f}%\")\n",
    "\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Gamma sensitivity check\n",
    "print(f\"\\nâ­ GAMMA SENSITIVITY CHECK:\")\n",
    "if len(GAMMA_VALUES) >= 2:\n",
    "    g1, g2 = GAMMA_VALUES[0], GAMMA_VALUES[1]\n",
    "    acc1 = all_results[f'FedAuditGAN_v7_Î³={g1}']['history']['acc'][-1]\n",
    "    acc2 = all_results[f'FedAuditGAN_v7_Î³={g2}']['history']['acc'][-1]\n",
    "    jfi1 = all_results[f'FedAuditGAN_v7_Î³={g1}']['history']['jfi'][-1]\n",
    "    jfi2 = all_results[f'FedAuditGAN_v7_Î³={g2}']['history']['jfi'][-1]\n",
    "    \n",
    "    print(f\"   Î³={g1} vs Î³={g2}:\")\n",
    "    print(f\"      Accuracy difference: {abs(acc1 - acc2):.2f}%\")\n",
    "    print(f\"      JFI difference: {abs(jfi1 - jfi2):.4f}\")\n",
    "    \n",
    "    if abs(acc1 - acc2) > 0.1 or abs(jfi1 - jfi2) > 0.001:\n",
    "        print(f\"   âœ… SUCCESS! Different gamma values produce DIFFERENT results!\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Results still similar\")\n",
    "\n",
    "# Improvement over FedAvg\n",
    "fedavg_acc = all_results['FedAvg']['history']['acc'][-1]\n",
    "fedavg_jfi = all_results['FedAvg']['history']['jfi'][-1]\n",
    "fedavg_gap = all_results['FedAvg']['history']['accuracy_gap'][-1]\n",
    "\n",
    "print(f\"\\nðŸ“ˆ IMPROVEMENT OVER FedAvg:\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    \n",
    "    print(f\"   {name}:\")\n",
    "    print(f\"      Accuracy: {'+' if acc >= fedavg_acc else ''}{acc - fedavg_acc:.2f}%\")\n",
    "    print(f\"      JFI: {'+' if jfi >= fedavg_jfi else ''}{jfi - fedavg_jfi:.4f}\")\n",
    "    print(f\"      Gap Reduction: {fedavg_gap - gap:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"â­ V7 FEATURES:\")\n",
    "print(f\"   âœ… ADDED BACK: FedProx proximal term (Î¼={MU})\")\n",
    "print(f\"   âŒ REMOVED: Gradient clipping\")\n",
    "print(f\"   âœ… KEPT: GAN trains EVERY round (including warm-up)\")\n",
    "print(f\"   âœ… KEPT: Audit every round after round {WARMUP_ROUNDS}\")\n",
    "print(f\"   âœ… KEPT: V2 Linear aggregation formula\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ðŸ“Š VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "colors = {\n",
    "    'FedAvg': '#e74c3c',\n",
    "    'FedAuditGAN_v7_Î³=0.3': '#3498db',\n",
    "    'FedAuditGAN_v7_Î³=0.7': '#2ecc71',\n",
    "}\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "rounds = range(1, N_ROUNDS + 1)\n",
    "\n",
    "# Plot 1: Global Accuracy\n",
    "ax = axes[0, 0]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, acc, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange', label=f'Warm-up (GAN trains!)')\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('CIFAR-10: Global Test Accuracy')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: JFI\n",
    "ax = axes[0, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    jfi = all_results[method]['history']['jfi']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, jfi, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('JFI')\n",
    "ax.set_title(\"Jain's Fairness Index\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Accuracy Gap\n",
    "ax = axes[0, 2]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    gap = all_results[method]['history']['accuracy_gap']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.plot(rounds, gap, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=2, label=name)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Accuracy Gap (%)')\n",
    "ax.set_title('Best-Worst Client Gap')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Bias (GAN-based)\n",
    "ax = axes[1, 0]\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    bias = all_results[method]['history']['bias']\n",
    "    ax.plot(rounds, bias, color=colors.get(method, '#95a5a6'), linewidth=2, label=name)\n",
    "ax.axvspan(1, WARMUP_ROUNDS, alpha=0.15, color='orange', label='Warm-up')\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Bias')\n",
    "ax.set_title('GAN-Measured Bias (Every Round)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Min-Max Range\n",
    "ax = axes[1, 1]\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    min_acc = all_results[method]['history']['min_client_acc']\n",
    "    max_acc = all_results[method]['history']['max_client_acc']\n",
    "    linestyle = '--' if method == 'FedAvg' else '-'\n",
    "    ax.fill_between(rounds, min_acc, max_acc, color=colors.get(method, '#95a5a6'), alpha=0.2)\n",
    "    ax.plot(rounds, min_acc, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=1.5)\n",
    "    ax.plot(rounds, max_acc, color=colors.get(method, '#95a5a6'), linestyle=linestyle, linewidth=1.5, label=name)\n",
    "ax.set_xlabel('Round')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Min-Max Client Accuracy Range')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Final Per-Client Accuracy\n",
    "ax = axes[1, 2]\n",
    "x = np.arange(N_CLIENTS)\n",
    "width = 0.25\n",
    "for i, method in enumerate(method_names):\n",
    "    name = all_results[method]['name']\n",
    "    client_accs = all_results[method]['history']['client_accuracies'][-1]\n",
    "    ax.bar(x + i*width, client_accs, width, label=name, color=colors.get(method, '#95a5a6'), alpha=0.8)\n",
    "ax.set_xlabel('Client ID')\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_title('Per-Client Accuracy (Final Round)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cifar10_v7_fedprox_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“ Results saved to: cifar10_v7_fedprox_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834bd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODELS AND RESULTS\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "os.makedirs('results_cifar10_v7_fedprox', exist_ok=True)\n",
    "\n",
    "method_names = list(all_results.keys())\n",
    "\n",
    "for method in method_names:\n",
    "    name = all_results[method]['name']\n",
    "    filename = f\"results_cifar10_v7_fedprox/{name.replace('=', '').replace('.', '_').replace('Î³', 'gamma')}_CIFAR10.pth\"\n",
    "    \n",
    "    save_dict = {\n",
    "        'model_state_dict': all_results[method]['model'].state_dict(),\n",
    "        'history': all_results[method]['history'],\n",
    "        'config': {\n",
    "            'n_rounds': N_ROUNDS,\n",
    "            'n_clients': N_CLIENTS,\n",
    "            'classes_per_client': CLASSES_PER_CLIENT,\n",
    "            'device': str(DEVICE),\n",
    "            'fedprox': True,\n",
    "            'mu': MU,\n",
    "            'gradient_clipping': False,\n",
    "            'gan_every_round': True,\n",
    "            'gan_during_warmup': True,\n",
    "            'aggregation_method': 'V2_LINEAR'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if method != 'FedAvg':\n",
    "        gamma_val = float(name.split('=')[1]) if '=' in name else 0.5\n",
    "        save_dict['config']['momentum'] = MOMENTUM\n",
    "        save_dict['config']['warmup_rounds'] = WARMUP_ROUNDS\n",
    "        save_dict['config']['gamma'] = gamma_val\n",
    "    \n",
    "    torch.save(save_dict, filename)\n",
    "    print(f\"âœ… Saved: {filename}\")\n",
    "\n",
    "with open('results_cifar10_v7_fedprox/all_results_summary.pkl', 'wb') as f:\n",
    "    summary = {\n",
    "        method: {\n",
    "            'name': all_results[method]['name'],\n",
    "            'history': all_results[method]['history'],\n",
    "            'final_acc': all_results[method]['history']['acc'][-1],\n",
    "            'final_jfi': all_results[method]['history']['jfi'][-1]\n",
    "        }\n",
    "        for method in method_names\n",
    "    }\n",
    "    pickle.dump(summary, f)\n",
    "print(\"âœ… Saved: results_cifar10_v7_fedprox/all_results_summary.pkl\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"ðŸ“Š FINAL SUMMARY (Fed-Audit-GAN v7.0 - FedProx)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"ðŸ”µ BASELINE:\")\n",
    "print(f\"   FedAvg: {all_results['FedAvg']['history']['acc'][-1]:.2f}% accuracy, JFI={all_results['FedAvg']['history']['jfi'][-1]:.4f}\")\n",
    "print(\"\\nðŸŸ¢ FED-AUDIT-GAN v7.0:\")\n",
    "for method in method_names:\n",
    "    if method == 'FedAvg':\n",
    "        continue\n",
    "    name = all_results[method]['name']\n",
    "    acc = all_results[method]['history']['acc'][-1]\n",
    "    jfi = all_results[method]['history']['jfi'][-1]\n",
    "    gap = all_results[method]['history']['accuracy_gap'][-1]\n",
    "    print(f\"   {name}: {acc:.2f}% accuracy, JFI={jfi:.4f}, Gap={gap:.2f}%\")\n",
    "\n",
    "print(\"\\nâ­ V7 FEATURES:\")\n",
    "print(f\"   âœ… FedProx proximal term (Î¼={MU})\")\n",
    "print(f\"   âŒ No gradient clipping\")\n",
    "print(f\"   âœ… GAN trains EVERY round (including warm-up)\")\n",
    "print(f\"   âœ… V2 Linear: Weight = (1-Î³)Ã—Acc + Î³Ã—Fair\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nðŸ“Š Check WandB: https://wandb.ai\")\n",
    "print(f\"   Project: FED_AUDIT_GAN_TEST_7_CIFAR10\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
